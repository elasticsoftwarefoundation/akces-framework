This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where comments have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: *.md, main/**/*.java, main/**/*.xml, main/**/*.properties, main/**/*.proto, main/**/*.imports, main/**/*.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------

================================================================
Directory Structure
================================================================
main/
  api/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                aggregate/
                  Aggregate.java
                  AggregateState.java
                  AggregateStateType.java
                  CommandHandlerFunction.java
                  CommandType.java
                  DomainEventType.java
                  EventBridgeHandlerFunction.java
                  EventHandlerFunction.java
                  EventSourcingHandlerFunction.java
                  ProtocolRecordType.java
                  SchemaType.java
                  UpcastingHandlerFunction.java
                annotations/
                  AggregateIdentifier.java
                  AggregateInfo.java
                  AggregateStateInfo.java
                  CommandHandler.java
                  CommandInfo.java
                  DatabaseModelEventHandler.java
                  DatabaseModelInfo.java
                  DomainEventInfo.java
                  EventBridgeHandler.java
                  EventHandler.java
                  EventSourcingHandler.java
                  PIIData.java
                  QueryModelEventHandler.java
                  QueryModelInfo.java
                  QueryModelStateInfo.java
                  UpcastingHandler.java
                commands/
                  Command.java
                  CommandBus.java
                  CommandBusHolder.java
                events/
                  DomainEvent.java
                  ErrorEvent.java
                processmanager/
                  AkcesProcess.java
                  ProcessManager.java
                  ProcessManagerState.java
                  UnknownAkcesProcessException.java
                query/
                  DatabaseModel.java
                  DatabaseModelEventHandlerFunction.java
                  QueryModel.java
                  QueryModelEventHandlerFunction.java
                  QueryModelState.java
                  QueryModelStateType.java
                AkcesException.java
    pom.xml
  client/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                client/
                  AkcesClient.java
                  AkcesClientAutoConfiguration.java
                  AkcesClientCommandException.java
                  AkcesClientController.java
                  AkcesClientControllerState.java
                  CommandRefusedException.java
                  CommandSendingFailedException.java
                  CommandSerializationException.java
                  CommandServiceApplication.java
                  CommandValidationException.java
                  MissingDomainEventException.java
                  UnknownSchemaException.java
                  UnroutableCommandException.java
        resources/
          META-INF/
            spring/
              org.springframework.boot.autoconfigure.AutoConfiguration.imports
          akces-client.properties
      test/
        java/
          org/
            elasticsoftware/
              akces/
                client/
                  commands/
                    CreateAccountCommand.java
                    InvalidCommand.java
                    UnroutableCommand.java
                  events/
                    AccountCreatedEvent.java
                  AkcesClientTestConfiguration.java
                  AkcesClientTests.java
        resources/
          akces-client.properties
          logback-test.xml
    pom.xml
  eventcatalog/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                eventcatalog/
                  EventCatalogProcessor.java
                  JsonSchemaGenerator.java
                  MessageTemplateGenerator.java
                  ServiceTemplateGenerator.java
      test/
        java/
          org/
            elasticsoftware/
              akces/
                eventcatalog/
                  EventCatalogProcessorTest.java
                  MessageTemplateGeneratorTests.java
                  ServiceTemplateGeneratorTests.java
    pom.xml
  query-support/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                query/
                  database/
                    beans/
                      DatabaseModelBeanFactoryPostProcessor.java
                      DatabaseModelEventHandlerFunctionAdapter.java
                    jdbc/
                      JdbcDatabaseModel.java
                    jpa/
                      JpaDatabaseModel.java
                      PartitionOffset.java
                      PartitionOffsetRepository.java
                    AkcesDatabaseModelAutoConfiguration.java
                    AkcesDatabaseModelController.java
                    AkcesDatabaseModelControllerState.java
                    DatabaseModelImplementationPresentCondition.java
                    DatabaseModelPartition.java
                    DatabaseModelPartitionState.java
                    DatabaseModelRuntime.java
                    DatabaseModelRuntimeFactory.java
                    KafkaDatabaseModelRuntime.java
                  models/
                    beans/
                      QueryModelBeanFactoryPostProcessor.java
                      QueryModelEventHandlerFunctionAdapter.java
                    AkcesQueryModelAutoConfiguration.java
                    AkcesQueryModelController.java
                    AkcesQueryModelControllerState.java
                    KafkaQueryModelRuntime.java
                    QueryModelExecutionCancelledException.java
                    QueryModelExecutionDisabledException.java
                    QueryModelExecutionException.java
                    QueryModelIdNotFoundException.java
                    QueryModelImplementationPresentCondition.java
                    QueryModelNotFoundException.java
                    QueryModelRuntime.java
                    QueryModelRuntimeFactory.java
                    QueryModels.java
                  QueryServiceApplication.java
        resources/
          META-INF/
            spring/
              org.springframework.boot.autoconfigure.AutoConfiguration.imports
          akces-databasemodel.properties
          akces-querymodel.properties
      test/
        java/
          org/
            elasticsoftware/
              akces/
                query/
                  database/
                    model/
                      DefaultJdbcModel.java
                    DatabaseModelRuntimeTests.java
                    DatabaseModelTestConfiguration.java
                  models/
                    account/
                      AccountQueryModel.java
                      AccountQueryModelState.java
                    wallet/
                      WalletQueryModel.java
                      WalletQueryModelState.java
                    QueryModelRuntimeTests.java
                    QueryModelTestConfiguration.java
        resources/
          db/
            changelog/
              liquibase.yaml
          logback-test.xml
    pom.xml
  runtime/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                aggregate/
                  AggregateRuntime.java
                  IndexParams.java
                beans/
                  AggregateBeanFactoryPostProcessor.java
                  AggregateStateUpcastingHandlerFunctionAdapter.java
                  AggregateValidator.java
                  CommandHandlerFunctionAdapter.java
                  DomainEventUpcastingHandlerFunctionAdapter.java
                  EventBridgeHandlerFunctionAdapter.java
                  EventHandlerFunctionAdapter.java
                  EventSourcingHandlerFunctionAdapter.java
                  ProtocolRecordTypeValueCodeGeneratorDelegate.java
                kafka/
                  AggregatePartition.java
                  AggregatePartitionCommandBus.java
                  AggregatePartitionState.java
                  AggregateRuntimeFactory.java
                  KafkaAggregateRuntime.java
                  PartitionUtils.java
                state/
                  AggregateStateRepository.java
                  AggregateStateRepositoryException.java
                  AggregateStateRepositoryFactory.java
                  InMemoryAggregateStateRepository.java
                  InMemoryAggregateStateRepositoryFactory.java
                  RocksDBAggregateStateRepository.java
                  RocksDBAggregateStateRepositoryFactory.java
                AggregateServiceApplication.java
                AkcesAggregateController.java
                AkcesControllerState.java
        resources/
          protobuf/
            AggregateStateRecord.proto
            CommandRecord.proto
            DomainEventRecord.proto
          akces-aggregateservice.properties
      test/
        java/
          org/
            elasticsoftware/
              akces/
                beans/
                  AotServicesTest.java
                  MinInsyncReplicasTest.java
              akcestest/
                aggregate/
                  account/
                    Account.java
                    AccountCreatedEvent.java
                    AccountCreatedEventV2.java
                    AccountState.java
                    CreateAccountCommand.java
                    PreviousAccountState.java
                  orders/
                    BuyOrderCreatedEvent.java
                    BuyOrderPlacedEvent.java
                    BuyOrderProcess.java
                    BuyOrderRejectedEvent.java
                    FxMarket.java
                    OrderProcess.java
                    OrderProcessManager.java
                    OrderProcessManagerState.java
                    PlaceBuyOrderCommand.java
                    UserOrderProcessesCreatedEvent.java
                  wallet/
                    AmountReservedEvent.java
                    BalanceAlreadyExistsErrorEvent.java
                    BalanceCreatedEvent.java
                    CreateBalanceCommand.java
                    CreateWalletCommand.java
                    CreditWalletCommand.java
                    ExternalAccountCreatedEvent.java
                    InsufficientFundsErrorEvent.java
                    InvalidAccountCreatedEvent.java
                    InvalidAmountErrorEvent.java
                    InvalidCurrencyErrorEvent.java
                    ReserveAmountCommand.java
                    Wallet.java
                    WalletCreatedEvent.java
                    WalletCreditedEvent.java
                    WalletState.java
                    WalletStateV2.java
                control/
                  AkcesAggregateControllerTests.java
                old/
                  BalanceCreatedEvent.java
                  BuyOrderPlacedEvent.java
                  CreateWalletCommand.java
                  WalletCreditedEvent.java
                protocol/
                  ProtocolTests.java
                schemas/
                  AccountCreatedEvent.java
                  AccountCreatedEventV2.java
                  AccountCreatedEventV3.java
                  AccountTypeV1.java
                  AccountTypeV2.java
                  CreditWalletCommand.java
                  JsonSchemaTests.java
                  NotCompatibleAccountCreatedEventV4.java
                state/
                  RocksDBAggregateStateRepositoryTests.java
                util/
                  HostUtilsTests.java
                AccountConfiguration.java
                AccountTests.java
                AggregateServiceApplicationTests.java
                RuntimeConfiguration.java
                RuntimeTests.java
                TestUtils.java
                WalletConfiguration.java
                WalletTests.java
        resources/
          akces-client.properties
          logback-test.xml
          test-application.yaml
    pom.xml
  shared/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                control/
                  AggregateServiceCommandType.java
                  AggregateServiceDomainEventType.java
                  AggregateServiceRecord.java
                  AkcesControlRecord.java
                  AkcesRegistry.java
                errors/
                  AggregateAlreadyExistsErrorEvent.java
                  CommandExecutionErrorEvent.java
                gdpr/
                  jackson/
                    AkcesGDPRModule.java
                    PIIDataDeserializerModifier.java
                    PIIDataJsonDeserializer.java
                    PIIDataJsonSerializer.java
                    PIIDataSerializerModifier.java
                  EncryptingGDPRContext.java
                  GDPRAnnotationUtils.java
                  GDPRContext.java
                  GDPRContextHolder.java
                  GDPRContextRepository.java
                  GDPRContextRepositoryException.java
                  GDPRContextRepositoryFactory.java
                  GDPRKeyUtils.java
                  InMemoryGDPRContextRepository.java
                  InMemoryGDPRContextRepositoryFactory.java
                  NoopGDPRContext.java
                  RocksDBGDPRContextRepository.java
                  RocksDBGDPRContextRepositoryFactory.java
                kafka/
                  CustomKafkaConsumerFactory.java
                  CustomKafkaProducerFactory.java
                  RecordAndMetadata.java
                protocol/
                  AggregateStateRecord.java
                  CommandRecord.java
                  CommandResponseRecord.java
                  DomainEventRecord.java
                  GDPRKeyRecord.java
                  PayloadEncoding.java
                  ProtocolRecord.java
                schemas/
                  IncompatibleSchemaException.java
                  InvalidSchemaVersionException.java
                  KafkaSchemaRegistry.java
                  PreviousSchemaVersionMissingException.java
                  SchemaException.java
                  SchemaNotBackwardsCompatibleException.java
                  SchemaNotFoundException.java
                  SchemaVersionNotFoundException.java
                serialization/
                  AkcesControlRecordSerde.java
                  BigDecimalSerializer.java
                  ProtocolRecordSerde.java
                util/
                  EnvironmentPropertiesPrinter.java
                  HostUtils.java
                  KafkaSender.java
                  KafkaUtils.java
      test/
        java/
          org/
            elasticsoftware/
              akces/
                gdpr/
                  jackson/
                    AkcesGDPRModuleTests.java
                  GDPRContextTests.java
                  RocksDBGDPRContextRepositoryTests.java
    pom.xml
  pom.xml
FRAMEWORK_OVERVIEW.md
README.md
RELEASE.md
SERVICES.md
TEST-APPS.md

================================================================
Files
================================================================

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/Aggregate.java
================
public interface Aggregate<S extends AggregateState> {
default String getName() {
return getClass().getSimpleName();
⋮----
Class<S> getStateClass();
⋮----
default CommandBus getCommandBus() {
return CommandBusHolder.getCommandBus(getClass());

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/AggregateState.java
================
public interface AggregateState {
⋮----
String getAggregateId();
⋮----
default String getIndexKey() {
return getAggregateId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/CommandHandlerFunction.java
================
public interface CommandHandlerFunction<S extends AggregateState, C extends Command, E extends DomainEvent> {
⋮----
Stream<E> apply(@NotNull C command, S state);
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override isCreate()");
⋮----
default CommandType<C> getCommandType() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getCommandType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getAggregate()");
⋮----
default List<DomainEventType<E>> getProducedDomainEventTypes() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getProducedDomainEventTypes()");
⋮----
default List<DomainEventType<E>> getErrorEventTypes() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getErrorEventTypes()");
⋮----
default CommandBus getCommandBus() {
return CommandBusHolder.getCommandBus(getAggregate().getClass());

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/EventHandlerFunction.java
================
public interface EventHandlerFunction<S extends AggregateState, InputEvent extends DomainEvent, E extends DomainEvent> {
⋮----
Stream<E> apply(@NotNull InputEvent event, S state);
⋮----
default DomainEventType<InputEvent> getEventType() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getEventType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getAggregate()");
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override isCreate()");
⋮----
default List<DomainEventType<E>> getProducedDomainEventTypes() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getProducedDomainEventTypes()");
⋮----
default List<DomainEventType<E>> getErrorEventTypes() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getErrorEventTypes()");
⋮----
default CommandBus getCommandBus() {
return CommandBusHolder.getCommandBus(getAggregate().getClass());

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/EventSourcingHandlerFunction.java
================
public interface EventSourcingHandlerFunction<S extends AggregateState, E extends DomainEvent> {
⋮----
S apply(@NotNull E event, S state);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override getEventType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override getAggregate()");
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override isCreate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/AggregateIdentifier.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/CommandHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/EventHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/EventSourcingHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/PIIData.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelEventHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelStateInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/commands/Command.java
================
public interface Command {
⋮----
String getAggregateId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/commands/CommandBus.java
================
public interface CommandBus {
void send(Command command);

================
File: main/api/src/main/java/org/elasticsoftware/akces/events/DomainEvent.java
================
public interface DomainEvent {
⋮----
String getAggregateId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/events/ErrorEvent.java
================
public interface ErrorEvent extends DomainEvent {

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/AkcesProcess.java
================
public interface AkcesProcess {
⋮----
String getProcessId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/ProcessManager.java
================
public interface ProcessManager<S extends AggregateState, P extends AkcesProcess> extends Aggregate<S> {

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/ProcessManagerState.java
================
public interface ProcessManagerState<P extends AkcesProcess> extends AggregateState {
P getAkcesProcess(String processId) throws UnknownAkcesProcessException;
⋮----
boolean hasAkcesProcess(String processId);

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/UnknownAkcesProcessException.java
================
public class UnknownAkcesProcessException extends AkcesException {
⋮----
public String getProcessId() {

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModel.java
================
public interface QueryModel<S extends QueryModelState> {
default String getName() {
return getClass().getSimpleName();
⋮----
Class<S> getStateClass();
⋮----
String getIndexName();

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModelEventHandlerFunction.java
================
public interface QueryModelEventHandlerFunction<S extends QueryModelState, E extends DomainEvent> {
⋮----
S apply(@NotNull E event, S state);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getEventType()");
⋮----
default QueryModel<S> getQueryModel() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getQueryModel()");
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override isCreate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModelState.java
================
public interface QueryModelState {
⋮----
String getIndexKey();

================
File: main/api/src/main/java/org/elasticsoftware/akces/AkcesException.java
================
public abstract class AkcesException extends RuntimeException {
⋮----
public String getAggregateName() {
⋮----
public String getAggregateId() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientAutoConfiguration.java
================
public class AkcesClientAutoConfiguration {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public AkcesControlRecordSerde controlSerde(ObjectMapper objectMapper) {
return new AkcesControlRecordSerde(objectMapper);
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public KafkaAdmin createKafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
⋮----
public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesClientSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ProducerFactory<String, ProtocolRecord> producerFactory(KafkaProperties properties) {
return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), serde.serializer());
⋮----
public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
⋮----
public ConsumerFactory<String, ProtocolRecord> commandResponseConsumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public ClassPathScanningCandidateComponentProvider domainEventScanner(Environment environment) {
ClassPathScanningCandidateComponentProvider provider = new ClassPathScanningCandidateComponentProvider(false);
provider.addIncludeFilter(new AnnotationTypeFilter(DomainEventInfo.class));
provider.setEnvironment(environment);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();
⋮----
public AkcesClientController akcesClient(@Qualifier("akcesClientProducerFactory") ProducerFactory<String, ProtocolRecord> producerFactory,
⋮----
return new AkcesClientController(producerFactory,

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientCommandException.java
================
public abstract class AkcesClientCommandException extends RuntimeException {
⋮----
this.commandType = commandInfo != null ? commandInfo.type() : null;
this.commandVersion = commandInfo != null ? commandInfo.version() : null;
⋮----
public Class<? extends Command> getCommandClass() {
⋮----
public String getCommandType() {
⋮----
public Integer getCommandVersion() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientControllerState.java
================


================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandRefusedException.java
================
public class CommandRefusedException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Command Refused because AkcesClient is not in RUNNING state");
⋮----
public AkcesClientControllerState getState() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandSendingFailedException.java
================
public class CommandSendingFailedException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Sending", cause);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandSerializationException.java
================
public class CommandSerializationException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Serializing", cause);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandServiceApplication.java
================
public class CommandServiceApplication {
public static void main(String[] args) {
SpringApplication application = new SpringApplication(CommandServiceApplication.class);
⋮----
application.setSources(Set.of(args));
⋮----
application.run();

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandValidationException.java
================
public class CommandValidationException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Validating", cause);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/MissingDomainEventException.java
================
public class MissingDomainEventException extends AkcesClientCommandException {
⋮----
commandClass.getAnnotation(CommandInfo.class),
⋮----
public String getSchemaName() {
⋮----
public int getVersion() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/UnknownSchemaException.java
================
public class UnknownSchemaException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Unknown Schema " + schemaIdentifier);
⋮----
public String getSchemaIdentifier() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/UnroutableCommandException.java
================
public class UnroutableCommandException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Unable to Route Command, no AggregateService found that handles the Command");

================
File: main/client/src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#

org.elasticsoftware.akces.client.AkcesClientAutoConfiguration

================
File: main/client/src/main/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/commands/CreateAccountCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/commands/InvalidCommand.java
================
public record InvalidCommand(String id) implements Command {
⋮----
public @NotNull String getAggregateId() {
return id();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/commands/UnroutableCommand.java
================
public record UnroutableCommand(String id) implements Command {
⋮----
public @NotNull String getAggregateId() {
return id();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/events/AccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/AkcesClientTestConfiguration.java
================
public class AkcesClientTestConfiguration {

================
File: main/client/src/test/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
akces.client.domainEventsPackage=org.elasticsoftware.akces.client.events
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: main/client/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.client" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/beans/QueryModelBeanFactoryPostProcessor.java
================
public class QueryModelBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
⋮----
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
⋮----
Arrays.asList(beanFactory.getBeanNamesForAnnotation(QueryModelInfo.class)).forEach(beanName -> {
BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
⋮----
Class<?> queryModelClass = Class.forName(bd.getBeanClassName());
List<Method> queryModelEventHandlers = Arrays.stream(queryModelClass.getMethods())
.filter(method -> method.isAnnotationPresent(QueryModelEventHandler.class))
.toList();
queryModelEventHandlers.forEach(eventHandlerMethod -> processQueryModelEventHandler(beanName, eventHandlerMethod, bdr));
⋮----
throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
⋮----
bdr.registerBeanDefinition(beanName + "QueryModelRuntime",
BeanDefinitionBuilder.genericBeanDefinition(QueryModelRuntimeFactory.class)
.addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
.addConstructorArgReference("akcesQueryModelSchemaRegistry")
.addConstructorArgReference(beanName)
.getBeanDefinition());
⋮----
throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
⋮----
private void processQueryModelEventHandler(String aggregateBeanName, Method queryModelEventHandlerMethod, BeanDefinitionRegistry bdr) {
QueryModelEventHandler queryModelEventHandler = queryModelEventHandlerMethod.getAnnotation(QueryModelEventHandler.class);
if (queryModelEventHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(queryModelEventHandlerMethod.getParameterTypes()[0]) &&
QueryModelState.class.isAssignableFrom(queryModelEventHandlerMethod.getParameterTypes()[1]) &&
QueryModelState.class.isAssignableFrom(queryModelEventHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = queryModelEventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_qmeh_" + queryModelEventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
bdr.registerBeanDefinition(beanName,
BeanDefinitionBuilder.genericBeanDefinition(QueryModelEventHandlerFunctionAdapter.class)
.addConstructorArgReference(aggregateBeanName)
.addConstructorArgValue(queryModelEventHandlerMethod.getName())
.addConstructorArgValue(queryModelEventHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(queryModelEventHandlerMethod.getParameterTypes()[1])
.addConstructorArgValue(queryModelEventHandler.create())
.addConstructorArgValue(eventInfo.type())
.addConstructorArgValue(eventInfo.version())
.setInitMethodName("init")
⋮----
throw new ApplicationContextException("Invalid QueryModelEventHandler method signature: " + queryModelEventHandlerMethod);
⋮----
public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
⋮----
public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelAutoConfiguration.java
================
public class AkcesQueryModelAutoConfiguration {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public static QueryModelBeanFactoryPostProcessor queryModelBeanFactoryPostProcessor() {
return new QueryModelBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesQueryModelSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
⋮----
public KafkaAdmin kafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
⋮----
public AkcesQueryModelController queryModelRuntimes(@Qualifier("akcesQueryModelKafkaAdmin") KafkaAdmin kafkaAdmin,
⋮----
return new AkcesQueryModelController(kafkaAdmin, consumerFactory, gdprContextRepositoryFactory);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelControllerState.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionCancelledException.java
================
public class QueryModelExecutionCancelledException extends QueryModelExecutionException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionDisabledException.java
================
public class QueryModelExecutionDisabledException extends QueryModelExecutionException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionException.java
================
public class QueryModelExecutionException extends RuntimeException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelIdNotFoundException.java
================
public class QueryModelIdNotFoundException extends QueryModelExecutionException {
⋮----
super("Id "+modelId+ "doesn't exist for QueryModel: "+modelClass.getSimpleName(), modelClass);
⋮----
public String getModelId() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelImplementationPresentCondition.java
================
public class QueryModelImplementationPresentCondition extends SpringBootCondition {
⋮----
public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) {
boolean match = Optional.ofNullable(context.getBeanFactory())
.map(beanFactory -> beanFactory.getBeanNamesForAnnotation(QueryModelInfo.class))
.filter(beanNames -> beanNames.length > 0).isPresent();
return match ? ConditionOutcome.match() : ConditionOutcome.noMatch("No QueryModel beans found");

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelNotFoundException.java
================
public class QueryModelNotFoundException extends QueryModelExecutionException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelRuntime.java
================
public interface QueryModelRuntime<S extends QueryModelState> {
String getName();
⋮----
String getIndexName();
⋮----
Class<? extends QueryModel<S>> getQueryModelClass();
⋮----
S apply(List<DomainEventRecord> eventRecords, S currentState) throws IOException;
⋮----
void validateDomainEventSchemas();
⋮----
boolean shouldHandlePIIData();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModels.java
================
public interface QueryModels {
<S extends QueryModelState> CompletionStage<S> getHydratedState(Class<? extends QueryModel<S>> modelClass, String id);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/QueryServiceApplication.java
================
public class QueryServiceApplication {
public static void main(String[] args) {
SpringApplication application = new SpringApplication(QueryServiceApplication.class);
⋮----
application.setSources(Set.of(args));
⋮----
application.run();

================
File: main/query-support/src/main/resources/akces-querymodel.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/account/AccountQueryModel.java
================
public class AccountQueryModel implements QueryModel<AccountQueryModelState> {
⋮----
public String getName() {
⋮----
public Class<AccountQueryModelState> getStateClass() {
⋮----
public String getIndexName() {
⋮----
public AccountQueryModelState create(AccountCreatedEvent event, AccountQueryModelState isNull) {
return new AccountQueryModelState(event.userId(), event.country(), event.firstName(), event.lastName(), event.email());

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/account/AccountQueryModelState.java
================
) implements QueryModelState {
⋮----
public String getIndexKey() {
return accountId();

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/wallet/WalletQueryModelState.java
================
public record WalletQueryModelState(String walletId, List<Balance> balances) implements QueryModelState {
⋮----
public String getIndexKey() {
return walletId();
⋮----
public BigDecimal getAvailableAmount() {
return amount.subtract(reservedAmount);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/aggregate/IndexParams.java
================


================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartitionCommandBus.java
================
class AggregatePartitionCommandBus extends CommandBusHolder {
static void registerCommandBus(AggregatePartition aggregatePartition) {
commandBusThreadLocal.set(aggregatePartition);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartitionState.java
================


================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/PartitionUtils.java
================
public final class PartitionUtils {
⋮----
public static Map<Integer, AggregatePartition> toAggregatePartitions(Collection<TopicPartition> topicPartitions) {
Set<Integer> partitions = topicPartitions.stream().map(TopicPartition::partition).collect(Collectors.toSet());
⋮----
partitions.forEach(partition -> {
Set<TopicPartition> aggregatePartition = topicPartitions.stream().filter(topicPartition ->
topicPartition.partition() == partition &&
(topicPartition.topic().endsWith(COMMANDS_SUFFIX) ||
topicPartition.topic().endsWith(DOMAINEVENTS_SUFFIX) ||
topicPartition.topic().endsWith(AGGREGRATESTATE_SUFFIX))).collect(Collectors.toSet());
if (aggregatePartition.size() == 3) {
TopicPartition command = aggregatePartition.stream().filter(topicPartition ->
topicPartition.topic().endsWith(COMMANDS_SUFFIX)).findFirst().orElseThrow();
TopicPartition domainEvent = aggregatePartition.stream().filter(topicPartition ->
topicPartition.topic().endsWith(DOMAINEVENTS_SUFFIX)).findFirst().orElseThrow();
TopicPartition aggregateState = aggregatePartition.stream().filter(topicPartition ->
topicPartition.topic().endsWith(AGGREGRATESTATE_SUFFIX)).findFirst().orElseThrow();
⋮----
throw new NoSuchElementException("Partition " + partition + " is incomplete, found " + aggregatePartition);
⋮----
public static TopicPartition toCommandTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition(aggregate.getName() + COMMANDS_SUFFIX, partition);
⋮----
public static TopicPartition toDomainEventTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition(aggregate.getName() + DOMAINEVENTS_SUFFIX, partition);
⋮----
public static TopicPartition toAggregateStateTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition(aggregate.getName() + AGGREGRATESTATE_SUFFIX, partition);
⋮----
public static TopicPartition toGDPRKeysTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition("Akces-GDPRKeys", partition);
⋮----
public static List<TopicPartition> toExternalDomainEventTopicPartitions(AggregateRuntime aggregate, int partition) {
return aggregate.getExternalDomainEventTypes().stream().map(externalDomainEvent ->
new TopicPartition(externalDomainEvent.typeName() + DOMAINEVENTS_SUFFIX, partition)).collect(Collectors.toList());
⋮----
public static TopicPartition parseReplyToTopicPartition(String replyTo) {
⋮----
int lastIndex = replyTo.lastIndexOf('-');
String topic = replyTo.substring(0, lastIndex);
int partition = Integer.parseInt(replyTo.substring(lastIndex + 1));
return new TopicPartition(topic, partition);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepository.java
================
public interface AggregateStateRepository extends Closeable {
default long getOffset() {
⋮----
void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture);
⋮----
void commit();
⋮----
void rollback();
⋮----
void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords);
⋮----
AggregateStateRecord get(String aggregateId);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepositoryException.java
================
public class AggregateStateRepositoryException extends RuntimeException {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepositoryFactory.java
================
public interface AggregateStateRepositoryFactory {
AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/InMemoryAggregateStateRepositoryFactory.java
================
public class InMemoryAggregateStateRepositoryFactory implements AggregateStateRepositoryFactory {
⋮----
public AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId) {
return new InMemoryAggregateStateRepository();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/RocksDBAggregateStateRepositoryFactory.java
================
public class RocksDBAggregateStateRepositoryFactory implements AggregateStateRepositoryFactory {
⋮----
public AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId) {
return new RocksDBAggregateStateRepository(
⋮----
aggregateRuntime.getName() + "-AggregateState-" + partitionId.toString(),
aggregateRuntime.getName() + "-AggregateState",
serde.serializer(),
serde.deserializer());

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/AggregateServiceApplication.java
================
public class AggregateServiceApplication {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public static void main(String[] args) {
SpringApplication application = new SpringApplication(AggregateServiceApplication.class);
⋮----
application.setSources(Set.of(args));
⋮----
application.run();
⋮----
public static AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
return new AggregateBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public AkcesControlRecordSerde akcesControlRecordSerde(ObjectMapper objectMapper) {
return new AkcesControlRecordSerde(objectMapper);
⋮----
public KafkaAdmin kafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
⋮----
public SchemaRegistryClient schemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry schemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient,
⋮----
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public ProducerFactory<String, ProtocolRecord> producerFactory(KafkaProperties properties) {
return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), serde.serializer());
⋮----
public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
⋮----
public ProducerFactory<String, AkcesControlRecord> controlProducerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), controlSerde.serializer());
⋮----
public AggregateStateRepositoryFactory aggregateStateRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBAggregateStateRepositoryFactory(serde, baseDir);
⋮----
public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/AkcesControllerState.java
================


================
File: main/runtime/src/main/resources/protobuf/AggregateStateRecord.proto
================
// org.elasticsoftware.akces.protocol.AggregateStateRecord

// Message for org.elasticsoftware.akces.protocol.AggregateStateRecord
message AggregateStateRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional int64 generation = 7;
  optional string tenantId = 8;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: main/runtime/src/main/resources/protobuf/CommandRecord.proto
================
// org.elasticsoftware.akces.protocol.CommandRecord

// Message for org.elasticsoftware.akces.protocol.CommandRecord
message CommandRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional string tenantId = 7;
  optional string id = 8;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: main/runtime/src/main/resources/protobuf/DomainEventRecord.proto
================
// org.elasticsoftware.akces.protocol.DomainEventRecord

// Message for org.elasticsoftware.akces.protocol.DomainEventRecord
message DomainEventRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional int64 generation = 7;
  optional string tenantId = 8;
  optional string id = 9;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: main/runtime/src/main/resources/akces-aggregateservice.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: main/runtime/src/test/java/org/elasticsoftware/akces/beans/AotServicesTest.java
================
public class AotServicesTest {
⋮----
void testLoadAotServices() {
AotServices.Loader loader = AotServices.factories();
List<ValueCodeGenerator.Delegate> additionalDelegates = loader.load(ValueCodeGenerator.Delegate.class).asList();
Assertions.assertFalse(additionalDelegates.isEmpty());

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return orderId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderPlacedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return orderId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderProcess.java
================
String clientReference) implements OrderProcess {
⋮----
public String getProcessId() {
return orderId();
⋮----
public DomainEvent handle(InsufficientFundsErrorEvent error) {
return new BuyOrderRejectedEvent(error.walletId(), orderId(), clientReference());
⋮----
public DomainEvent handle(InvalidCurrencyErrorEvent error) {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderRejectedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/FxMarket.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcess.java
================
public sealed interface OrderProcess extends AkcesProcess permits BuyOrderProcess {
String orderId();
⋮----
FxMarket market();
⋮----
BigDecimal quantity();
⋮----
BigDecimal limitPrice();
⋮----
String clientReference();
⋮----
DomainEvent handle(InsufficientFundsErrorEvent error);
⋮----
DomainEvent handle(InvalidCurrencyErrorEvent error);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/PlaceBuyOrderCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/UserOrderProcessesCreatedEvent.java
================
public record UserOrderProcessesCreatedEvent(@NotNull @AggregateIdentifier String userId) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/AmountReservedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/BalanceAlreadyExistsErrorEvent.java
================
@NotNull String currency) implements ErrorEvent {
⋮----
public String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/BalanceCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreateBalanceCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreateWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreditWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/ExternalAccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InsufficientFundsErrorEvent.java
================
) implements ErrorEvent {
⋮----
public @Nonnull String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidAccountCreatedEvent.java
================
public record InvalidAccountCreatedEvent(String middleName, String currency) implements DomainEvent {
⋮----
public String getAggregateId() {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidAmountErrorEvent.java
================
) implements ErrorEvent {
⋮----
public @Nonnull String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidCurrencyErrorEvent.java
================
) implements ErrorEvent {
⋮----
public String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/ReserveAmountCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletCreditedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletState.java
================
) implements AggregateState {
⋮----
public String getAggregateId() {
return id();
⋮----
public BigDecimal getAvailableAmount() {
return amount.subtract(reservedAmount);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/BalanceCreatedEvent.java
================
public record BalanceCreatedEvent(@AggregateIdentifier @NotNull String id, String currency) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/BuyOrderPlacedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return orderId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/CreateWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/WalletCreditedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/protocol/ProtocolTests.java
================
public class ProtocolTests {
⋮----
public void testJacksonProtobuf() throws IOException {
⋮----
DomainEventRecord testRecord = new DomainEventRecord("tenant1", "WalletCreated", 1, "{}".getBytes(StandardCharsets.UTF_8), JSON, "1", UUID.randomUUID().toString(), 1L);
⋮----
ProtobufSchema schema = ProtobufSchemaLoader.std.load(new StringReader(protobuf));
ObjectMapper objectMapper = new ProtobufMapper();
byte[] serializedTestRecord = objectMapper.writer(schema).writeValueAsBytes(testRecord);
⋮----
DomainEventRecord deserializedTestRecord = objectMapper.readerFor(DomainEventRecord.class).with(schema).readValue(serializedTestRecord);
⋮----
assertEquals(deserializedTestRecord.name(), testRecord.name());
assertEquals(deserializedTestRecord.version(), testRecord.version());
assertArrayEquals(deserializedTestRecord.payload(), testRecord.payload());
assertEquals(deserializedTestRecord.encoding(), testRecord.encoding());
assertEquals(deserializedTestRecord.aggregateId(), testRecord.aggregateId());
⋮----
public void generateDomainEventRecordProtobufSchema() throws JsonMappingException {
ProtobufMapper mapper = new ProtobufMapper();
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(DomainEventRecord.class);
NativeProtobufSchema nativeProtobufSchema = schemaWrapper.getSource();
⋮----
String asProtofile = nativeProtobufSchema.toString();
⋮----
System.out.println(asProtofile);
⋮----
public void generateCommandRecordProtobufSchema() throws JsonMappingException {
⋮----
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(CommandRecord.class);
⋮----
public void generateAggregateStateRecordProtobufSchema() throws JsonMappingException {
⋮----
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(AggregateStateRecord.class);
⋮----
public void generateCommandResponseRecordProtobufSchema() throws JsonMappingException {
⋮----
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(CommandResponseRecord.class);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountTypeV1.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountTypeV2.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/CreditWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/NotCompatibleAccountCreatedEventV4.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/state/RocksDBAggregateStateRepositoryTests.java
================
public class RocksDBAggregateStateRepositoryTests {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
private final ObjectMapper objectMapper = new ObjectMapper();
⋮----
private final Future<RecordMetadata> producerResponse = mock(Future.class);
⋮----
public static void cleanUp() throws IOException {
⋮----
Files.walk(Paths.get("/tmp/rocksdb"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
public void testCreate() throws RocksDBException, IOException {
⋮----
new RocksDBAggregateStateRepository("/tmp/rocksdb",
⋮----
serde.serializer(),
serde.deserializer())) {
AggregateStateRecord record = repository.get("1234");
Assertions.assertNull(record);
⋮----
public void testSingleUpdate() throws RocksDBException, IOException, ExecutionException, InterruptedException {
⋮----
WalletState state = new WalletState(id, List.of(new WalletState.Balance("USD", BigDecimal.ZERO)));
AggregateStateRecord record = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(state),
⋮----
UUID.randomUUID().toString(),
⋮----
repository.prepare(record, producerResponse);
⋮----
when(producerResponse.get()).thenReturn(new RecordMetadata(new TopicPartition("Wallet-AggregateState", 0), 12, 0, System.currentTimeMillis(), 16, 345));
repository.commit();
⋮----
AggregateStateRecord result = repository.get(id);
Assertions.assertNotNull(result);
⋮----
Assertions.assertEquals(12, repository.getOffset());

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/util/HostUtilsTests.java
================
public class HostUtilsTests {
⋮----
public void testGetHostName() {
String hostName = HostUtils.getHostName();
System.out.println(hostName);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/RuntimeConfiguration.java
================
public class RuntimeConfiguration {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/TestUtils.java
================
public class TestUtils {
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static <E extends DomainEvent> void prepareExternalSchemas(SchemaRegistryClient src, List<Class<E>> externalDomainEvents) {
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_7, OptionPreset.PLAIN_JSON);
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS, JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
DomainEventInfo info = eventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(eventClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
⋮----
controlProducer.commitTransaction();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/WalletConfiguration.java
================
public class WalletConfiguration {
⋮----
public AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
return new AggregateBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient() {
return new MockSchemaRegistryClient();
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient src,
⋮----
return new KafkaSchemaRegistry(src, objectMapper);

================
File: main/runtime/src/test/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
akces.client.domainEventsPackage=org.elasticsoftware.akcestest.aggregate

================
File: main/runtime/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.kafka" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.gdpr" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.state" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.client" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: main/runtime/src/test/resources/test-application.yaml
================
spring:
  main:
    allow-bean-definition-overriding: true

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceRecord.java
================
) implements AkcesControlRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AkcesControlRecord.java
================
public sealed interface AkcesControlRecord permits AggregateServiceRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/errors/AggregateAlreadyExistsErrorEvent.java
================
) implements ErrorEvent {
⋮----
public String getAggregateId() {
return aggregateId();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/errors/CommandExecutionErrorEvent.java
================
) implements ErrorEvent {
⋮----
public String getAggregateId() {
return aggregateId();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataDeserializerModifier.java
================
public class PIIDataDeserializerModifier extends BeanDeserializerModifier {
private final PIIDataJsonDeserializer instance = new PIIDataJsonDeserializer();
⋮----
public BeanDeserializerBuilder updateBuilder(DeserializationConfig config,
⋮----
Iterator<SettableBeanProperty> it = builder.getProperties();
⋮----
while (it.hasNext()) {
SettableBeanProperty p = it.next();
if (p.getAnnotation(PIIData.class) != null) {
builder.addOrReplaceProperty(p.withValueDeserializer(instance), true);
⋮----
if (builder.getValueInstantiator() != null) {
SettableBeanProperty[] constructorArguments = builder.getValueInstantiator().getFromObjectArguments(config);
⋮----
if (constructorArguments[i].getAnnotation(PIIData.class) != null) {
updatedArguments[i] = constructorArguments[i].withValueDeserializer(instance);
⋮----
builder.setValueInstantiator(new PersonalDataValueInstantiator((StdValueInstantiator) builder.getValueInstantiator(), updatedArguments));
⋮----
static class PersonalDataValueInstantiator extends StdValueInstantiator {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataJsonDeserializer.java
================
public class PIIDataJsonDeserializer extends StringDeserializer {
⋮----
public String deserialize(JsonParser p, DeserializationContext ctxt) throws IOException {
String encryptedString = super.deserialize(p, ctxt);
GDPRContext gdprContext = GDPRContextHolder.getCurrentGDPRContext();
return gdprContext != null ? gdprContext.decrypt(encryptedString) : encryptedString;

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataJsonSerializer.java
================
public class PIIDataJsonSerializer extends JsonSerializer<Object> {
⋮----
public void serialize(Object value, JsonGenerator gen, SerializerProvider provider) throws IOException {
GDPRContext gdprContext = GDPRContextHolder.getCurrentGDPRContext();
⋮----
gen.writeString(gdprContext.encrypt((String) value));
⋮----
gen.writeString((String) value);
⋮----
public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint) throws JsonMappingException {
visitor.expectStringFormat(typeHint);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataSerializerModifier.java
================
public class PIIDataSerializerModifier extends BeanSerializerModifier {
private final PIIDataJsonSerializer instance = new PIIDataJsonSerializer();
⋮----
public List<BeanPropertyWriter> changeProperties(final SerializationConfig config,
⋮----
if (null == writer.getAnnotation(PIIData.class)) {
newWriters.add(writer);
⋮----
newWriters.add(new PersonalDataPropertyWriter(writer, instance));
⋮----
static class PersonalDataPropertyWriter extends BeanPropertyWriter {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRAnnotationUtils.java
================
public final class GDPRAnnotationUtils {
⋮----
public static Boolean hasPIIDataAnnotation(Class<?> type) {
return hasAnnotation(type, PIIData.class);
⋮----
public static Boolean hasAnnotation(Class<?> type, final Class<? extends Annotation> annotation) {
return hasAnnotationInternal(type, annotation, new HashSet<>());
⋮----
private static Boolean hasAnnotationInternal(Class<?> type, final Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
visitedClasses.add(type);
if (methodsHaveAnnotation(type, annotation, visitedClasses)) {
⋮----
if (constructorParametersHaveAnnotation(type, annotation, visitedClasses)) {
⋮----
if (fieldsHaveAnnotation(type, annotation, visitedClasses)) {
⋮----
private static boolean methodsHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
⋮----
if (Stream.of(klass.getDeclaredMethods()).anyMatch(method -> method.isAnnotationPresent(annotation) ||
(!BeanUtils.isSimpleValueType(method.getReturnType()) &&
!visitedClasses.contains(method.getReturnType()) &&
hasAnnotationInternal(method.getReturnType(), annotation, visitedClasses)))) {
⋮----
if (klass.isArray()) {
klass = klass.getComponentType();
⋮----
klass = klass.getSuperclass();
⋮----
private static boolean constructorParametersHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
return stream(type.getConstructors())
.flatMap(constructor -> stream(constructor.getParameters()))
.anyMatch(parameter -> parameter.isAnnotationPresent(annotation) ||
(!BeanUtils.isSimpleValueType(parameter.getType()) &&
!visitedClasses.contains(parameter.getType()) &&
hasAnnotationInternal(parameter.getType(), annotation, visitedClasses)));
⋮----
private static boolean fieldsHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
return Arrays.stream(type.getDeclaredFields())
.anyMatch(field -> field.isAnnotationPresent(annotation) ||
(!BeanUtils.isSimpleValueType(field.getType()) &&
!visitedClasses.contains(field.getType()) &&
hasAnnotationInternal(field.getType(), annotation, visitedClasses)));

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextHolder.java
================
public final class GDPRContextHolder {
⋮----
public static GDPRContext getCurrentGDPRContext() {
return currentContext.get();
⋮----
public static GDPRContext setCurrentGDPRContext(@Nullable GDPRContext gdprContext) {
final GDPRContext ctx = currentContext.get();
currentContext.set(gdprContext);
⋮----
public static GDPRContext resetCurrentGDPRContext() {
⋮----
currentContext.remove();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepository.java
================
public interface GDPRContextRepository extends Closeable {
default long getOffset() {
⋮----
void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture);
⋮----
void commit();
⋮----
void rollback();
⋮----
void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords);
⋮----
boolean exists(String aggregateId);
⋮----
GDPRContext get(String aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepositoryException.java
================
public class GDPRContextRepositoryException extends RuntimeException {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepositoryFactory.java
================
public interface GDPRContextRepositoryFactory {
GDPRContextRepository create(String runtimeName, Integer partitionId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRKeyUtils.java
================
public class GDPRKeyUtils {
⋮----
Pattern.compile("^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$");
⋮----
secureRandom = SecureRandom.getInstance("NativePRNGNonBlocking");
⋮----
secureRandom = new SecureRandom();
⋮----
public static SecretKeySpec createKey() {
⋮----
secureRandom.nextBytes(keyBytes);
return new SecretKeySpec(keyBytes, "AES");
⋮----
public static SecureRandom secureRandom() {
⋮----
public static boolean isUUID(String possibleUUID) {
return UUID_REGEX.matcher(possibleUUID).matches();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/InMemoryGDPRContextRepositoryFactory.java
================
public class InMemoryGDPRContextRepositoryFactory implements GDPRContextRepositoryFactory {
⋮----
public GDPRContextRepository create(String runtimeName, Integer partitionId) {
return new InMemoryGDPRContextRepository();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepositoryFactory.java
================
public class RocksDBGDPRContextRepositoryFactory implements GDPRContextRepositoryFactory {
⋮----
public GDPRContextRepository create(String runtimeName, Integer partitionId) {
return new RocksDBGDPRContextRepository(
⋮----
runtimeName + "-Akces-GDPRKeys-" + partitionId.toString(),
⋮----
serde.serializer(),
serde.deserializer());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/kafka/CustomKafkaConsumerFactory.java
================
public class CustomKafkaConsumerFactory<K, V> extends DefaultKafkaConsumerFactory<K, V> {
⋮----
protected Consumer<K, V> createKafkaConsumer(Map<String, Object> configProps) {
⋮----
configProps.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, configProps.get(ConsumerConfig.CLIENT_ID_CONFIG));
return super.createKafkaConsumer(configProps);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/kafka/CustomKafkaProducerFactory.java
================
public class CustomKafkaProducerFactory<K, V> extends DefaultKafkaProducerFactory<K, V> {
⋮----
protected Producer<K, V> createTransactionalProducer(String transactionId) {
⋮----
Producer<K, V> newProducer = createRawProducer(getTxProducerConfigs(transactionId));
⋮----
newProducer.initTransactions();
⋮----
newProducer.close(ProducerFactory.DEFAULT_PHYSICAL_CLOSE_TIMEOUT);
⋮----
KafkaException newEx = new KafkaException("initTransactions() failed and then close() failed", ex);
newEx.addSuppressed(ex2);
⋮----
throw new KafkaException("initTransactions() failed", ex);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/kafka/RecordAndMetadata.java
================


================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/AggregateStateRecord.java
================
) implements ProtocolRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/CommandRecord.java
================
) implements ProtocolRecord {
⋮----
this(UUID.randomUUID().toString(),

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/CommandResponseRecord.java
================
) implements ProtocolRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/DomainEventRecord.java
================
) implements ProtocolRecord {
⋮----
this(UUID.randomUUID().toString(), tenantId, name, version, payload, encoding, aggregateId, correlationId, generation);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/GDPRKeyRecord.java
================
) implements ProtocolRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/PayloadEncoding.java
================


================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/ProtocolRecord.java
================
public sealed interface ProtocolRecord permits AggregateStateRecord, CommandRecord, DomainEventRecord, GDPRKeyRecord, CommandResponseRecord {
String tenantId();
⋮----
String name();
⋮----
int version();
⋮----
byte[] payload();
⋮----
PayloadEncoding encoding();
⋮----
String aggregateId();
⋮----
String correlationId();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/IncompatibleSchemaException.java
================
public class IncompatibleSchemaException extends SchemaException {
⋮----
public int getSchemaVersion() {
⋮----
public List<Difference> getDifferences() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/InvalidSchemaVersionException.java
================
public class InvalidSchemaVersionException extends SchemaException {
⋮----
public int getSchemaVersion() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/PreviousSchemaVersionMissingException.java
================
public class PreviousSchemaVersionMissingException extends SchemaException {
⋮----
public int getSchemaVersion() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaException.java
================
public class SchemaException extends RuntimeException {
⋮----
public String getSchemaIdentifier() {
⋮----
public Class<?> getImplementationClass() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaVersionNotFoundException.java
================
public class SchemaVersionNotFoundException extends SchemaException {
⋮----
public int getSchemaVersion() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/serialization/BigDecimalSerializer.java
================
public final class BigDecimalSerializer extends StdSerializer<BigDecimal> {
⋮----
public void serialize(BigDecimal value, JsonGenerator gen, SerializerProvider provider) throws IOException {
gen.writeString(value.toPlainString());
⋮----
public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint)
⋮----
visitor.expectStringFormat(typeHint);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/EnvironmentPropertiesPrinter.java
================
public class EnvironmentPropertiesPrinter {
private static final Logger logger = LoggerFactory.getLogger(EnvironmentPropertiesPrinter.class);
⋮----
public void handleContextRefreshed(ContextRefreshedEvent event) {
ConfigurableEnvironment env = (ConfigurableEnvironment) event.getApplicationContext().getEnvironment();
logger.info("******* Environment Properties *******");
env.getPropertySources()
.stream()
.filter(ps -> ps instanceof MapPropertySource)
.map(ps -> ((MapPropertySource) ps).getSource().keySet())
.flatMap(Collection::stream)
.distinct()
.filter(key -> key.startsWith("akces.") ||
key.startsWith("spring.") ||
key.startsWith("management.") ||
key.startsWith("server."))
.sorted()
.forEach(key -> logger.info("{}={}", key, env.getProperty(key)));
logger.info("**************************************");

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/HostUtils.java
================
public class HostUtils {
⋮----
public static String getHostName() {
⋮----
String hostName = System.getenv("HOSTNAME");
⋮----
if (hostName == null || hostName.isEmpty()) {
⋮----
InetAddress addr = InetAddress.getLocalHost();
hostName = addr.getHostName();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/KafkaSender.java
================
public class KafkaSender {
⋮----
public static <K, V> Future<RecordMetadata> send(Producer<K, V> producer, ProducerRecord<K, V> producerRecord) {
return send(producer, producerRecord, null);
⋮----
public static <K, V> Future<RecordMetadata> send(Producer<K, V> producer, ProducerRecord<K, V> producerRecord, Callback callback) {
Future<RecordMetadata> future = producer.send(producerRecord, callback);
if (future.isDone()) {
⋮----
future.get();
⋮----
if (e.getCause() instanceof ApiException) {
throw (ApiException) e.getCause();
} else if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/KafkaUtils.java
================
public final class KafkaUtils {
⋮----
public static String getIndexTopicName(String indexName, String indexKey) {
⋮----
public static NewTopic createCompactedTopic(String name, int numPartitions, short replicationFactor) {
NewTopic topic = new NewTopic(name, numPartitions, replicationFactor);
⋮----
String minInSyncReplicas = String.valueOf(calculateQuorum(replicationFactor));
return topic.configs(Map.of(
⋮----
public static int calculateQuorum(short replicationFactor) {

================
File: main/shared/src/test/java/org/elasticsoftware/akces/gdpr/jackson/AkcesGDPRModuleTests.java
================
public class AkcesGDPRModuleTests {
⋮----
public void testVersion() {
Version version = AkcesGDPRModule.generateVersion(Semver.parse("0.11.0-SNAPSHOT"));
Assertions.assertNotEquals(Version.unknownVersion(), version);
Assertions.assertEquals(new Version(0, 11, 0, "SNAPSHOT", "org.elasticsoftwarefoundation.akces", "akces-runtime"), version);
⋮----
public void testVersionFromManifest() {
⋮----
AkcesGDPRModule akcesGDPRModule = new AkcesGDPRModule();
Version version = akcesGDPRModule.version();
Assertions.assertEquals(Version.unknownVersion(), version);

================
File: main/shared/src/test/java/org/elasticsoftware/akces/gdpr/GDPRContextTests.java
================
public class GDPRContextTests {
⋮----
public void testEncryptDecrypt() {
GDPRContext ctx = new EncryptingGDPRContext("ef234add-e0df-4769-b5f4-612a3207bad3", GDPRKeyUtils.createKey().getEncoded(), true);
String encryptedData = ctx.encrypt("test");
System.out.println(encryptedData);
String decryptedData = ctx.decrypt(encryptedData);
Assertions.assertEquals("test", decryptedData);
⋮----
public void testEncrypt() {
⋮----
String name = ctx.encrypt("Jasin Terlouw");
String street = ctx.encrypt("Gershwinstraat 125");
System.out.println("name: " + name);
System.out.println("street: " + street);
Assertions.assertEquals("Jasin Terlouw", ctx.decrypt(name));
Assertions.assertEquals("Gershwinstraat 125", ctx.decrypt(street));
⋮----
public void testEncryptForTests() {
⋮----
String firstName = ctx.encrypt("Fahim");
String lastName = ctx.encrypt("Zuijderwijk");
String email = ctx.encrypt("FahimZuijderwijk@jourrapide.com");
System.out.println(firstName);
System.out.println(lastName);
System.out.println(email);
Assertions.assertEquals("Fahim", ctx.decrypt(firstName));
Assertions.assertEquals("Zuijderwijk", ctx.decrypt(lastName));
Assertions.assertEquals("FahimZuijderwijk@jourrapide.com", ctx.decrypt(email));
⋮----
public void testBadPaddingException() {
⋮----
GDPRContext ctx = new EncryptingGDPRContext(aggregateId, key, true);
⋮----
ctx.decrypt(encrypted);
⋮----
String encryptedFirstName = ctx.encrypt("Fahim");
String descryptedFirstName = ctx.decrypt(encryptedFirstName);
⋮----
Assertions.assertEquals("Fahim", descryptedFirstName);
⋮----
public void testECBEncryption() {
⋮----
GDPRContext ctx = new EncryptingGDPRContext(aggregateId, GDPRKeyUtils.createKey().getEncoded(), false);
⋮----
public void testECBEncryptionWithOtherContext() {
⋮----
SecretKeySpec secretKey = GDPRKeyUtils.createKey();
GDPRContext one = new EncryptingGDPRContext(aggregateId, secretKey.getEncoded(), false);
GDPRContext two = new EncryptingGDPRContext(aggregateId, secretKey.getEncoded(), false);
⋮----
String encryptedFirstName = one.encrypt("Fahim");
String decryptedFirstName = two.decrypt(encryptedFirstName);
⋮----
Assertions.assertEquals("Fahim", decryptedFirstName);

================
File: main/shared/src/test/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepositoryTests.java
================
public class RocksDBGDPRContextRepositoryTests {
⋮----
public void testWriteInTransaction() {
ProtocolRecordSerde serde = new ProtocolRecordSerde();
GDPRContextRepository repository = new RocksDBGDPRContextRepository(
⋮----
serde.serializer(),
serde.deserializer());
⋮----
repository.process(List.of(new ConsumerRecord<>(
⋮----
new GDPRKeyRecord(
⋮----
GDPRKeyUtils.createKey().getEncoded()))));
⋮----
Assertions.assertTrue(repository.exists("4117b11f-3dde-4b71-b80c-fa20a12d9add"));

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/EventBridgeHandlerFunction.java
================
public interface EventBridgeHandlerFunction<S extends AggregateState,E extends DomainEvent> {
void apply(@NotNull E event, @NotNull CommandBus commandBus);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing EventBridgeHandlerFunction directly, you must override getEventType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing EventBridgeHandlerFunction directly, you must override getAggregate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/UpcastingHandlerFunction.java
================
public interface UpcastingHandlerFunction<T , R , TR extends ProtocolRecordType<T>, RR extends ProtocolRecordType<R>> {
R apply(T t);
⋮----
default TR getInputType() {
throw new UnsupportedOperationException("When implementing UpcastingHandlerFunction directly, you must override getInputType()");
⋮----
default RR getOutputType() {
throw new UnsupportedOperationException("When implementing UpcastingHandlerFunction directly, you must override getOutputType()");
⋮----
default Aggregate<? extends AggregateState> getAggregate() {
throw new UnsupportedOperationException("When implementing UpcastingHandlerFunction directly, you must override getAggregate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/AggregateStateInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/DatabaseModelEventHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/DatabaseModelInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/EventBridgeHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/UpcastingHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/commands/CommandBusHolder.java
================
public class CommandBusHolder {
⋮----
public static CommandBus getCommandBus(Class<? extends Aggregate> aggregateClass) {
return commandBusThreadLocal.get();

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/DatabaseModel.java
================
public interface DatabaseModel {
Map<String,Long> getOffsets(Set<String> partitionIds);
⋮----
Object startTransaction();
⋮----
void commitTransaction(Object transactionMarker, Map<String,Long> offsets);

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/DatabaseModelEventHandlerFunction.java
================
public interface DatabaseModelEventHandlerFunction<E extends DomainEvent> {
void accept(@NotNull E event);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getEventType()");
⋮----
default DatabaseModel getDatabaseModel() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getDatabaseModel()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModelStateType.java
================


================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClient.java
================
public interface AkcesClient {
⋮----
default CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nonnull Command command) {
return send(tenantId, null, command);
⋮----
default CompletionStage<List<DomainEvent>> send(@Nonnull Command command) {
return send(command, null);
⋮----
default CompletionStage<List<DomainEvent>> send(@Nonnull Command command, @Nullable String correlationId) {
return send(DEFAULT_TENANT_ID, correlationId, command);
⋮----
CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command);
⋮----
default void sendAndForget(@Nonnull Command command) {
sendAndForget(DEFAULT_TENANT_ID, null, command);
⋮----
default void sendAndForget(@Nonnull Command command, @Nullable String correlationId) {
sendAndForget(DEFAULT_TENANT_ID, correlationId, command);
⋮----
default void sendAndForget(@Nonnull String tenantId, @Nonnull Command command) {
sendAndForget(tenantId, null, command);
⋮----
void sendAndForget(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command);

================
File: main/eventcatalog/src/main/java/org/elasticsoftware/akces/eventcatalog/EventCatalogProcessor.java
================
public class EventCatalogProcessor extends AbstractProcessor {
⋮----
public boolean process(Set<? extends TypeElement> annotations, RoundEnvironment roundEnv) {
⋮----
String annotationName = annotation.getQualifiedName().toString();
if (CommandInfo.class.getCanonicalName().equals(annotationName)) {
processCommandInfoAnnotations(roundEnv);
} else if (DomainEventInfo.class.getCanonicalName().equals(annotationName)) {
processDomainEventInfoAnnotations(roundEnv);
} else if (AggregateInfo.class.getCanonicalName().equals(annotationName)) {
processAggregateInfoAnnotations(roundEnv);
} else if (CommandHandler.class.getCanonicalName().equals(annotationName)) {
processCommandHandlerAnnotations(roundEnv);
} else if (EventHandler.class.getCanonicalName().equals(annotationName)) {
processEventHandlerAnnotations(roundEnv);
⋮----
matchHandlersToAggregates();
⋮----
buildEventRelationships();
⋮----
String repoBaseUrl = processingEnv.getOptions().getOrDefault(
⋮----
if (!repoBaseUrl.endsWith("/")) {
⋮----
List<String> owners = Arrays.asList(processingEnv.getOptions().getOrDefault(
⋮----
).split("\\s*,\\s*"));
⋮----
String schemaDomain = processingEnv.getOptions().getOrDefault(
⋮----
generateCatalogEntries(repoBaseUrl, owners, schemaDomain);
⋮----
private void processCommandInfoAnnotations(RoundEnvironment roundEnv) {
Set<? extends Element> commandElements = roundEnv.getElementsAnnotatedWith(
processingEnv.getElementUtils().getTypeElement(CommandInfo.class.getCanonicalName())
⋮----
if (element.getKind() == ElementKind.CLASS || element.getKind() == ElementKind.RECORD) {
⋮----
CommandInfo commandInfo = typeElement.getAnnotation(CommandInfo.class);
commandCache.put(commandInfo, typeElement);
⋮----
processingEnv.getMessager().printMessage(
⋮----
"Found command: " + commandInfo.type() + "-v" + commandInfo.version() + " at " + typeElement.getQualifiedName()
⋮----
private void processDomainEventInfoAnnotations(RoundEnvironment roundEnv) {
Set<? extends Element> eventElements = roundEnv.getElementsAnnotatedWith(
processingEnv.getElementUtils().getTypeElement(DomainEventInfo.class.getCanonicalName())
⋮----
DomainEventInfo eventInfo = typeElement.getAnnotation(DomainEventInfo.class);
eventCache.put(eventInfo, typeElement);
⋮----
"Found event: " + eventInfo.type() + "-v" + eventInfo.version() + " at " + typeElement.getQualifiedName()
⋮----
private void processAggregateInfoAnnotations(RoundEnvironment roundEnv) {
Set<? extends Element> aggregateElements = roundEnv.getElementsAnnotatedWith(
processingEnv.getElementUtils().getTypeElement(AggregateInfo.class.getCanonicalName())
⋮----
if (element.getKind() == ElementKind.CLASS) {
⋮----
AggregateInfo aggregateInfo = typeElement.getAnnotation(AggregateInfo.class);
String id = aggregateInfo.value().isEmpty() ? typeElement.getSimpleName().toString() : aggregateInfo.value();
aggregateCache.put(aggregateInfo, typeElement);
⋮----
"Found aggregate: " + id + " at " + typeElement.getQualifiedName()
⋮----
private void processCommandHandlerAnnotations(RoundEnvironment roundEnv) {
Set<? extends Element> handlerElements = roundEnv.getElementsAnnotatedWith(
processingEnv.getElementUtils().getTypeElement(CommandHandler.class.getCanonicalName())
⋮----
if (element.getKind() == ElementKind.METHOD) {
CommandHandler handlerInfo = element.getAnnotation(CommandHandler.class);
commandHandlerCache.put(handlerInfo, element);
⋮----
"Found command handler at " + element.getEnclosingElement() + "." + element
⋮----
private void processEventHandlerAnnotations(RoundEnvironment roundEnv) {
⋮----
processingEnv.getElementUtils().getTypeElement(EventHandler.class.getCanonicalName())
⋮----
EventHandler handlerInfo = element.getAnnotation(EventHandler.class);
eventHandlerCache.put(handlerInfo, element);
⋮----
"Found event handler at " + element.getEnclosingElement() + "." + element
⋮----
private void matchHandlersToAggregates() {
⋮----
for (Map.Entry<AggregateInfo, TypeElement> entry : aggregateCache.entrySet()) {
typeToAggregate.put(entry.getValue(), entry.getKey());
⋮----
for (Map.Entry<CommandHandler, Element> entry : commandHandlerCache.entrySet()) {
CommandHandler handler = entry.getKey();
Element methodElement = entry.getValue();
Element enclosingClass = methodElement.getEnclosingElement();
⋮----
if (enclosingClass.getKind() == ElementKind.CLASS) {
⋮----
AggregateInfo aggregateInfo = typeToAggregate.get(enclosingType);
⋮----
.computeIfAbsent(aggregateInfo, k -> new ArrayList<>())
.add(handler);
⋮----
enclosingType.getQualifiedName()
⋮----
for (Map.Entry<EventHandler, Element> entry : eventHandlerCache.entrySet()) {
EventHandler handler = entry.getKey();
⋮----
private void buildEventRelationships() {
⋮----
for (Map.Entry<CommandInfo, TypeElement> entry : commandCache.entrySet()) {
TypeElement typeElement = entry.getValue();
AnnotationMirror annotationMirror = getAnnotationMirror(typeElement, CommandInfo.class.getCanonicalName());
⋮----
commandAnnotations.put(typeElement, annotationMirror);
⋮----
for (Map.Entry<DomainEventInfo, TypeElement> entry : eventCache.entrySet()) {
⋮----
AnnotationMirror annotationMirror = getAnnotationMirror(typeElement, DomainEventInfo.class.getCanonicalName());
⋮----
eventAnnotations.put(typeElement, annotationMirror);
⋮----
processCommandHandlers();
⋮----
processEventHandlers();
⋮----
logAggregateRelationships();
⋮----
private void processCommandHandlers() {
⋮----
for (Map.Entry<AggregateInfo, List<CommandHandler>> entry : aggregateToCommandHandlers.entrySet()) {
AggregateInfo aggregateInfo = entry.getKey();
List<CommandHandler> handlers = entry.getValue();
⋮----
Set<TypeElement> producedEvents = aggregateProducedEvents.computeIfAbsent(
⋮----
Set<TypeElement> errorEvents = aggregateErrorEvents.computeIfAbsent(
⋮----
Set<TypeElement> handledCommands = aggregateHandledCommands.computeIfAbsent(
⋮----
Element methodElement = commandHandlerCache.get(handler);
⋮----
List<? extends VariableElement> parameters = executableElement.getParameters();
if (!parameters.isEmpty()) {
VariableElement firstParam = parameters.getFirst();
TypeMirror commandTypeMirror = firstParam.asType();
TypeElement commandTypeElement = getTypeElementFromTypeMirror(commandTypeMirror);
⋮----
handledCommands.add(commandTypeElement);
⋮----
AnnotationMirror handlerMirror = getAnnotationMirror(methodElement, CommandHandler.class.getCanonicalName());
⋮----
List<TypeElement> produces = getClassArrayFromAnnotation(handlerMirror, "produces");
⋮----
producedEvents.addAll(produces);
⋮----
List<TypeElement> errors = getClassArrayFromAnnotation(handlerMirror, "errors");
⋮----
errorEvents.addAll(errors);
⋮----
private void processEventHandlers() {
⋮----
for (Map.Entry<AggregateInfo, List<EventHandler>> entry : aggregateToEventHandlers.entrySet()) {
⋮----
List<EventHandler> handlers = entry.getValue();
⋮----
Set<TypeElement> handledEvents = aggregateHandledEvents.computeIfAbsent(
⋮----
Element methodElement = eventHandlerCache.get(handler);
⋮----
TypeMirror eventTypeMirror = firstParam.asType();
TypeElement eventTypeElement = getTypeElementFromTypeMirror(eventTypeMirror);
⋮----
handledEvents.add(eventTypeElement);
⋮----
AnnotationMirror handlerMirror = getAnnotationMirror(methodElement, EventHandler.class.getCanonicalName());
⋮----
private void logAggregateRelationships() {
⋮----
for (AggregateInfo aggregateInfo : aggregateCache.keySet()) {
TypeElement aggregateType = aggregateCache.get(aggregateInfo);
String aggregateName = aggregateType.getSimpleName().toString();
⋮----
Set<TypeElement> commands = aggregateHandledCommands.getOrDefault(aggregateInfo, Collections.emptySet());
if (!commands.isEmpty()) {
StringBuilder commandsInfo = new StringBuilder();
⋮----
commandsInfo.append(command.getSimpleName());
⋮----
AnnotationMirror cmdAnnotation = commandAnnotations.get(command);
⋮----
String type = getAnnotationValue(cmdAnnotation, "type");
String version = getAnnotationValue(cmdAnnotation, "version");
commandsInfo.append("[type=").append(type)
.append(", v=").append(version).append("] ");
⋮----
commandsInfo.append(" ");
⋮----
private AnnotationMirror getAnnotationMirror(Element element, String annotationClassName) {
for (AnnotationMirror mirror : element.getAnnotationMirrors()) {
String annotationName = mirror.getAnnotationType().toString();
if (annotationClassName.equals(annotationName)) {
⋮----
private List<TypeElement> getClassArrayFromAnnotation(AnnotationMirror annotationMirror, String elementName) {
⋮----
annotationMirror.getElementValues().entrySet()) {
⋮----
if (elementName.equals(entry.getKey().getSimpleName().toString())) {
AnnotationValue annotationValue = entry.getValue();
if (annotationValue.getValue() instanceof List<?> valueList) {
⋮----
if (av.getValue() instanceof DeclaredType declaredType) {
TypeElement typeElement = (TypeElement) declaredType.asElement();
result.add(typeElement);
⋮----
private TypeElement getClassFromAnnotation(AnnotationMirror annotationMirror, String elementName) {
⋮----
Object value = annotationValue.getValue();
⋮----
return (TypeElement) declaredType.asElement();
⋮----
private String getAnnotationValue(AnnotationMirror annotationMirror, String elementName) {
⋮----
return entry.getValue().getValue().toString();
⋮----
/**
     * Helper method to get a TypeElement from a TypeMirror.
     */
private TypeElement getTypeElementFromTypeMirror(TypeMirror typeMirror) {
⋮----
Element element = declaredType.asElement();
⋮----
"Could not get TypeElement for: " + typeMirror + " - " + e.getMessage()
⋮----
private void generateCatalogEntries(String repositoryBaseUrl, List<String> owners, String schemaDomain) {
// For each aggregate, find its domain and create service documentation
⋮----
TypeElement aggregateType = entry.getValue();
⋮----
String aggregateName = aggregateInfo.value().isEmpty() ?
aggregateType.getSimpleName().toString() : aggregateInfo.value();
⋮----
// get the AggregateStateInfo annotation from the aggregateInfo.stateClass
AnnotationMirror aggregateInfoMirror = getAnnotationMirror(aggregateType, AggregateInfo.class.getCanonicalName());
// get the stateClass type from the mirror
TypeElement stateClass = getClassFromAnnotation(requireNonNull(aggregateInfoMirror), "stateClass");
AggregateStateInfo stateInfo = requireNonNull(stateClass).getAnnotation(AggregateStateInfo.class);
⋮----
String serviceDoc = ServiceTemplateGenerator.generate(new ServiceTemplateGenerator.ServiceMetadata(
⋮----
stateInfo.version()+".0.0",
⋮----
aggregateInfo.description().isBlank() ? aggregateName + " Aggregate" : aggregateInfo.description(),
⋮----
Stream.concat(
aggregateHandledCommands.getOrDefault(aggregateInfo, Collections.emptySet()).stream()
.map(typeElement -> typeElement.getAnnotation(CommandInfo.class))
.filter(Objects::nonNull)
.map(annotation -> new ServiceTemplateGenerator.Message(
annotation.type(),
annotation.version() + ".0.0")),
aggregateHandledEvents.getOrDefault(aggregateInfo, Collections.emptySet()).stream()
.map(typeElement -> typeElement.getAnnotation(DomainEventInfo.class))
⋮----
annotation.version() + ".0.0"))
).toList(),
⋮----
aggregateProducedEvents.getOrDefault(aggregateInfo, Collections.emptySet()).stream()
⋮----
aggregateErrorEvents.getOrDefault(aggregateInfo, Collections.emptySet()).stream()
⋮----
repositoryBaseUrl + aggregateType.getQualifiedName().toString().replace('.', '/') + ".java"));
⋮----
writeToEventCatalog(servicePath, "index.mdx", serviceDoc);
⋮----
CommandInfo commandInfo = entry.getKey();
TypeElement commandType = entry.getValue();
⋮----
for (Map.Entry<AggregateInfo, Set<TypeElement>> aggEntry : aggregateHandledCommands.entrySet()) {
if (aggEntry.getValue().contains(commandType)) {
AggregateInfo aggregateInfo = aggEntry.getKey();
String aggregateName = aggregateInfo.value();
⋮----
String commandName = commandInfo.type();
⋮----
String commandDoc = MessageTemplateGenerator.generate(new MessageTemplateGenerator.EventMetadata(
commandInfo.type(),
⋮----
commandInfo.version() + ".0.0",
commandInfo.description().isBlank() ? commandInfo.type() + " Command" : commandInfo.description(),
⋮----
repositoryBaseUrl + commandType.getQualifiedName().toString().replace('.', '/') + ".java"
⋮----
writeToEventCatalog(commandPath, "index.mdx", commandDoc);
⋮----
String schema = JsonSchemaGenerator.generate(processingEnv, commandType, "akces-command:" + schemaDomain , commandInfo.type(), commandInfo.version());
⋮----
writeToEventCatalog(commandPath, "schema.json", schema);
⋮----
DomainEventInfo eventInfo = entry.getKey();
TypeElement eventType = entry.getValue();
⋮----
for (Map.Entry<AggregateInfo, Set<TypeElement>> aggEntry : aggregateProducedEvents.entrySet()) {
if (aggEntry.getValue().contains(eventType)) {
⋮----
String eventName = eventInfo.type();
String versionSuffix = eventInfo.version() > 1 ? "-v" + eventInfo.version() : "";
⋮----
String eventDoc = MessageTemplateGenerator.generate(new MessageTemplateGenerator.EventMetadata(
eventInfo.type(),
⋮----
eventInfo.version() + ".0.0",
eventInfo.description().isBlank() ? eventInfo.type() + " DomainEvent" : eventInfo.description(),
⋮----
repositoryBaseUrl + eventType.getQualifiedName().toString().replace('.', '/') + ".java"
⋮----
writeToEventCatalog(eventPath, "index.mdx", eventDoc);
⋮----
String schema = JsonSchemaGenerator.generate(processingEnv, eventType,  "akces-domainevent:" + schemaDomain, eventInfo.type(), eventInfo.version());
⋮----
writeToEventCatalog(eventPath, "schema.json", schema);
⋮----
private void writeToEventCatalog(String path, String fileName, String content) {
⋮----
FileObject resourceFile = processingEnv.getFiler().createResource(
⋮----
try (Writer writer = resourceFile.openWriter()) {
writer.write(content);
⋮----
"Error writing file " + path + "/" + fileName + ": " + e.getMessage()

================
File: main/eventcatalog/src/main/java/org/elasticsoftware/akces/eventcatalog/JsonSchemaGenerator.java
================
public class JsonSchemaGenerator {
⋮----
public static String generate(ProcessingEnvironment processingEnv,
⋮----
schema.put("$schema", "http://json-schema.org/draft-07/schema#");
schema.put("id", "urn:" + category + "." + type + ":" + version);
schema.put("type", "object");
⋮----
extractProperties(processingEnv, typeElement, requiredProps, properties);
⋮----
schema.put("properties", properties);
⋮----
if (!requiredProps.isEmpty()) {
schema.put("required", requiredProps);
⋮----
schema.put("additionalProperties", false);
⋮----
return mapToJson(schema);
⋮----
processingEnv.getMessager().printMessage(Diagnostic.Kind.ERROR,
"Error generating JSON schema for " + typeElement + ": " + e.getMessage(),
⋮----
private static void extractProperties(ProcessingEnvironment processingEnv, TypeElement typeElement, List<String> requiredProps, Map<String, Object> properties) {
for (Element enclosedElement : typeElement.getEnclosedElements()) {
if (enclosedElement.getKind() == ElementKind.FIELD) {
⋮----
Set<Modifier> modifiers = field.getModifiers();
if (modifiers.contains(Modifier.STATIC) || modifiers.contains(Modifier.TRANSIENT)) {
⋮----
String fieldName = field.getSimpleName().toString();
TypeMirror fieldType = field.asType();
⋮----
boolean required = hasRequiredAnnotation(field);
⋮----
requiredProps.add(fieldName);
⋮----
Map<String, Object> fieldSchema = mapTypeToJsonSchema(fieldType, required, processingEnv);
⋮----
String docComment = processingEnv.getElementUtils().getDocComment(field);
if (docComment != null && !docComment.isEmpty()) {
fieldSchema.put("description", docComment.trim());
⋮----
properties.put(fieldName, fieldSchema);
⋮----
private static Map<String, Object> mapTypeToJsonSchema(TypeMirror typeMirror, boolean required, ProcessingEnvironment processingEnv) {
⋮----
if (typeMirror.getKind().isPrimitive()) {
switch (typeMirror.getKind()) {
⋮----
schema.put("type", "boolean");
⋮----
schema.put("type", "integer");
⋮----
schema.put("type", "number");
⋮----
schema.put("type", "string");
⋮----
String typeName = getQualifiedClassName(typeMirror, processingEnv);
if (typeName.equals("java.lang.String")) {
schema.put("type", required ? "string" : List.of("string", "null"));
} else if (typeName.equals("java.lang.Boolean")) {
schema.put("type", required ? "boolean" : List.of("boolean", "null"));
} else if (typeName.equals("java.lang.Integer") ||
typeName.equals("java.lang.Long") ||
typeName.equals("java.lang.Short") ||
typeName.equals("java.lang.Byte")) {
schema.put("type", required ? "integer" : List.of("integer", "null"));
} else if (typeName.equals("java.lang.Float") ||
typeName.equals("java.lang.Double")) {
schema.put("type", required ? "number" : List.of("number", "null"));
} else if (typeName.equals("java.util.UUID")) {
⋮----
schema.put("format", "uuid");
} else if (typeName.equals("java.math.BigDecimal")) {
⋮----
} else if (typeName.startsWith("java.time.")) {
⋮----
if (typeName.contains("LocalDate")) {
schema.put("format", "date");
} else if (typeName.contains("LocalTime")) {
schema.put("format", "time");
⋮----
schema.put("format", "date-time");
⋮----
} else if (typeName.startsWith("java.util.List") ||
typeName.startsWith("java.util.ArrayList") ||
typeName.startsWith("java.util.Collection")) {
schema.put("type", required ? "array" : List.of("array", "null"));
⋮----
if (typeMirror instanceof DeclaredType declaredType && !declaredType.getTypeArguments().isEmpty()) {
TypeMirror itemType = declaredType.getTypeArguments().getFirst();
schema.put("items", mapTypeToJsonSchema(itemType, true, processingEnv));
⋮----
anyType.put("type", "object");
schema.put("items", anyType);
⋮----
} else if (typeName.startsWith("java.util.Map") ||
typeName.startsWith("java.util.HashMap")) {
schema.put("type", required ? "object" : List.of("object", "null"));
schema.put("additionalProperties", true);
} else if (typeName.startsWith("java.util.Set") ||
typeName.startsWith("java.util.HashSet")) {
⋮----
schema.put("uniqueItems", true);
⋮----
TypeElement typeElement = getTypeElementFromTypeMirror(typeMirror, processingEnv);
if (typeElement != null && typeElement.getKind() == ElementKind.ENUM) {
⋮----
if (enclosedElement.getKind() == ElementKind.ENUM_CONSTANT) {
enumValues.add(enclosedElement.getSimpleName().toString());
⋮----
if (!enumValues.isEmpty()) {
schema.put("enum", enumValues);
⋮----
private static String getQualifiedClassName(TypeMirror typeMirror, ProcessingEnvironment processingEnv) {
⋮----
Element element = declaredType.asElement();
⋮----
return typeElement.getQualifiedName().toString();
⋮----
TypeMirror erasure = processingEnv.getTypeUtils().erasure(typeMirror);
⋮----
return typeMirror.toString();
⋮----
private static boolean hasRequiredAnnotation(Element element) {
⋮----
List<String> requiredAnnotations = List.of(
⋮----
if (element.getAnnotationMirrors().stream()
.anyMatch(am -> am.getAnnotationType().toString().equals(annotationName))) {
⋮----
private static String mapToJson(Map<String, Object> map) {
⋮----
StringBuilder json = new StringBuilder();
json.append("{\n");
⋮----
Iterator<Map.Entry<String, Object>> it = map.entrySet().iterator();
while (it.hasNext()) {
Map.Entry<String, Object> entry = it.next();
json.append("  \"").append(entry.getKey()).append("\": ");
appendValue(json, entry.getValue(), 2);
⋮----
if (it.hasNext()) {
json.append(",");
⋮----
json.append("\n");
⋮----
json.append("}");
return json.toString();
⋮----
private static void appendValue(StringBuilder json, Object value, int indent) {
⋮----
json.append("null");
⋮----
json.append("\"").append(escapeJsonString(value.toString())).append("\"");
⋮----
json.append(value);
⋮----
if (list.isEmpty()) {
json.append("[]");
⋮----
json.append("[\n");
String indentStr = " ".repeat(indent);
⋮----
for (int i = 0; i < list.size(); i++) {
json.append(indentStr).append("  ");
appendValue(json, list.get(i), indent + 2);
⋮----
if (i < list.size() - 1) {
⋮----
json.append(indentStr).append("]");
⋮----
if (map.isEmpty()) {
json.append("{}");
⋮----
json.append(indentStr).append("  \"").append(entry.getKey()).append("\": ");
appendValue(json, entry.getValue(), indent + 2);
⋮----
json.append(indentStr).append("}");
⋮----
private static String escapeJsonString(String input) {
⋮----
StringBuilder escaped = new StringBuilder();
for (int i = 0; i < input.length(); i++) {
char ch = input.charAt(i);
⋮----
escaped.append("\\\"");
⋮----
escaped.append("\\\\");
⋮----
escaped.append("\\b");
⋮----
escaped.append("\\f");
⋮----
escaped.append("\\n");
⋮----
escaped.append("\\r");
⋮----
escaped.append("\\t");
⋮----
String hex = Integer.toHexString(ch);
escaped.append("\\u");
⋮----
escaped.append("0".repeat(4 - hex.length()));
escaped.append(hex);
⋮----
escaped.append(ch);
⋮----
return escaped.toString();
⋮----
private static TypeElement getTypeElementFromTypeMirror(TypeMirror typeMirror, ProcessingEnvironment processingEnv) {
⋮----
processingEnv.getMessager().printMessage(
⋮----
"Could not get TypeElement for: " + typeMirror + " - " + e.getMessage()

================
File: main/eventcatalog/src/main/java/org/elasticsoftware/akces/eventcatalog/MessageTemplateGenerator.java
================
public class MessageTemplateGenerator {
⋮----
public static String generate(EventMetadata event) {
⋮----
StringBuilder ownersBuilder = new StringBuilder();
for (String owner : event.owners()) {
ownersBuilder.append("    - ").append(owner).append("\n");
⋮----
String ownersList = ownersBuilder.toString().stripTrailing();
⋮----
return String.format(
⋮----
event.id(),
event.name(),
event.version(),
event.summary(),
⋮----
event.language(),
event.repositoryUrl()

================
File: main/eventcatalog/src/main/java/org/elasticsoftware/akces/eventcatalog/ServiceTemplateGenerator.java
================
public class ServiceTemplateGenerator {
⋮----
public static String generate(ServiceMetadata service) {
⋮----
StringBuilder ownersBuilder = new StringBuilder();
for (String owner : service.owners()) {
ownersBuilder.append("    - ").append(owner).append("\n");
⋮----
String ownersList = ownersBuilder.toString().stripTrailing();
⋮----
StringBuilder receivesBuilder = new StringBuilder();
if (service.receives().isEmpty()) {
receivesBuilder.append("  []");
⋮----
for (Message command : service.receives()) {
receivesBuilder.append("  - id: ").append(command.id()).append("\n");
receivesBuilder.append("    version: ").append(command.version()).append("\n");
⋮----
String receivesList = receivesBuilder.toString().stripTrailing();
⋮----
StringBuilder sendsBuilder = new StringBuilder();
if (service.sends().isEmpty()) {
sendsBuilder.append("  []");
⋮----
for (Message event : service.sends()) {
sendsBuilder.append("  - id: ").append(event.id()).append("\n");
sendsBuilder.append("    version: ").append(event.version()).append("\n");
⋮----
String sendsList = sendsBuilder.toString().stripTrailing();
⋮----
.replace("#{service.id}", service.id())
.replace("#{service.version}", service.version())
.replace("#{service.name}", service.name())
.replace("#{service.summary}", service.summary())
.replace("#{ownersList}", ownersList)
.replace("#{receivesList}", receivesList)
.replace("#{sendsList}", sendsList)
.replace("#{service.language}", service.language())
.replace("#{service.repositoryUrl}", service.repositoryUrl());

================
File: main/eventcatalog/src/test/java/org/elasticsoftware/akces/eventcatalog/EventCatalogProcessorTest.java
================
public class EventCatalogProcessorTest {
⋮----
void testProcessorWithSingleAggregate() {
⋮----
JavaFileObject aggregateFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject aggregateStateFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject commandFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject eventFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject eventFileV2 = JavaFileObjects.forSourceString(
⋮----
Compilation compilation = javac()
.withProcessors(new EventCatalogProcessor())
.compile(aggregateFile,aggregateStateFile, commandFile, eventFile, eventFileV2);
⋮----
assertThat(compilation).succeeded();
⋮----
assertThat(compilation)
.hadNoteContaining("Found aggregate: Account");
⋮----
.hadNoteContaining("Found command: CreateAccount");
⋮----
.hadNoteContaining("Found event: AccountCreated");
⋮----
.hadNoteContaining("Found command handler");
⋮----
.hadNoteContaining("Matched command handler");
⋮----
.hadNoteContaining("Aggregate Account handles commands");
⋮----
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/Account/commands/CreateAccount/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/Account/commands/CreateAccount/schema.json");
⋮----
compilation.generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/AccountAggregate/commands/CreateAccount/index.mdx")
.ifPresent(file -> {
⋮----
String content = file.getCharContent(true).toString();
System.out.println(content);
⋮----
System.err.println("Error reading file content: " + e.getMessage());
⋮----
compilation.generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/AccountAggregate/commands/CreateAccount/schema.json")
⋮----
void testProcessorWithMultipleAggregates() throws IOException {
⋮----
JavaFileObject accountAggregateFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject accountAggregateStateFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject accountCommandFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject accountEventFile = JavaFileObjects.forSourceString(
⋮----
JavaFileObject accountEventFileV2 = JavaFileObjects.forSourceString(
⋮----
JavaFileObject orderProcessFile = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.OrderProcess", """
⋮----
JavaFileObject buyOrderProcessFile = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.BuyOrderProcess", """
⋮----
JavaFileObject orderProcessManager = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.OrderProcessManager", """
⋮----
JavaFileObject orderProcessManagerState = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.OrderProcessManagerState", """
⋮----
JavaFileObject fxMarket = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.FxMarket", """
⋮----
JavaFileObject buyOrderCreatedEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.BuyOrderCreatedEvent", """
⋮----
JavaFileObject buyOrderPlacedEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.BuyOrderPlacedEvent", """
⋮----
JavaFileObject buyOrderRejectedEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.BuyOrderRejectedEvent", """
⋮----
JavaFileObject insufficientFundsErrorEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.wallet.InsufficientFundsErrorEvent", """
⋮----
JavaFileObject invalidCurrencyErrorEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.wallet.InvalidCurrencyErrorEvent", """
⋮----
JavaFileObject placeBuyOrderCommand = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.PlaceBuyOrderCommand", """
⋮----
JavaFileObject userOrderProcessesCreatedEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.orders.UserOrderProcessesCreatedEvent", """
⋮----
JavaFileObject amountReservedEvent = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.wallet.AmountReservedEvent", """
⋮----
JavaFileObject reserverAmountCommand = JavaFileObjects.forSourceString("org.elasticsoftware.akcestest.aggregate.wallet.ReserveAmountCommand", """
⋮----
.compile(accountAggregateFile, accountAggregateStateFile, accountCommandFile, accountEventFile, accountEventFileV2,
⋮----
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/Account/events/AccountCreated/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/Account/events/AccountCreated/schema.json");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/commands/PlaceBuyOrder/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/commands/PlaceBuyOrder/schema.json");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderCreated/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderCreated/schema.json");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderPlaced/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderPlaced/schema.json");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderRejected/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderRejected/schema.json");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/UserOrderProcessesCreated/index.mdx");
assertThat(compilation).generatedFile(StandardLocation.CLASS_OUTPUT, "META-INF/eventcatalog/services/OrderProcessManager/events/UserOrderProcessesCreated/schema.json");
⋮----
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
⋮----
validateGeneratedSchema(compilation, objectMapper,"META-INF/eventcatalog/services/Account/commands/CreateAccount/schema.json","{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"properties\":{\"country\":{\"type\":\"string\"},\"email\":{\"type\":\"string\"},\"firstName\":{\"type\":\"string\"},\"lastName\":{\"type\":\"string\"},\"userId\":{\"type\":\"string\"}},\"required\":[\"country\",\"email\",\"firstName\",\"lastName\",\"userId\"],\"additionalProperties\":false}");
validateGeneratedSchema(compilation, objectMapper, "META-INF/eventcatalog/services/OrderProcessManager/events/BuyOrderCreated/schema.json", "{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"properties\":{\"clientReference\":{\"type\":\"string\"},\"limitPrice\":{\"type\":\"string\"},\"market\":{\"type\":\"object\",\"properties\":{\"baseCurrency\":{\"type\":[\"string\",\"null\"]},\"id\":{\"type\":[\"string\",\"null\"]},\"quoteCurrency\":{\"type\":[\"string\",\"null\"]}},\"additionalProperties\":false},\"orderId\":{\"type\":\"string\"},\"quantity\":{\"type\":\"string\"},\"userId\":{\"type\":\"string\"}},\"required\":[\"clientReference\",\"limitPrice\",\"market\",\"orderId\",\"quantity\",\"userId\"],\"additionalProperties\":false}");
⋮----
private void validateGeneratedSchema(Compilation compilation, ObjectMapper objectMapper, String schemaLocation, String schemaRegistrySchemaString) throws IOException {
JsonNode schemaRegistrySchema = objectMapper.readTree(schemaRegistrySchemaString);
String eventCatalogSchemaString = compilation.generatedFile(StandardLocation.CLASS_OUTPUT, schemaLocation).orElseThrow().getCharContent(true).toString();
System.out.println(eventCatalogSchemaString);
JsonNode eventCatalogSchema = objectMapper.readTree(eventCatalogSchemaString);
⋮----
List<Difference> differences = SchemaDiff.compare(new JsonSchema(schemaRegistrySchema).rawSchema(), new JsonSchema(eventCatalogSchema).rawSchema());
⋮----
Assertions.assertTrue(differences.isEmpty(), "Schema differences found: " + differences);
⋮----
public void testSchemaWithEnumAndList() {

================
File: main/eventcatalog/src/test/java/org/elasticsoftware/akces/eventcatalog/MessageTemplateGeneratorTests.java
================
class MessageTemplateGeneratorTests {
⋮----
void shouldGenerateEventTemplate() {
⋮----
List.of(
⋮----
String result = MessageTemplateGenerator.generate(accountCreated);
⋮----
assertEquals(expected, result);
⋮----
void shouldHandleMultipleOwners() {
⋮----
String result = MessageTemplateGenerator.generate(event);

================
File: main/eventcatalog/src/test/java/org/elasticsoftware/akces/eventcatalog/ServiceTemplateGeneratorTests.java
================
public class ServiceTemplateGeneratorTests {
⋮----
void shouldGenerateServiceTemplate() {
⋮----
List.of(
⋮----
List.of(new ServiceTemplateGenerator.Message("CreateAccount", "1.0.0")),
List.of(new ServiceTemplateGenerator.Message("AccountCreated", "1.0.0")),
⋮----
String result = ServiceTemplateGenerator.generate(accountAggregate);
⋮----
assertTrue(result.contains("id: AccountAggregate"));
assertTrue(result.contains("version: 1.0.0"));
assertTrue(result.contains("name: Account Aggregate"));
assertTrue(result.contains("Core aggregate that manages user account lifecycle"));
⋮----
assertTrue(result.contains("- jwijgerd"));
assertTrue(result.contains("- framework-developers"));
⋮----
assertTrue(result.contains("- id: CreateAccount"));
assertTrue(result.contains("- id: AccountCreated"));
⋮----
assertTrue(result.contains("language: Java"));
assertTrue(result.contains("url: https://github.com/elasticsoftwarefoundation/akces-framework/test-apps/crypto-trading/eventcatalog/domains/Accounts/services/AccountAggregate"));
⋮----
void shouldHandleEmptyLists() {
⋮----
List.of("tester"),
List.of(),
⋮----
String result = ServiceTemplateGenerator.generate(emptyListsAggregate);
⋮----
assertTrue(result.contains("receives:\n  []"));
assertTrue(result.contains("sends:\n  []"));
⋮----
void shouldGenerateCorrectYamlFormat() {
⋮----
List.of(new ServiceTemplateGenerator.Message("TestCommand", "1.0.0")),
List.of(new ServiceTemplateGenerator.Message("TestEvent", "1.0.0")),
⋮----
String result = ServiceTemplateGenerator.generate(minimalAggregate);
⋮----
assertEquals(expected, result);
⋮----
void shouldHandleMultipleReceivesAndSends() {
⋮----
String result = ServiceTemplateGenerator.generate(multiCommandAggregate);

================
File: main/eventcatalog/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.9.1-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-eventcatalog</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: EventCatalog Annotation Processor</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-expression</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.auto.service</groupId>
            <artifactId>auto-service</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>jakarta.annotation</groupId>
            <artifactId>jakarta.annotation-api</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.google.testing.compile</groupId>
            <artifactId>compile-testing</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-web</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
        </plugins>
    </build>
</project>

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/beans/DatabaseModelBeanFactoryPostProcessor.java
================
public class DatabaseModelBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
⋮----
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
⋮----
Arrays.asList(beanFactory.getBeanNamesForAnnotation(DatabaseModelInfo.class)).forEach(beanName -> {
BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
⋮----
Class<?> queryModelClass = Class.forName(bd.getBeanClassName());
List<Method> queryModelEventHandlers = Arrays.stream(queryModelClass.getMethods())
.filter(method -> method.isAnnotationPresent(DatabaseModelEventHandler.class))
.toList();
queryModelEventHandlers.forEach(eventHandlerMethod -> processDatabaseModelEventHandler(beanName, eventHandlerMethod, bdr));
⋮----
throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
⋮----
bdr.registerBeanDefinition(beanName + "DatabaseModelRuntime",
BeanDefinitionBuilder.genericBeanDefinition(DatabaseModelRuntimeFactory.class)
.addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
.addConstructorArgReference("akcesDatabaseModelSchemaRegistry")
.addConstructorArgReference(beanName)
.getBeanDefinition());
bdr.registerBeanDefinition(beanName + "AkcesDatabaseModelController",
BeanDefinitionBuilder.genericBeanDefinition(AkcesDatabaseModelController.class)
.addConstructorArgReference("akcesDatabaseModelConsumerFactory")
.addConstructorArgReference("akcesDatabaseModelControlConsumerFactory")
.addConstructorArgReference("akcesDatabaseModelGDPRContextRepositoryFactory")
.addConstructorArgReference(beanName + "DatabaseModelRuntime")
.setInitMethodName("start")
.setDestroyMethodName("close")
⋮----
throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
⋮----
private void processDatabaseModelEventHandler(String aggregateBeanName, Method databaseModelEventHandlerMethod, BeanDefinitionRegistry bdr) {
if (databaseModelEventHandlerMethod.getParameterCount() == 1 &&
DomainEvent.class.isAssignableFrom(databaseModelEventHandlerMethod.getParameterTypes()[0]) &&
void.class.equals(databaseModelEventHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = databaseModelEventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_dmeh_" + databaseModelEventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
bdr.registerBeanDefinition(beanName,
BeanDefinitionBuilder.genericBeanDefinition(DatabaseModelEventHandlerFunctionAdapter.class)
.addConstructorArgReference(aggregateBeanName)
.addConstructorArgValue(databaseModelEventHandlerMethod.getName())
.addConstructorArgValue(databaseModelEventHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(eventInfo.type())
.addConstructorArgValue(eventInfo.version())
.setInitMethodName("init")
⋮----
throw new ApplicationContextException("Invalid DatabaseModelEventHandler method signature: " + databaseModelEventHandlerMethod);
⋮----
public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
⋮----
public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jdbc/JdbcDatabaseModel.java
================
public class JdbcDatabaseModel implements DatabaseModel {
⋮----
private final DefaultTransactionDefinition transactionDefinition = new DefaultTransactionDefinition(TransactionDefinition.PROPAGATION_REQUIRED);
⋮----
this.transactionDefinition.setIsolationLevel(TransactionDefinition.ISOLATION_READ_COMMITTED);
this.transactionTemplate = new TransactionTemplate(transactionManager, transactionDefinition);
⋮----
public Map<String, Long> getOffsets(Set<String> partitionIds) {
return transactionTemplate.execute(status -> {
⋮----
.formatted(String.join(",", Collections.nCopies(partitionIds.size(), "?")));
⋮----
return jdbcTemplate.query(
⋮----
ps.setString(idx++, partitionId);
⋮----
(rs, rowNum) -> Map.entry(
rs.getString("partition_id"),
rs.getLong("record_offset")
⋮----
).stream().collect(Collectors.toMap(
⋮----
public Object startTransaction() {
return transactionManager.getTransaction(transactionDefinition);
⋮----
public void commitTransaction(Object transactionMarker, Map<String, Long> offsets) {
⋮----
throw new IllegalArgumentException("Invalid transaction marker");
⋮----
detectDatabaseType();
⋮----
String sql = getUpsertSql("partition_offsets", "partition_id", "record_offset");
⋮----
jdbcTemplate.batchUpdate(
⋮----
offsets.entrySet().stream()
.map(entry -> new Object[]{entry.getKey(), entry.getValue()})
.collect(Collectors.toList())
⋮----
transactionManager.commit(status);
⋮----
transactionManager.rollback(status);
throw new RuntimeException("Failed to commit offsets", e);
⋮----
private void detectDatabaseType() throws SQLException {
if(databaseType == null && jdbcTemplate.getDataSource() != null) {
try (Connection connection = jdbcTemplate.getDataSource().getConnection()) {
DatabaseMetaData metadata = connection.getMetaData();
databaseType = metadata.getDatabaseProductName().toLowerCase();
⋮----
private String getUpsertSql(String tableName, String keyColumn, String valueColumn) {
⋮----
""".formatted(tableName, keyColumn, valueColumn, keyColumn, valueColumn, valueColumn);
⋮----
""".formatted(tableName, keyColumn, valueColumn, valueColumn, valueColumn);
⋮----
""".formatted(tableName, keyColumn, valueColumn,
⋮----
default -> throw new UnsupportedOperationException("Unsupported database: " + databaseType);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jpa/JpaDatabaseModel.java
================
public class JpaDatabaseModel implements DatabaseModel {
⋮----
private final DefaultTransactionDefinition transactionDefinition = new DefaultTransactionDefinition(PROPAGATION_REQUIRED);
⋮----
transactionDefinition.setIsolationLevel(ISOLATION_READ_COMMITTED);
⋮----
public Map<String, Long> getOffsets(Set<String> partitionIds) {
return repository.findByPartitionIdIn(partitionIds).stream()
.collect(Collectors.toMap(
⋮----
public Object startTransaction() {
return transactionManager.getTransaction(transactionDefinition);
⋮----
public void commitTransaction(Object transactionMarker, Map<String, Long> offsets) {
⋮----
throw new IllegalArgumentException("Invalid transaction marker");
⋮----
repository.saveAll(
offsets.entrySet().stream()
.map(e -> new PartitionOffset(e.getKey(), e.getValue()))
.collect(Collectors.toList())
⋮----
transactionManager.commit(status);
⋮----
transactionManager.rollback(status);
throw new RuntimeException("Failed to commit offsets", e);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jpa/PartitionOffset.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/AkcesDatabaseModelAutoConfiguration.java
================
public class AkcesDatabaseModelAutoConfiguration {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public static DatabaseModelBeanFactoryPostProcessor databaseModelBeanFactoryPostProcessor() {
return new DatabaseModelBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesDatabaseModelSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public AkcesControlRecordSerde akcesControlRecordSerde(ObjectMapper objectMapper) {
return new AkcesControlRecordSerde(objectMapper);
⋮----
public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
⋮----
public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/AkcesDatabaseModelController.java
================
public class AkcesDatabaseModelController extends Thread implements AutoCloseable, ConsumerRebalanceListener, ApplicationContextAware, AkcesRegistry {
private static final Logger logger = LoggerFactory.getLogger(AkcesDatabaseModelController.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
super(databaseModelRuntime.getName() + "-AkcesDatabaseModelController");
⋮----
this.executorService = Executors.newCachedThreadPool(new CustomizableThreadFactory(databaseModelRuntime.getName() + "DatabaseModelPartitionThread-"));
⋮----
public void run() {
⋮----
controlRecordConsumerFactory.createConsumer(
databaseModelRuntime.getName() + "DatabaseModel-Akces-Control",
databaseModelRuntime.getName() + "-" + HostUtils.getHostName() + "DatabaseModel-Akces-Control",
⋮----
controlConsumer.subscribe(List.of("Akces-Control"), this);
⋮----
process();
⋮----
logger.info("Closing {} DatabaseModelPartitions", databaseModelPartitions.size());
databaseModelPartitions.values().forEach(databaseModelPartition -> {
⋮----
databaseModelPartition.close();
⋮----
logger.error("Error closing DatabaseModelPartition " + databaseModelPartition.getId(), e);
⋮----
controlConsumer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing controlConsumer", e);
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
logger.error("Error in AkcesDatabaseModelController", e);
⋮----
private void process() {
⋮----
processControlRecords();
⋮----
logger.error("Unrecoverable exception in AkcesDatabaseModelController", e);
⋮----
databaseModelRuntime.validateDomainEventSchemas();
⋮----
if (!partitionsToAssign.isEmpty()) {
⋮----
controlConsumer.seekToBeginning(partitionsToAssign);
⋮----
Map<TopicPartition, Long> initializedEndOffsets = controlConsumer.endOffsets(partitionsToAssign);
⋮----
ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
while (!initializedEndOffsets.isEmpty()) {
consumerRecords.forEach(record -> {
AkcesControlRecord controlRecord = record.value();
⋮----
if (aggregateServices.putIfAbsent(record.key(), aggregateServiceRecord) == null) {
logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
⋮----
logger.info("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
⋮----
if (consumerRecords.isEmpty()) {
initializedEndOffsets.entrySet().removeIf(entry -> entry.getValue() <= controlConsumer.position(entry.getKey()));
⋮----
consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
⋮----
DatabaseModelPartition databaseModelPartition = databaseModelPartitions.remove(topicPartition.partition());
⋮----
logger.info("Stopping DatabaseModelPartition {}", databaseModelPartition.getId());
⋮----
logger.error("Error closing DatabaseModelPartition", e);
⋮----
partitionsToRevoke.clear();
⋮----
DatabaseModelPartition aggregatePartition = new DatabaseModelPartition(
⋮----
topicPartition.partition(),
new TopicPartition("Akces-GDPRKeys", topicPartition.partition()),
databaseModelRuntime.getDomainEventTypes(),
⋮----
databaseModelPartitions.put(aggregatePartition.getId(), aggregatePartition);
logger.info("Starting DatabaseModelPartition {}", aggregatePartition.getId());
executorService.submit(aggregatePartition);
⋮----
partitionsToAssign.clear();
⋮----
private void processControlRecords() {
⋮----
if (!consumerRecords.isEmpty()) {
⋮----
if (!aggregateServices.containsKey(record.key())) {
⋮----
aggregateServices.put(record.key(), aggregateServiceRecord);
⋮----
public void close() throws Exception {
logger.info("Shutting down AkcesDatabaseModelController");
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesDatabaseModelController has been shutdown");
⋮----
logger.warn("AkcesDatabaseModelController did not shutdown within 10 seconds");
⋮----
public void onPartitionsRevoked(Collection<TopicPartition> topicPartitions) {
⋮----
if (!topicPartitions.isEmpty()) {
⋮----
partitionsToRevoke.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, revoking partitions: {}",
topicPartitions.stream().map(TopicPartition::partition).toList());
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, revoking partitions: {}",
⋮----
public void onPartitionsAssigned(Collection<TopicPartition> topicPartitions) {
⋮----
partitionsToAssign.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, assigning partitions : {}",
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, assigning partitions : {}",
⋮----
private boolean producesDomainEvent(List<AggregateServiceDomainEventType> producedEvents, DomainEventType<?> externalDomainEventType) {
⋮----
if (producedEvent.typeName().equals(externalDomainEventType.typeName()) &&
producedEvent.version() == externalDomainEventType.version()) {
⋮----
public String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType) {
List<AggregateServiceRecord> services = aggregateServices.values().stream()
.filter(commandServiceRecord -> producesDomainEvent(commandServiceRecord.producedEvents(), externalDomainEventType))
.toList();
if (services.size() == 1) {
return services.getFirst().domainEventTopic();
⋮----
throw new IllegalStateException("Cannot determine which service produces DomainEvent " + externalDomainEventType.typeName() + " v" + externalDomainEventType.version());
⋮----
public CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass) {
throw new UnsupportedOperationException();
⋮----
public String resolveTopic(@Nonnull Class<? extends Command> commandClass) {
⋮----
public String resolveTopic(@Nonnull CommandType<?> commandType) {
⋮----
public Integer resolvePartition(@Nonnull String aggregateId) {
⋮----
public boolean isRunning() {
return processState == RUNNING && databaseModelPartitions.values().stream().allMatch(DatabaseModelPartition::isProcessing);
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/AkcesDatabaseModelControllerState.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelImplementationPresentCondition.java
================
public class DatabaseModelImplementationPresentCondition extends SpringBootCondition {
⋮----
public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) {
boolean match = Optional.ofNullable(context.getBeanFactory())
.map(beanFactory -> beanFactory.getBeanNamesForAnnotation(DatabaseModelInfo.class))
.filter(beanNames -> beanNames.length > 0).isPresent();
return match ? ConditionOutcome.match() : ConditionOutcome.noMatch("No DatabaseModel beans found");

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelPartition.java
================
public class DatabaseModelPartition implements Runnable, AutoCloseable {
private static final Logger logger = LoggerFactory.getLogger(DatabaseModelPartition.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
⋮----
this.gdprContextRepository = gdprContextRepositoryFactory.create(runtime.getName(), id);
⋮----
public Integer getId() {
⋮----
public void run() {
⋮----
logger.info("Starting DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
this.consumer = consumerFactory.createConsumer(
runtime.getName() + "DatabaseModel-partition-" + id,
runtime.getName() + "DatabaseModel-partition-" + id + "-" + HostUtils.getHostName(),
⋮----
Set<String> distinctTopics = externalDomainEventTypes.stream()
.map(akcesRegistry::resolveTopic)
.collect(Collectors.toSet());
externalEventPartitions.addAll(distinctTopics.stream()
.map(topic -> new TopicPartition(topic, id))
.collect(Collectors.toSet()));
⋮----
consumer.assign(Stream.concat(
runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty(),
externalEventPartitions.stream()).toList());
logger.info("Assigned partitions {} for DatabaseModelPartition {} of {}DatabaseModel", consumer.assignment(), id, runtime.getName());
⋮----
process();
⋮----
logger.info("Shutting down DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
⋮----
logger.error("Unexpected error in DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName(), t);
⋮----
consumer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing consumer/producer", e);
⋮----
gdprContextRepository.close();
⋮----
logger.error("Error closing gdpr context repository", e);
⋮----
logger.info("Finished Shutting down DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
shutdownLatch.countDown();
⋮----
public void close() {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("DatabaseModelPartition={} has been shutdown", id);
⋮----
logger.warn("DatabaseModelPartition={} did not shutdown within 10 seconds", id);
⋮----
private void setupGDPRContext(String tenantId, String aggregateId) {
⋮----
GDPRContextHolder.setCurrentGDPRContext(gdprContextRepository.get(aggregateId));
⋮----
private void tearDownGDPRContext() {
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
private void process() {
⋮----
ConsumerRecords<String, ProtocolRecord> allRecords = consumer.poll(Duration.ofMillis(10));
if (!allRecords.isEmpty()) {
processRecords(allRecords);
⋮----
ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = consumer.poll(Duration.ofMillis(10));
gdprContextRepository.process(gdprKeyRecords.records(gdprKeyPartition));
⋮----
if (gdprKeyRecords.isEmpty() && initializedEndOffsets.getOrDefault(gdprKeyPartition, 0L) <= consumer.position(gdprKeyPartition)) {
⋮----
consumer.resume(externalEventPartitions);
⋮----
logger.info(
⋮----
runtime.getName(),
runtime.shouldHandlePIIData() ? "Handle PII Data" : "Not Handle PII Data");
⋮----
runtime.initializeOffsets(externalEventPartitions).forEach((topicPartition, offset)
-> consumer.seek(topicPartition, offset + 1));
⋮----
if(runtime.shouldHandlePIIData()) {
⋮----
long gdprKeyRepositoryOffset = gdprContextRepository.getOffset();
⋮----
runtime.getName());
consumer.seek(gdprKeyPartition, gdprContextRepository.getOffset() + 1);
⋮----
consumer.seekToBeginning(singletonList(gdprKeyPartition));
⋮----
initializedEndOffsets = consumer.endOffsets(List.of(gdprKeyPartition));
logger.info("Loading GDPR Keys for DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
⋮----
consumer.pause(externalEventPartitions);
⋮----
logger.error("Fatal error during " + processState + " phase, shutting down DatabaseModelPartition " + id + " of " + runtime.getName() + "DatabaseModel", e);
⋮----
private void processRecords(ConsumerRecords<String, ProtocolRecord> allRecords) {
if (logger.isTraceEnabled()) {
logger.trace("Processing {} records in a single batch", allRecords.count());
logger.trace("Processing {} gdpr key records", allRecords.records(gdprKeyPartition).size());
if (!externalEventPartitions.isEmpty()) {
logger.trace("Processing {} external event records", externalEventPartitions.stream()
.map(externalEventPartition -> allRecords.records(externalEventPartition).size())
.mapToInt(Integer::intValue).sum());
⋮----
List<ConsumerRecord<String, ProtocolRecord>> gdprKeyRecords = allRecords.records(gdprKeyPartition);
if (!gdprKeyRecords.isEmpty()) {
gdprContextRepository.process(gdprKeyRecords);
⋮----
runtime.apply(allRecords.partitions().stream()
.filter(topicPartition -> !topicPartition.equals(gdprKeyPartition))
.collect(Collectors.toMap(
⋮----
logger.error("IOException while processing events", e);
⋮----
public boolean isProcessing() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelPartitionState.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelRuntime.java
================
public interface DatabaseModelRuntime {
String getName();
⋮----
void apply(Map<TopicPartition, List<ConsumerRecord<String, ProtocolRecord>>> records,
⋮----
void validateDomainEventSchemas();
⋮----
boolean shouldHandlePIIData();
⋮----
Collection<DomainEventType<?>> getDomainEventTypes();
⋮----
Map<TopicPartition,Long> initializeOffsets(Collection<TopicPartition> topicPartitions);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelRuntimeFactory.java
================
public class DatabaseModelRuntimeFactory implements FactoryBean<DatabaseModelRuntime>, ApplicationContextAware {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
public DatabaseModelRuntime getObject() throws Exception {
return createRuntime(databaseModel);
⋮----
public Class<?> getObjectType() {
⋮----
private DatabaseModelRuntime createRuntime(DatabaseModel databaseModel) {
⋮----
DatabaseModelInfo databaseModelInfo = databaseModel.getClass().getAnnotation(DatabaseModelInfo.class);
⋮----
throw new IllegalStateException("Class implementing DatabaseModel must be annotated with @DatabaseModelInfo");
⋮----
.setDatabaseModelInfo(databaseModelInfo)
.setDatabaseModel(databaseModel)
.setObjectMapper(objectMapper)
.setDatabaseModel(databaseModel);
⋮----
applicationContext.getBeansOfType(DatabaseModelEventHandlerFunction.class).values().stream()
.filter(adapter -> adapter.getDatabaseModel().equals(databaseModel))
.forEach(adapter -> {
DomainEventType<?> type = adapter.getEventType();
runtimeBuilder.addDatabaseModelEventHandler(type, adapter);
runtimeBuilder.addDomainEvent(type);
⋮----
return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/KafkaDatabaseModelRuntime.java
================
public class KafkaDatabaseModelRuntime implements DatabaseModelRuntime {
private static final Logger logger = LoggerFactory.getLogger(KafkaDatabaseModelRuntime.class);
⋮----
public String getName() {
⋮----
public Map<TopicPartition, Long> initializeOffsets(Collection<TopicPartition> topicPartitions) {
Set<String> partitionIds = topicPartitions.stream()
.map(TopicPartition::toString)
.collect(Collectors.toSet());
⋮----
Map<String, Long> offsets = databaseModel.getOffsets(partitionIds);
⋮----
return topicPartitions.stream()
.collect(Collectors.toMap(
⋮----
tp -> offsets.getOrDefault(tp.toString(), 0L)
⋮----
public void apply(Map<TopicPartition, List<ConsumerRecord<String, ProtocolRecord>>> records,
⋮----
final Object transactionMarker = databaseModel.startTransaction();
⋮----
for (TopicPartition topicPartition : records.keySet()) {
for (DomainEventRecord eventRecord : records.get(topicPartition).stream().map(consumerRecord -> (DomainEventRecord) consumerRecord.value()).toList()) {
⋮----
DomainEventType<?> domainEventType = getDomainEventType(eventRecord);
⋮----
DatabaseModelEventHandlerFunction<DomainEvent> eventHandler = databaseModelEventHandlers.get(domainEventType);
⋮----
if (Boolean.TRUE.equals(domainEventType.piiData())) {
GDPRContextHolder.setCurrentGDPRContext(gdprContextSupplier.apply(eventRecord.aggregateId()));
⋮----
eventHandler.accept(materialize(domainEventType, eventRecord));
⋮----
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
databaseModel.commitTransaction(transactionMarker, records.entrySet().stream()
⋮----
entry -> entry.getKey().toString(),
entry -> entry.getValue().stream()
.mapToLong(ConsumerRecord::offset)
.max()
.orElse(0L)
⋮----
public void validateDomainEventSchemas() {
for (DomainEventType<?> domainEventType : domainEvents.values()) {
schemaRegistry.validate(domainEventType);
⋮----
public Collection<DomainEventType<?>> getDomainEventTypes() {
return domainEvents.values();
⋮----
public boolean shouldHandlePIIData() {
⋮----
private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
⋮----
private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {
⋮----
return domainEvents.entrySet().stream()
.filter(entry -> entry.getValue().external())
.filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
.filter(entry -> entry.getValue().version() <= eventRecord.version())
.max(Comparator.comparingInt(entry -> entry.getValue().version()))
.map(Map.Entry::getValue).orElse(null);
⋮----
public static class Builder {
⋮----
public Builder setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
⋮----
public Builder setObjectMapper(ObjectMapper objectMapper) {
⋮----
public Builder setDatabaseModelInfo(DatabaseModelInfo databaseModelInfo) {
⋮----
public Builder setDatabaseModel(DatabaseModel databaseModel) {
⋮----
public Builder addDomainEvent(DomainEventType<?> domainEvent) {
this.domainEvents.put(domainEvent.typeClass(), domainEvent);
⋮----
public Builder addDatabaseModelEventHandler(DomainEventType<?> eventType,
⋮----
this.databaseModelEventHandlers.put(eventType, databaseModelEventHandler);
⋮----
public KafkaDatabaseModelRuntime build() {
final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
return new KafkaDatabaseModelRuntime(
⋮----
databaseModelInfo.value(),
databaseModelInfo.version(),
⋮----
public String toString() {
return "KafkaDatabaseModelRuntime{"+getName()+"}";

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/KafkaQueryModelRuntime.java
================
public class KafkaQueryModelRuntime<S extends QueryModelState> implements QueryModelRuntime<S> {
private static final Logger logger = LoggerFactory.getLogger(KafkaQueryModelRuntime.class);
⋮----
public String getName() {
return type.typeName();
⋮----
public String getIndexName() {
return type.indexName();
⋮----
public Class<? extends QueryModel<S>> getQueryModelClass() {
⋮----
public S apply(List<DomainEventRecord> eventRecords, S currentState) throws IOException {
⋮----
DomainEventType<?> domainEventType = getDomainEventType(eventRecord);
⋮----
if (state == null && createStateHandler != null && createStateHandler.getEventType().equals(domainEventType)) {
state = createStateHandler.apply(materialize(domainEventType, eventRecord), null);
⋮----
QueryModelEventHandlerFunction<S, DomainEvent> eventHandler = queryModelEventHandlers.get(domainEventType);
⋮----
state = eventHandler.apply(materialize(domainEventType, eventRecord), state);
⋮----
public void validateDomainEventSchemas() {
for (DomainEventType<?> domainEventType : domainEvents.values()) {
schemaRegistry.validate(domainEventType);
⋮----
public boolean shouldHandlePIIData() {
⋮----
private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
⋮----
private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {
⋮----
return domainEvents.entrySet().stream()
.filter(entry -> entry.getValue().external())
.filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
.filter(entry -> entry.getValue().version() <= eventRecord.version())
.max(Comparator.comparingInt(entry -> entry.getValue().version()))
.map(Map.Entry::getValue).orElse(null);
⋮----
public static class Builder<S extends QueryModelState> {
⋮----
public Builder<S> setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
⋮----
public Builder<S> setObjectMapper(ObjectMapper objectMapper) {
⋮----
public Builder<S> setStateType(QueryModelStateType<S> stateType) {
⋮----
public Builder<S> setQueryModelClass(Class<? extends QueryModel<S>> queryModelClass) {
⋮----
public Builder<S> setCreateHandler(QueryModelEventHandlerFunction<S, DomainEvent> createStateHandler) {
⋮----
public Builder<S> addDomainEvent(DomainEventType<?> domainEvent) {
this.domainEvents.put(domainEvent.typeClass(), domainEvent);
⋮----
public Builder<S> addQueryModelEventHandler(DomainEventType<?> eventType,
⋮----
this.queryModelEventHandlers.put(eventType, eventSourcingHandler);
⋮----
public KafkaQueryModelRuntime<S> build() {
final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
⋮----
public String toString() {
return "KafkaQueryModelRuntime{"+getName()+"}";

================
File: main/query-support/src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#

org.elasticsoftware.akces.query.models.AkcesQueryModelAutoConfiguration
org.elasticsoftware.akces.query.database.AkcesDatabaseModelAutoConfiguration

================
File: main/query-support/src/main/resources/akces-databasemodel.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/wallet/WalletQueryModel.java
================
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
⋮----
public String getName() {
⋮----
public Class<WalletQueryModelState> getStateClass() {
⋮----
public String getIndexName() {
⋮----
public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
return new WalletQueryModelState(event.id(), List.of());
⋮----
public WalletQueryModelState createBalance(BalanceCreatedEvent event, WalletQueryModelState currentState) {
WalletQueryModelState.Balance balance = new WalletQueryModelState.Balance(event.currency(), BigDecimal.ZERO);
List<WalletQueryModelState.Balance> balances = new ArrayList<>(currentState.balances());
balances.add(balance);
return new WalletQueryModelState(currentState.walletId(), balances);
⋮----
public WalletQueryModelState creditWallet(WalletCreditedEvent event, WalletQueryModelState currentState) {
return new WalletQueryModelState(
currentState.walletId(),
currentState.balances().stream().map(balance -> {
if (balance.currency().equals(event.currency())) {
⋮----
balance.currency(),
balance.amount().add(event.amount()),
balance.reservedAmount()
⋮----
}).toList());

================
File: main/query-support/src/test/resources/db/changelog/liquibase.yaml
================
databaseChangeLog:
  - changeSet:
      id: create-partition-offsets-table
      author: jwijgerd
      changes:
        - createTable:
            tableName: partition_offsets
            columns:
              - column:
                  name: partition_id
                  type: varchar(255)
                  constraints:
                    primaryKey: true
                    nullable: false
              - column:
                  name: record_offset
                  type: bigint
                  constraints:
                    nullable: false
      rollback:
        - dropTable:
            tableName: partition_offsets

  - changeSet:
      id: create-account-table
      author: jwijgerd
      changes:
        - createTable:
            tableName: account
            columns:
              - column:
                  name: user_id
                  type: varchar(255)
                  constraints:
                    primaryKey: true
                    nullable: false
              - column:
                  name: country
                  type: varchar(2)
                  constraints:
                    nullable: false
              - column:
                  name: first_name
                  type: varchar(255)
                  constraints:
                    nullable: false
              - column:
                  name: last_name
                  type: varchar(255)
                  constraints:
                    nullable: false
              - column:
                  name: email
                  type: varchar(255)
                  constraints:
                    nullable: false
                    unique: true
      rollback:
        - dropTable:
            tableName: account

================
File: main/query-support/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.query.models" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.query.database" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.gdpr" level="info" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/AggregateValidator.java
================
public class AggregateValidator {
⋮----
public void validate() {
ensureCreateHandler();
ensureCreateEvent();
validateCommandHandlers();
validateEventHandlers();
validateEventSourcingHandlers();
validateEventUpcastingHandlers();
validateStateUpcastingHandlers();
validateEventBridgeHandlers();
⋮----
private void ensureCreateHandler() {
if (commandHandlers.stream().noneMatch(CommandType::create) && eventHandlers.stream().noneMatch(DomainEventType::create)) {
throw new IllegalStateException("No create handler registered for aggregate " + aggregateClass.getName());
⋮----
if (commandHandlers.stream().filter(CommandType::create).count() > 1) {
throw new IllegalStateException("Multiple command create handlers registered for aggregate " + aggregateClass.getName());
⋮----
if (eventHandlers.stream().filter(DomainEventType::create).count() > 1) {
throw new IllegalStateException("Multiple event create handlers registered for aggregate " + aggregateClass.getName());
⋮----
private void ensureCreateEvent() {
⋮----
if (producedDomainEventTypes.stream().filter(DomainEventType::create).count() != 1) {
throw new IllegalStateException("Exactly one create event should be produced by aggregate " + aggregateClass.getName());
⋮----
private void validateCommandHandlers() {
⋮----
String commandKey = commandType.typeName() + "_v" + commandType.version();
if (!seenCommandTypes.add(commandKey)) {
throw new IllegalStateException("Duplicate command handler for command " +
commandType.typeName() + " version " + commandType.version() +
" in aggregate " + aggregateClass.getName());
⋮----
private void validateEventHandlers() {
⋮----
String eventKey = eventType.typeName() + "_v" + eventType.version();
if (!seenEventTypes.add(eventKey)) {
throw new IllegalStateException("Duplicate event handler for event " +
eventType.typeName() + " version " + eventType.version() +
⋮----
private void validateEventSourcingHandlers() {
⋮----
if (eventSourcingHandlers.isEmpty()) {
throw new IllegalStateException("No event sourcing handlers registered for aggregate " + aggregateClass.getName());
⋮----
throw new IllegalStateException("Duplicate event sourcing handler for event " +
⋮----
if (eventSourcingHandlers.stream().noneMatch(DomainEventType::create)) {
throw new IllegalStateException("No event sourcing handler for create event in aggregate " + aggregateClass.getName());
⋮----
if (eventSourcingHandlers.stream().noneMatch(h -> h.typeName().equals(producedEventType.typeName()) &&
h.version() == producedEventType.version()) &&
eventUpcastingHandlers.stream().noneMatch(h -> h.inputType().typeName().equals(producedEventType.typeName()) &&
h.inputType().version() == producedEventType.version())) {
throw new IllegalStateException("No event sourcing handler or upcasting handler for produced event " +
producedEventType.typeName() + " version " + producedEventType.version() +
⋮----
private void validateEventBridgeHandlers() {
⋮----
throw new IllegalStateException("Duplicate event bridge handler for event " +
⋮----
private void validateEventUpcastingHandlers() {
⋮----
DomainEventType<?> inputType = (DomainEventType<?>) handler.inputType();
DomainEventType<?> outputType = (DomainEventType<?>) handler.outputType();
⋮----
if (!inputType.typeName().equals(outputType.typeName())) {
throw new IllegalArgumentException("Input event type " + inputType.typeName() +
" does not match output event type " + outputType.typeName());
⋮----
if (outputType.version() - inputType.version() != 1) {
throw new IllegalArgumentException("Output event version " + outputType.version() +
" must be one greater than input event version " + inputType.version());
⋮----
if (eventSourcingHandlers.stream().noneMatch(h -> h.typeName().equals(inputType.typeName()) &&
h.version() == inputType.version()) &&
eventUpcastingHandlers.stream().noneMatch(h -> h.inputType().typeName().equals(inputType.typeName()) &&
h.inputType().version() == inputType.version())) {
throw new IllegalStateException("No event sourcing handler or upcasting handler for input event " +
inputType.typeName() + " version " + inputType.version() +
⋮----
private void validateStateUpcastingHandlers() {
⋮----
AggregateStateType<?> inputType = (AggregateStateType<?>) handler.inputType();
AggregateStateType<?> outputType = (AggregateStateType<?>) handler.outputType();
⋮----
throw new IllegalArgumentException("Input state type " + inputType.typeName() +
" does not match output state type " + outputType.typeName());
⋮----
throw new IllegalArgumentException("Output state version " + outputType.version() +
" must be one greater than input state version " + inputType.version());
⋮----
if (!outputType.typeClass().equals(stateClass) &&
stateUpcastingHandlers.stream().noneMatch(h -> h.outputType().typeName().equals(outputType.typeName()) &&
h.outputType().version() == outputType.version())) {
throw new IllegalStateException("No upcasting handler for output state " +
outputType.typeName() + " version " + outputType.version() +
⋮----
public void detectCommandHandler(CommandType<?> commandType,
⋮----
commandHandlers.add(commandType);
this.producedDomainEventTypes.addAll(producedDomainEventTypes);
this.errorEventTypes.addAll(errorEventTypes);
⋮----
public void detectEventHandler(DomainEventType<?> inputEventType,
⋮----
eventHandlers.add(inputEventType);
⋮----
public void detectEventSourcingHandler(DomainEventType<?> domainEventType) {
eventSourcingHandlers.add(domainEventType);
⋮----
public void detectEventBridgeHandler(DomainEventType<?> inputEventType) {
eventBridgeHandlers.add(inputEventType);
⋮----
public void detectUpcastingHandler(DomainEventType<?> inputEventType, DomainEventType<?> outputEventType) {
eventUpcastingHandlers.add(new UpcastingHandler(inputEventType, outputEventType));
⋮----
public void detectUpcastingHandler(AggregateStateType<?> inputStateType, AggregateStateType<?> outputStateType) {
stateUpcastingHandlers.add(new UpcastingHandler(inputStateType, outputStateType));

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/ProtocolRecordTypeValueCodeGeneratorDelegate.java
================
public class ProtocolRecordTypeValueCodeGeneratorDelegate implements ValueCodeGenerator.Delegate {
⋮----
public CodeBlock generateCode(ValueCodeGenerator valueCodeGenerator, Object value) {
⋮----
) -> CodeBlock.builder()
.add("new $T($S, $L, $T.class, $L, $L, $L, $L)",
⋮----
.build();
⋮----
.add("new $T($S, $L, $T.class, $L, $L, $L)",
⋮----
.add("new $T($S, $L, $T.class, $L, $L, $S, $L)",

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/InMemoryAggregateStateRepository.java
================
public class InMemoryAggregateStateRepository implements AggregateStateRepository {
private static final Logger log = LoggerFactory.getLogger(InMemoryAggregateStateRepository.class);
⋮----
public void close() {
stateRecordMap.clear();
transactionStateRecordMap.clear();
⋮----
public void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture) {
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
⋮----
public void commit() {
⋮----
if (!transactionStateRecordMap.isEmpty()) {
⋮----
this.offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
log.trace("Committing {} records and offset {}", transactionStateRecordMap.size(), this.offset);
transactionStateRecordMap.values().forEach(recordAndMetadata -> stateRecordMap.put(recordAndMetadata.record().aggregateId(), recordAndMetadata.record()));
⋮----
public void rollback() {
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
AggregateStateRecord record = (AggregateStateRecord) consumerRecord.value();
⋮----
stateRecordMap.put(record.aggregateId(), record);
⋮----
stateRecordMap.remove(consumerRecord.key());
⋮----
this.offset = consumerRecord.offset();
⋮----
public AggregateStateRecord get(String aggregateId) {
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
return stateRecordMap.get(aggregateId);
⋮----
public long getOffset() {

================
File: main/runtime/src/test/java/org/elasticsoftware/akces/beans/MinInsyncReplicasTest.java
================
public class MinInsyncReplicasTest {
⋮----
void testScenarios() {
⋮----
Assertions.assertEquals(1, calculateQuorum((short) 1));
Assertions.assertEquals(2, calculateQuorum((short) 2));
Assertions.assertEquals(2, calculateQuorum((short) 3));
Assertions.assertEquals(3, calculateQuorum((short) 4));
Assertions.assertEquals(3, calculateQuorum((short) 5));
Assertions.assertEquals(4, calculateQuorum((short) 6));
Assertions.assertEquals(4, calculateQuorum((short) 7));
Assertions.assertEquals(5, calculateQuorum((short) 8));
Assertions.assertEquals(5, calculateQuorum((short) 9));
Assertions.assertEquals(6, calculateQuorum((short) 10));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountCreatedEventV2.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountState.java
================
Boolean twoFactorEnabled) implements AggregateState {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/CreateAccountCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/PreviousAccountState.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcessManager.java
================
public class OrderProcessManager implements Aggregate<OrderProcessManagerState> {
⋮----
public String getName() {
⋮----
public Class<OrderProcessManagerState> getStateClass() {
⋮----
public Stream<UserOrderProcessesCreatedEvent> create(AccountCreatedEvent event, OrderProcessManagerState isNull) {
return Stream.of(new UserOrderProcessesCreatedEvent(event.userId()));
⋮----
public OrderProcessManagerState create(UserOrderProcessesCreatedEvent event, OrderProcessManagerState isNull) {
return new OrderProcessManagerState(event.userId());
⋮----
public OrderProcessManagerState handle(BuyOrderCreatedEvent event, OrderProcessManagerState state) {
return new OrderProcessManagerState(state.userId(), new ArrayList<>(state.runningProcesses()) {{
add(new BuyOrderProcess(
event.orderId(),
event.market(),
event.quantity(),
event.limitPrice(),
event.clientReference()));
⋮----
public OrderProcessManagerState handle(BuyOrderRejectedEvent event, OrderProcessManagerState state) {
⋮----
removeIf(process -> process.orderId().equals(event.orderId()));
⋮----
public OrderProcessManagerState handle(BuyOrderPlacedEvent event, OrderProcessManagerState state) {
⋮----
public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {
⋮----
String orderId = UUID.randomUUID().toString();
⋮----
getCommandBus().send(new ReserveAmountCommand(
state.userId(),
command.market().quoteCurrency(),
command.quantity().multiply(command.limitPrice()),
⋮----
return Stream.of(new BuyOrderCreatedEvent(
⋮----
command.market(),
command.quantity(),
command.limitPrice(),
command.clientReference()));
⋮----
public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {
⋮----
OrderProcess orderProcess = state.getAkcesProcess(event.referenceId());
⋮----
return Stream.of(new BuyOrderPlacedEvent(state.userId(), orderProcess.orderId(), orderProcess.market(), orderProcess.quantity(), orderProcess.limitPrice()));
⋮----
return Stream.empty();
⋮----
public Stream<DomainEvent> handle(InsufficientFundsErrorEvent errorEvent, OrderProcessManagerState state) {
return Stream.of(state.getAkcesProcess(errorEvent.referenceId()).handle(errorEvent));
⋮----
public Stream<DomainEvent> handle(InvalidCurrencyErrorEvent errorEvent, OrderProcessManagerState state) {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcessManagerState.java
================
this(userId, List.of());
⋮----
public String getAggregateId() {
return userId();
⋮----
public OrderProcess getAkcesProcess(String processId) {
return runningProcesses.stream().filter(p -> p.orderId().equals(processId)).findFirst()
.orElseThrow(() -> new UnknownAkcesProcessException("OrderProcessManager", userId(), processId));
⋮----
public boolean hasAkcesProcess(String processId) {
return runningProcesses.stream().anyMatch(p -> p.orderId().equals(processId));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletStateV2.java
================
) implements AggregateState {
⋮----
public String getAggregateId() {
return id();
⋮----
this(currency, BigDecimal.ZERO, List.of());
⋮----
this(currency, amount, List.of());
⋮----
public BigDecimal getAvailableAmount() {
return amount.subtract(reservations().stream().map(Reservation::amount).reduce(BigDecimal.ZERO, BigDecimal::add));
⋮----
if (amount.compareTo(BigDecimal.ZERO) <= 0) {
throw new IllegalArgumentException("Reservation amount must be positive");

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/control/AkcesAggregateControllerTests.java
================
public class AkcesAggregateControllerTests {
⋮----
public void testSerialization() throws IOException {
ObjectMapper objectMapper = new ObjectMapper();
AggregateServiceRecord record = new AggregateServiceRecord(
⋮----
List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
Collections.emptyList());
assertNotNull(record);
⋮----
public void testDeserialization() throws JsonProcessingException {
⋮----
AkcesControlRecord deserialized = objectMapper.readValue(serializedRecord, AggregateServiceRecord.class);
assertNotNull(deserialized);
assertInstanceOf(AggregateServiceRecord.class, deserialized);
assertEquals("Account", ((AggregateServiceRecord) deserialized).aggregateName());
assertEquals("Account-Commands", ((AggregateServiceRecord) deserialized).commandTopic());
⋮----
public void testSerde() {
AkcesControlRecordSerde serde = new AkcesControlRecordSerde(new ObjectMapper());
⋮----
byte[] serialized = serde.serializer().serialize("Akces-Control", record);
assertNotNull(serialized);
⋮----
AkcesControlRecord deserialized = serde.deserializer().deserialize("Akces-Control", serialized);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEventV2.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEventV3.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/AccountConfiguration.java
================
public class AccountConfiguration {
⋮----
public AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
return new AggregateBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient() {
return new MockSchemaRegistryClient();
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient src,
⋮----
return new KafkaSchemaRegistry(src, objectMapper);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/AccountTests.java
================
public class AccountTests {
⋮----
public void testAggregateStateSerializationWithChangingSchema() throws JsonProcessingException {
⋮----
String serializedState= objectMapper.writeValueAsString(new PreviousAccountState(userId, "US", "John", "Doe", "john.doe@example.com"));
AccountState accountState = objectMapper.readValue(serializedState, AccountState.class);
Assertions.assertEquals(userId, accountState.userId());
Assertions.assertNotNull(accountState.twoFactorEnabled());
Assertions.assertFalse(accountState.twoFactorEnabled());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceCommandType.java
================
public <C extends Command> CommandType<C> toLocalCommandType(Class<C> typeClass) {
return new CommandType<>(typeName, version, typeClass, create, true, hasPIIDataAnnotation(typeClass));

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AkcesRegistry.java
================
public interface AkcesRegistry {
CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass);
⋮----
String resolveTopic(@Nonnull Class<? extends Command> commandClass);
⋮----
String resolveTopic(@Nonnull CommandType<?> commandType);
⋮----
String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType);
⋮----
Integer resolvePartition(@Nonnull String aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/AkcesGDPRModule.java
================
public class AkcesGDPRModule extends Module {
private static Version extractVersion() {
String className = format("/%s.class", AkcesGDPRModule.class.getName().replace('.', '/'));
URL resource = AkcesGDPRModule.class.getResource(className);
⋮----
String classPath = resource.toString();
if (!classPath.startsWith("jar")) {
⋮----
return Version.unknownVersion();
⋮----
String manifestPath = classPath.substring(0, classPath.lastIndexOf("!") + 1) +
⋮----
Manifest manifest = new Manifest(new URL(manifestPath).openStream());
Attributes attr = manifest.getMainAttributes();
String value = attr.getValue("Implementation-Version");
⋮----
return generateVersion(Semver.parse(value));
⋮----
public static Version generateVersion(Semver semver) {
⋮----
return new Version(
semver.getMajor(),
semver.getMinor(),
semver.getPatch(),
!semver.getPreRelease().isEmpty() ? semver.getPreRelease().getFirst() : null,
⋮----
public String getModuleName() {
⋮----
public Version version() {
return extractVersion();
⋮----
public void setupModule(SetupContext setupContext) {
setupContext.addBeanSerializerModifier(new PIIDataSerializerModifier());
setupContext.addBeanDeserializerModifier(new PIIDataDeserializerModifier());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/EncryptingGDPRContext.java
================
public final class EncryptingGDPRContext implements GDPRContext {
private static final Logger logger = LoggerFactory.getLogger(EncryptingGDPRContext.class);
⋮----
throw new IllegalArgumentException("Key size needs to be 32 bytes");
⋮----
SecretKeySpec keySpec = new SecretKeySpec(encryptionKey, "AES");
⋮----
UUID aggregateUUID = UUID.fromString(aggregateId);
ivParameterSpec = new IvParameterSpec(ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array());
⋮----
encryptingCipher = Cipher.getInstance("AES/" + aesMode + "/PKCS5PADDING");
decryptingCipher = Cipher.getInstance("AES/" + aesMode + "/PKCS5PADDING");
encryptingCipher.init(Cipher.ENCRYPT_MODE, keySpec, ivParameterSpec, GDPRKeyUtils.secureRandom());
decryptingCipher.init(Cipher.DECRYPT_MODE, keySpec, ivParameterSpec, GDPRKeyUtils.secureRandom());
⋮----
throw new SerializationException(e);
⋮----
throw new IllegalArgumentException(e);
⋮----
public String encrypt(@Nullable String data) {
⋮----
logger.trace("Encrypting data for aggregateId '{}' with algorithm {} and encryptionKey (hash) {}",
⋮----
encryptingCipher.getAlgorithm(),
HexFormat.of().formatHex(encryptionKey));
return Base64.getUrlEncoder().encodeToString(encryptingCipher.doFinal(data.getBytes(StandardCharsets.UTF_8)));
⋮----
public String decrypt(@Nullable String encryptedData) {
⋮----
if (encryptedData.length() % 4 == 0) {
byte[] encryptedBytes = Base64.getUrlDecoder().decode(encryptedData);
⋮----
logger.trace("Decrypting data for aggregateId '{}' with algorithm {} and encryptionKey (hash) {}",
⋮----
decryptingCipher.getAlgorithm(),
⋮----
return new String(decryptingCipher.doFinal(encryptedBytes), StandardCharsets.UTF_8);
⋮----
public String aggregateId() {
⋮----
public byte[] getEncryptionKey() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContext.java
================
public sealed interface GDPRContext permits NoopGDPRContext, EncryptingGDPRContext {
⋮----
String encrypt(@Nullable String data);
⋮----
String decrypt(@Nullable String encryptedData);
⋮----
String aggregateId();
⋮----
default byte[] getEncryptionKey() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/InMemoryGDPRContextRepository.java
================
public class InMemoryGDPRContextRepository implements GDPRContextRepository {
private static final Logger log = LoggerFactory.getLogger(InMemoryGDPRContextRepository.class);
⋮----
private final LoadingCache<String, GDPRContext> gdprContexts = Caffeine.newBuilder()
.maximumSize(1000)
.build(this::createGDPRContext);
⋮----
public void close() {
stateRecordMap.clear();
transactionStateRecordMap.clear();
gdprContexts.invalidateAll();
gdprContexts.cleanUp();
⋮----
public void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture) {
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
⋮----
gdprContexts.invalidate(record.aggregateId());
⋮----
public void commit() {
⋮----
if (!transactionStateRecordMap.isEmpty()) {
⋮----
this.offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
log.trace("Committing {} records and offset {}", transactionStateRecordMap.size(), this.offset);
transactionStateRecordMap.values().forEach(recordAndMetadata -> stateRecordMap.put(recordAndMetadata.record().aggregateId(), recordAndMetadata.record()));
⋮----
public void rollback() {
⋮----
gdprContexts.invalidateAll(transactionStateRecordMap.keySet());
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
GDPRKeyRecord record = (GDPRKeyRecord) consumerRecord.value();
⋮----
stateRecordMap.put(record.aggregateId(), record);
⋮----
stateRecordMap.remove(consumerRecord.key());
⋮----
gdprContexts.invalidate(consumerRecord.key());
this.offset = consumerRecord.offset();
⋮----
public boolean exists(String aggregateId) {
return getGDPRKeyRecord(aggregateId) != null;
⋮----
public GDPRContext get(String aggregateId) {
return gdprContexts.get(aggregateId);
⋮----
private GDPRContext createGDPRContext(String aggregateId) {
GDPRKeyRecord record = getGDPRKeyRecord(aggregateId);
⋮----
checkAggregateIdType(aggregateId);
return new EncryptingGDPRContext(record.aggregateId(), record.payload(), aggregateIdIsUUID);
⋮----
return new NoopGDPRContext(aggregateId);
⋮----
private GDPRKeyRecord getGDPRKeyRecord(String aggregateId) {
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
return stateRecordMap.get(aggregateId);
⋮----
public long getOffset() {
⋮----
private void checkAggregateIdType(String aggregateId) {
⋮----
UUID.fromString(aggregateId);
⋮----
log.trace("AggregateId '{}' is a UUID", aggregateId);
⋮----
log.trace("AggregateId '{}' is not a UUID", aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/NoopGDPRContext.java
================
public record NoopGDPRContext(String aggregateId) implements GDPRContext {
⋮----
public String encrypt(@Nullable String data) {
⋮----
public String decrypt(@Nullable String encryptedData) {
⋮----
public String aggregateId() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepository.java
================
public class RocksDBGDPRContextRepository implements GDPRContextRepository {
private static final Logger log = LoggerFactory.getLogger(RocksDBGDPRContextRepository.class);
⋮----
private final LoadingCache<String, GDPRContext> gdprContexts = Caffeine.newBuilder()
.maximumSize(1000)
.build(this::createGDPRContext);
⋮----
RocksDB.loadLibrary();
final Options options = new Options();
final TransactionDBOptions transactionDBOptions = new TransactionDBOptions();
options.setCreateIfMissing(true);
⋮----
options.setAllowFAllocate(false);
this.rocksDBDataDir = new File(baseDir, partitionId);
⋮----
Files.createDirectories(this.rocksDBDataDir.getParentFile().toPath());
Files.createDirectories(this.rocksDBDataDir.getAbsoluteFile().toPath());
db = TransactionDB.open(options, transactionDBOptions, this.rocksDBDataDir.getAbsolutePath());
⋮----
initializeOffset();
log.info("RocksDBGDPRContextRepository for partition {} initialized in folder {}", partitionId, this.rocksDBDataDir.getAbsolutePath());
⋮----
throw new GDPRContextRepositoryException("Error initializing RocksDB", e);
⋮----
public void close() {
⋮----
db.syncWal();
⋮----
log.error("Error syncing WAL. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
db.close();
⋮----
private void initializeOffset() {
⋮----
byte[] offsetBytes = db.get(OFFSET);
⋮----
lastOffset = Longs.fromByteArray(offsetBytes);
⋮----
throw new GDPRContextRepositoryException("Error initializing offset", e);
⋮----
private void updateOffset(long offset) {
⋮----
log.trace("Updated offset to {}", offset);
⋮----
public long getOffset() {
⋮----
public void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture) {
checkAggregateIdType(record.aggregateId());
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
⋮----
public void commit() {
if (!transactionStateRecordMap.isEmpty()) {
⋮----
Transaction transaction = db.beginTransaction(new WriteOptions());
⋮----
for (RecordAndMetadata<?> recordAndMetadata : transactionStateRecordMap.values()) {
transaction.put(keyBytes(recordAndMetadata.record().aggregateId()), serializer.serialize(topicName, recordAndMetadata.record()));
⋮----
long offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
transaction.put(OFFSET, Longs.toByteArray(offset));
transaction.commit();
transaction.close();
updateOffset(offset);
⋮----
throw new GDPRContextRepositoryException("Error committing records", e);
⋮----
transactionStateRecordMap.clear();
⋮----
public void rollback() {
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
long offset = consumerRecords.stream()
.map(ConsumerRecord::offset)
⋮----
if(consumerRecord.value() != null) {
⋮----
transaction.put(keyBytes(consumerRecord.key()), serializer.serialize(topicName, consumerRecord.value()));
log.trace("{} Wrote record with key {}", rocksDBDataDir.getAbsolutePath(), consumerRecord.key());
⋮----
transaction.delete(keyBytes(consumerRecord.key()));
⋮----
consumerRecords.forEach(consumerRecord -> gdprContexts.invalidate(consumerRecord.key()));
⋮----
throw new GDPRContextRepositoryException("Error processing records", e);
⋮----
public boolean exists(String aggregateId) {
return transactionStateRecordMap.containsKey(aggregateId) || db.keyExists(keyBytes(aggregateId));
⋮----
public GDPRContext get(String aggregateId) {
return gdprContexts.get(aggregateId);
⋮----
private GDPRContext createGDPRContext(String aggregateId) {
GDPRKeyRecord record = getGDPRKeyRecord(aggregateId);
⋮----
checkAggregateIdType(aggregateId);
return new EncryptingGDPRContext(record.aggregateId(), record.payload(), aggregateIdIsUUID);
⋮----
return new NoopGDPRContext(aggregateId);
⋮----
private GDPRKeyRecord getGDPRKeyRecord(String aggregateId) {
log.trace("{} Getting record for aggregateId {}",rocksDBDataDir.getAbsolutePath(), aggregateId);
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
byte[] keyBytes = keyBytes(aggregateId);
if (db.keyExists(keyBytes)) {
⋮----
return (GDPRKeyRecord) deserializer.deserialize(topicName, db.get(keyBytes));
⋮----
throw new GDPRContextRepositoryException("Problem reading record with aggregateId " + aggregateId, e);
⋮----
private byte[] keyBytes(String aggregateId) {
⋮----
UUID aggregateUUID = UUID.fromString(aggregateId);
return ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array();
⋮----
return aggregateId.getBytes(StandardCharsets.UTF_8);
⋮----
private void checkAggregateIdType(String aggregateId) {
⋮----
UUID.fromString(aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/KafkaSchemaRegistry.java
================
public class KafkaSchemaRegistry {
private static final Logger logger = LoggerFactory.getLogger(KafkaSchemaRegistry.class);
⋮----
this.schemaGeneratorTheadLocal = ThreadLocal.withInitial(() -> createJsonSchemaGenerator(objectMapper));
⋮----
public JsonSchema validate(CommandType<?> commandType) throws SchemaException {
return validate(commandType, true);
⋮----
public JsonSchema validate(DomainEventType<?> domainEventType) throws SchemaException {
return validate(domainEventType, false);
⋮----
private JsonSchema validate(SchemaType<?> schemaType, boolean strict) throws SchemaException {
⋮----
logger.info("Validating schema {} v{}", schemaType.getSchemaName(), schemaType.version());
JsonSchema localSchema = generateJsonSchema(schemaType);
⋮----
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas(schemaType.getSchemaName(), false, false);
if (!registeredSchemas.isEmpty()) {
logger.trace("Found {} schemas for type {}", registeredSchemas.size(), schemaType.typeName());
⋮----
ParsedSchema registeredSchema = registeredSchemas.stream()
.filter(parsedSchema -> getSchemaVersion(schemaType, parsedSchema) == schemaType.version())
.findFirst().orElse(null);
⋮----
logger.trace("Found schema for type {} v{}", schemaType.typeName(), schemaType.version());
⋮----
List<Difference> differences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());
if (!differences.isEmpty()) {
⋮----
List<Difference> violatingDifferences = differences.stream()
.filter(difference -> !difference.getType().equals(Difference.Type.PROPERTY_REMOVED_FROM_CLOSED_CONTENT_MODEL))
.toList();
if (!violatingDifferences.isEmpty()) {
⋮----
throw new IncompatibleSchemaException(
schemaType.getSchemaName(),
schemaType.version(),
schemaType.typeClass(),
⋮----
throw new SchemaVersionNotFoundException(
⋮----
schemaType.typeClass());
⋮----
throw new SchemaNotFoundException(
⋮----
throw new SchemaException(
⋮----
public JsonSchema registerAndValidate(SchemaType<?> schemaType, boolean forceRegisterOnIncompatibleSchema) throws SchemaException {
⋮----
String schemaName = schemaType.getSchemaName();
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas(
⋮----
if (registeredSchemas.isEmpty()) {
if (!schemaType.external()) {
if (schemaType.version() == 1) {
⋮----
schemaRegistryClient.register(
⋮----
throw new PreviousSchemaVersionMissingException(
⋮----
if (schemaType.external() && schemaType.relaxExternalValidation()) {
⋮----
if (!registeredSchema.deepEquals(localSchema)) {
⋮----
if (!Objects.equals(registeredSchema.toString(), localSchema.toString())) {
List<Difference> violatingDifferences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());
⋮----
logger.warn("Found an incompatible schema for {} v{} but forceRegisterOnIncompatibleSchema=true. Overwriting existing entry in SchemaRegistry", schemaName, schemaType.version());
⋮----
schemaRegistryClient.deleteSchemaVersion(
⋮----
"" + schemaType.version());
// then do a hard delete of the version
⋮----
"" + schemaType.version(),
⋮----
// and recreate it
⋮----
logger.error(
⋮----
} else if (schemaType.external()) {
// we did not find any schema with the exact version.
// since we are registering the type ourselves, this is an error
⋮----
// ensure we have an ordered list of schemas
registeredSchemas.sort(Comparator.comparingInt(parsedSchema -> getSchemaVersion(schemaType, parsedSchema)));
// see if the new version is exactly one higher than the last version
if (schemaType.version() != getSchemaVersion(schemaType,registeredSchemas.getLast()) + 1) {
throw new InvalidSchemaVersionException(
⋮----
registeredSchemas.getLast().version(),
⋮----
// see if the new schema is backwards compatible with the previous ones
List<Difference> differences = SchemaDiff.compare(
((JsonSchema)registeredSchemas.getLast()).rawSchema(),
localSchema.rawSchema())
.stream().filter(diff ->
!SchemaDiff.COMPATIBLE_CHANGES.contains(diff.getType()) &&
!Difference.Type.REQUIRED_PROPERTY_ADDED_TO_UNOPEN_CONTENT_MODEL.equals(diff.getType())).toList();
if (differences.isEmpty()) {
// register the new schema
⋮----
// incompatible
throw new SchemaNotBackwardsCompatibleException(
⋮----
getSchemaVersion(schemaType,registeredSchemas.getLast()),
⋮----
// schema is fine, return
⋮----
public JsonSchema generateJsonSchema(SchemaType<?> schemaType) {
return new JsonSchema(schemaGeneratorTheadLocal.get().generateSchema(schemaType.typeClass()), List.of(), Map.of(), schemaType.version());
⋮----
private int getSchemaVersion(SchemaType<?> schemaType, ParsedSchema parsedSchema) {
⋮----
return schemaRegistryClient.getVersion(schemaType.getSchemaName(), parsedSchema);
⋮----
throw new RuntimeException(e);
⋮----
private SchemaGenerator createJsonSchemaGenerator(ObjectMapper objectMapper) {
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapper,
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
return new SchemaGenerator(configBuilder.build());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaNotBackwardsCompatibleException.java
================
public class SchemaNotBackwardsCompatibleException extends SchemaException {
⋮----
public int getPreviousSchemaVersion() {
⋮----
public int getSchemaVersion() {
⋮----
public List<Difference> getDifferences() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaNotFoundException.java
================
public class SchemaNotFoundException extends SchemaException {
⋮----
super("Schema "+schemaIdentifier+" for class "+implementationClass.getName()+" Not Found", schemaIdentifier, implementationClass);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/serialization/AkcesControlRecordSerde.java
================
public final class AkcesControlRecordSerde implements Serde<AkcesControlRecord> {
⋮----
this.serializer = new SerializerImpl(objectMapper);
this.deserializer = new DeserializerImpl(objectMapper);
⋮----
public Serializer<AkcesControlRecord> serializer() {
⋮----
public Deserializer<AkcesControlRecord> deserializer() {
⋮----
public byte[] serialize(String topic, AkcesControlRecord data) {
⋮----
return objectMapper.writeValueAsBytes(csr);
⋮----
throw new SerializationException("Unsupported AkcesControlRecord type " + data.getClass().getSimpleName());
⋮----
throw new SerializationException(e);
⋮----
public AkcesControlRecord deserialize(String topic, byte[] data) {
⋮----
} else if (topic.endsWith("Akces-Control")) {
return objectMapper.readValue(data, AkcesControlRecord.class);
⋮----
throw new SerializationException("Unsupported topic " + topic);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/serialization/ProtocolRecordSerde.java
================
public final class ProtocolRecordSerde implements Serde<ProtocolRecord> {
⋮----
private final ObjectMapper objectMapper = new ProtobufMapper();
⋮----
ProtobufSchema domainEventRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(domainEventRecordProto));
ProtobufSchema aggregateStateRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(aggregateStateRecordProto));
ProtobufSchema commandRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(commandRecordProto));
ProtobufSchema gdprKeyRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(gdprKeyRecordProto));
ProtobufSchema commandResponseRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(commandResponseRecordProto));
serializer = new SerializerImpl(objectMapper.writer(domainEventRecordSchema),
objectMapper.writer(aggregateStateRecordSchema),
objectMapper.writer(commandRecordSchema),
objectMapper.writer(gdprKeyRecordSchema),
objectMapper.writer(commandResponseRecordSchema));
deserializer = new DeserializerImpl(objectMapper.readerFor(DomainEventRecord.class).with(domainEventRecordSchema),
objectMapper.readerFor(AggregateStateRecord.class).with(aggregateStateRecordSchema),
objectMapper.readerFor(CommandRecord.class).with(commandRecordSchema),
objectMapper.readerFor(GDPRKeyRecord.class).with(gdprKeyRecordSchema),
objectMapper.readerFor(CommandResponseRecord.class).with(commandResponseRecordSchema));
⋮----
throw new SerializationException(e);
⋮----
public Serializer<ProtocolRecord> serializer() {
⋮----
public Deserializer<ProtocolRecord> deserializer() {
⋮----
public byte[] serialize(String topic, ProtocolRecord data) {
⋮----
return domainEventRecordWriter.writeValueAsBytes(r);
⋮----
return aggregateStateRecordWriter.writeValueAsBytes(r);
⋮----
return commandRecordWriter.writeValueAsBytes(r);
⋮----
return commandResponseRecordWriter.writeValueAsBytes(e);
⋮----
return gdprKeyRecordWriter.writeValueAsBytes(r);
⋮----
throw new SerializationException("Unsupported ProtocolRecord type " + data.getClass().getSimpleName());
⋮----
public ProtocolRecord deserialize(String topic, byte[] data) {
⋮----
} else if (topic.endsWith("DomainEvents") || topic.endsWith("DomainEventIndex")) {
return domainEventRecordReader.readValue(data);
} else if (topic.endsWith("AggregateState")) {
return aggregateStateRecordReader.readValue(data);
} else if (topic.endsWith("Commands")) {
return commandRecordReader.readValue(data);
} else if (topic.endsWith("CommandResponses")) {
return commandResponseRecordReader.readValue(data);
} else if (topic.endsWith("GDPRKeys")) {
return gdprKeyRecordReader.readValue(data);
⋮----
throw new SerializationException("Unsupported topic name " + topic + " cannot determine ProtocolRecordType");

================
File: RELEASE.md
================
## Release process

This project uses the Maven Release Plugin and GitHub Actions to create releases.\
Just run `mvn release:prepare release:perform && git push` in the root to select the version to be released and create a
VCS tag.

GitHub Actions will
start [the build process](https://github.com/elasticsoftwarefoundation/akces-framework/actions/workflows/maven-publish.yml).

If successful, the build will be automatically published
to [Github Packages](https://maven.pkg.github.com/elasticsoftwarefoundation/akces-framework/).

================
File: SERVICES.md
================
# Akces Framework Services

## Overview

The Akces Framework provides a set of specialized services that implement the CQRS (Command Query Responsibility Segregation) and Event Sourcing patterns using Apache Kafka as the underlying infrastructure. This document describes the main services in the framework and how they interact to form a complete event-driven architecture.

## Core Services Architecture

The Akces Framework consists of three main service types that work together:

1. **Command Services** - Handle commands and validate business logic
2. **Aggregate Services** - Maintain event-sourced state for domain entities
3. **Query Services** - Provide optimized read models for client applications

These services are managed by the Akces Operator, a Kubernetes operator that automates the deployment, scaling, and management of the Akces services in a Kubernetes environment.

## Akces Operator

The Akces Operator is a Kubernetes operator built using the Java Operator SDK that manages the lifecycle of Akces Framework services in a Kubernetes cluster.

### Features

- Automated deployment of Command, Aggregate, and Query services
- Kafka topic management (creation and configuration)
- State management for deployed services
- Resource scaling based on workload
- Configuration management

### Custom Resources

The operator defines three Custom Resource Definitions (CRDs):

#### 1. Aggregate

The Aggregate custom resource defines an Aggregate service that maintains the state of domain entities using event sourcing.

```yaml
apiVersion: akces.elasticsoftware.org/v1
kind: Aggregate
metadata:
  name: account-aggregate
spec:
  replicas: 3
  image: ghcr.io/elasticsoftwarefoundation/akces-aggregate-service:0.9.0
  aggregateNames:
    - Account
    - OrderProcessManager
    - Wallet
  applicationName: "Akces Account Aggregate Service"
  enableSchemaOverwrites: false
  args:
    - "--spring.profiles.active=prod"
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
```

#### 2. CommandService

The CommandService custom resource defines a Command service that validates and processes commands before sending them to the appropriate aggregate.

```yaml
apiVersion: akces.elasticsoftware.org/v1
kind: CommandService
metadata:
  name: account-command
spec:
  replicas: 3
  image: ghcr.io/elasticsoftwarefoundation/akces-command-service:0.9.0
  applicationName: "Akces Account Command Service"
  args:
    - "--spring.profiles.active=prod"
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
```

#### 3. QueryService

The QueryService custom resource defines a Query service that provides optimized read models for client applications.

```yaml
apiVersion: akces.elasticsoftware.org/v1
kind: QueryService
metadata:
  name: account-query
spec:
  replicas: 3
  image: ghcr.io/elasticsoftwarefoundation/akces-query-service:0.9.0
  applicationName: "Akces Account Query Service"
  args:
    - "--spring.profiles.active=prod"
  env:
    - name: DB_HOST
      value: "postgres.default"
    - name: DB_PORT
      value: "5432"
  applicationProperties: |
    # Custom application properties
    spring.datasource.url=jdbc:postgresql://${DB_HOST}:${DB_PORT}/accounts
    spring.datasource.username=${DB_USER}
    spring.datasource.password=${DB_PASSWORD}
    spring.jpa.hibernate.ddl-auto=validate
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
```

### Reconciliation Process

For each custom resource, the operator:

1. Creates a ConfigMap with the service configuration
2. Deploys a StatefulSet to run the service instances
3. Creates a Service for network access
4. Manages Kafka topics required by the service
5. Updates the status of the resource based on the StatefulSet's status

## Service Details

### Aggregate Service

The Aggregate Service is responsible for:

- Processing commands received from the Command Service
- Maintaining the event-sourced state of domain aggregates
- Applying business logic through command handlers
- Emitting domain events to Kafka topics
- Storing state snapshots for efficient recovery

Key characteristics:
- Stateful service (requires persistent storage)
- Uses RocksDB for state storage
- Consumes and produces to Kafka topics
- Processes events in a strictly ordered fashion per aggregate

Configuration:
- Deployed with a PersistentVolumeClaim for state storage
- Configured with Kafka connection details
- Uses ZGC for efficient memory management
- Exposes health endpoints for monitoring

### Command Service

The Command Service is responsible for:

- Receiving commands from client applications
- Validating command structure and content
- Routing commands to the appropriate aggregate
- Managing command response handling
- Implementing schema validation using JSON Schema

Key characteristics:
- Stateless service
- Acts as the entry point for write operations
- Handles command validation and routing
- Provides synchronous and asynchronous APIs for clients

Configuration:
- Deployed without persistent storage
- Configured with Kafka connection details
- Exposes HTTP endpoints for client requests

### Query Service

The Query Service is responsible for:

- Building and maintaining read models from domain events
- Providing optimized views of domain data
- Serving queries from client applications
- Updating read models based on domain events

Key characteristics:
- Stateful service (requires persistent storage)
- Consumes domain events from Kafka topics
- Maintains query-optimized state
- Can integrate with traditional databases (SQL, NoSQL)

Configuration:
- Deployed with a PersistentVolumeClaim for state storage
- Can be configured with custom application properties
- Supports custom environment variables for database connections
- Exposes HTTP endpoints for client queries

## Kafka Topic Structure

The operator manages the following Kafka topics for each aggregate:

- `<AggregateName>-Commands` - Commands sent to the aggregate
- `<AggregateName>-DomainEvents` - Domain events emitted by the aggregate
- `<AggregateName>-AggregateState` - Compacted topic storing the latest state of each aggregate instance

Additionally, the framework uses the following system topics:

- `Akces-Control` - Control messages and metadata
- `Akces-CommandResponses` - Responses to commands (for client notification)
- `Akces-GDPRKeys` - Encryption keys for GDPR-protected data

## Deployment Patterns

The Akces services can be deployed in various patterns:

1. **Monolithic Deployment** - All three service types deployed together for simple applications
2. **Microservice Deployment** - Each aggregate type gets its own set of services
3. **Hybrid Deployment** - Command services handle multiple aggregates, while query services are specialized

## Configuration and Integration

### Spring Boot Integration

All Akces services are built on Spring Boot and provide:

- Health endpoints for monitoring
- Graceful shutdown
- Structured logging with Logback
- Support for configuration via properties files or environment variables

### Customization Options

Each service type can be customized:

- Custom application properties
- Environment variables
- Resource limits and requests
- Replica count for scaling
- Custom Docker images for domain-specific logic

### High Availability

The operator deploys services with:

- Multiple replicas for redundancy
- Affinity rules for distributing across nodes
- Probes for health checking
- Graceful shutdown handling

## Usage Examples

### Deploying a Complete CQRS System

To deploy a complete CQRS system for a domain, create instances of all three CRDs:

```yaml
# 1. Create the Aggregate Service
apiVersion: akces.elasticsoftware.org/v1
kind: Aggregate
metadata:
  name: wallet-aggregate
spec:
  replicas: 3
  image: ghcr.io/elasticsoftwarefoundation/akces-aggregate-service:0.9.0
  aggregateNames:
    - Wallet
  applicationName: "Wallet Aggregate Service"
  enableSchemaOverwrites: false

---
# 2. Create the Command Service
apiVersion: akces.elasticsoftware.org/v1
kind: CommandService
metadata:
  name: wallet-command
spec:
  replicas: 3
  image: ghcr.io/elasticsoftwarefoundation/akces-command-service:0.9.0
  applicationName: "Wallet Command Service"

---
# 3. Create the Query Service
apiVersion: akces.elasticsoftware.org/v1
kind: QueryService
metadata:
  name: wallet-query
spec:
  replicas: 3
  image: ghcr.io/elasticsoftwarefoundation/akces-query-service:0.9.0
  applicationName: "Wallet Query Service"
  applicationProperties: |
    spring.application.name=Wallet Query Service
    akces.querymodels.enabled=true
    akces.querymodels.packages=com.example.wallet.query
```

### Scaling Services

To scale a service, update the `replicas` field:

```yaml
apiVersion: akces.elasticsoftware.org/v1
kind: Aggregate
metadata:
  name: wallet-aggregate
spec:
  replicas: 5  # Increased from 3 to 5
  # other fields remain the same
```

### Accessing Service Information

The operator updates the status of each resource with information about the running service:

```bash
kubectl get aggregate wallet-aggregate -o yaml
```

Example output:
```yaml
apiVersion: akces.elasticsoftware.org/v1
kind: Aggregate
metadata:
  name: wallet-aggregate
spec:
  # [...]
status:
  readyReplicas: 3
```

## Best Practices

1. **Resource Planning**
   - Size CPU and memory resources based on the expected load
   - Monitor resource usage to adjust as needed

2. **Persistence**
   - Use SSDs or high-performance disks for state storage
   - In GCP, consider using Hyperdisk Balanced for optimal performance

3. **Scaling**
   - Horizontally scale services to match workload
   - Ensure Kafka partitions match or exceed service replica count

4. **Monitoring**
   - Set up monitoring for the health endpoints
   - Monitor Kafka lag to detect processing delays

5. **Backup**
   - Regular backups of Kafka topics are recommended
   - Implement disaster recovery plans for data safety

## Troubleshooting

Common issues and solutions:

1. **Service Not Starting**
   - Check ConfigMap exists and has correct format
   - Verify image pull secrets are configured
   - Check resource constraints

2. **Command Processing Issues**
   - Verify Kafka topics exist and are accessible
   - Check schema compatibility in Schema Registry
   - Review service logs for validation errors

3. **Query Service Not Updating**
   - Check Kafka consumer lag
   - Verify event handlers are correctly implemented
   - Check database connectivity if using external databases

4. **Performance Issues**
   - Tune Kafka parameters for performance
   - Adjust JVM settings via environment variables
   - Scale up resources or increase replicas

5. **Operator Issues**
   - Check operator logs for reconciliation errors
   - Verify RBAC permissions for the operator

## Conclusion

The Akces Framework services provide a complete implementation of CQRS and Event Sourcing patterns using Kafka as the backbone infrastructure. The Kubernetes operator simplifies the deployment and management of these services, allowing teams to focus on domain-specific implementations rather than infrastructure concerns.

By separating commands, aggregates, and queries into distinct services, the framework provides flexibility, scalability, and resilience while maintaining the integrity of the event-sourced data model.

================
File: TEST-APPS.md
================
# Akces Framework Test Applications

This document provides an overview of the test applications included in the Akces Framework repository. These applications demonstrate real-world usage patterns and implementation examples of the Akces Framework's CQRS (Command Query Responsibility Segregation) and Event Sourcing capabilities.

## Crypto Trading Application

The primary test application is a Crypto Trading platform that showcases how to build a distributed, event-driven system using the Akces Framework.

### Overview

The Crypto Trading application simulates a cryptocurrency trading platform where users can:

- Create accounts
- Manage crypto wallets with various currency balances
- Place market orders to buy cryptocurrencies
- View market information from exchanges (via Coinbase API integration)

The application demonstrates the full CQRS pattern with clear separation between commands (write operations) and queries (read operations), as well as event sourcing for maintaining complete audit trails and state reconstruction.

### Architecture

The application follows a modular architecture split into three main components:

1. **Aggregates Module** (`crypto-trading/aggregates`): Contains domain models, commands, events, and business logic
2. **Commands Module** (`crypto-trading/commands`): Provides API endpoints for write operations
3. **Queries Module** (`crypto-trading/queries`): Provides API endpoints for read operations and maintains query-optimized data models

### Key Aggregates

The application defines several key aggregates that demonstrate different aspects of domain modeling with Akces:

#### Account Aggregate

Represents a user account in the trading system:

- Creates new user accounts
- Stores basic user information including PII (Personally Identifiable Information) that is automatically protected through Akces's GDPR compliance features

```java
public final class Account implements Aggregate<AccountState> {
    public Stream<AccountCreatedEvent> create(CreateAccountCommand cmd, AccountState isNull) {
        return Stream.of(new AccountCreatedEvent(cmd.userId(), cmd.country(), 
                                               cmd.firstName(), cmd.lastName(), cmd.email()));
    }
}
```

#### Wallet Aggregate

Manages a user's cryptocurrency holdings:

- Creates wallets automatically when accounts are created
- Supports multiple currency balances within a wallet
- Handles crediting and debiting operations
- Implements reservation system for pending transactions
- Demonstrates state versioning/migration with `WalletState` and `WalletStateV2`

```java
public final class Wallet implements Aggregate<WalletStateV2> {
    public Stream<DomainEvent> credit(CreditWalletCommand cmd, WalletStateV2 currentState) {
        // Validation and business logic
        return Stream.of(new WalletCreditedEvent(currentState.id(), cmd.currency(), 
                                               cmd.amount(), balance.amount().add(cmd.amount())));
    }
}
```

#### CryptoMarket Aggregate

Represents a trading market for a specific cryptocurrency pair:

- Creates market definitions with base and quote currencies
- Processes market orders
- Integrates with external Coinbase API for pricing
- Demonstrates external service integration

```java
public class CryptoMarket implements Aggregate<CryptoMarketState> {
    public Stream<DomainEvent> handle(PlaceMarketOrderCommand command, CryptoMarketState currentState) {
        // Market order processing logic with external API integration
        Ticker currentTicker = coinbaseService.getTicker(currentState.id());
        // Further processing...
    }
}
```

#### OrderProcessManager Aggregate

Demonstrates the Process Manager pattern for coordinating complex workflows:

- Orchestrates the full lifecycle of buy orders across multiple aggregates
- Reacts to events from different aggregates to advance the process
- Handles compensating transactions for failures
- Shows how to implement saga patterns with Akces

```java
public class OrderProcessManager implements Aggregate<OrderProcessManagerState> {
    public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {
        // Reserve funds first
        getCommandBus().send(new ReserveAmountCommand(/*...*/));
        // Create the order
        return Stream.of(new BuyOrderCreatedEvent(/*...*/));
    }
    
    // Event handlers to advance the process based on events from other aggregates
    public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {
        // Place the market order now that funds are reserved
        getCommandBus().send(new PlaceMarketOrderCommand(/*...*/));
        return Stream.of(new BuyOrderPlacedEvent(/*...*/));
    }
}
```

### Commands and Events

The application defines a rich set of commands and events that demonstrate best practices:

#### Commands

- `CreateAccountCommand`: Creates a new user account
- `CreateWalletCommand`: Creates a new wallet
- `CreateBalanceCommand`: Adds a new currency balance to a wallet
- `CreditWalletCommand`: Adds funds to a wallet balance
- `DebitWalletCommand`: Removes funds from a wallet balance
- `ReserveAmountCommand`: Reserves funds for a pending transaction
- `PlaceBuyOrderCommand`: Initiates a buy order process
- `PlaceMarketOrderCommand`: Places an order on a market

#### Events

- `AccountCreatedEvent`: Signals account creation
- `WalletCreatedEvent`: Signals wallet creation
- `BalanceCreatedEvent`: Signals addition of a new balance
- `WalletCreditedEvent`: Signals successful crediting of funds
- `WalletDebitedEvent`: Signals successful debiting of funds
- `AmountReservedEvent`: Signals successful fund reservation
- `BuyOrderCreatedEvent`: Signals creation of a buy order
- `MarketOrderFilledEvent`: Signals successful execution of a market order

#### Error Events

The application handles errors through specialized error events:

- `InvalidAmountErrorEvent`: Signals an invalid amount in a command
- `InsufficientFundsErrorEvent`: Signals insufficient funds for an operation
- `InvalidCryptoCurrencyErrorEvent`: Signals an unknown cryptocurrency
- `MarketOrderRejectedErrorEvent`: Signals rejection of a market order

### Query Models

The application defines query models that are optimized for read operations:

- `AccountQueryModel`: Provides efficient access to account information
- `WalletQueryModel`: Provides efficient access to wallet balances
- `CryptoMarketModel`: Provides access to market information

These models are kept up-to-date by subscribing to domain events and are stored in formats optimized for queries.

### Database Integration

The application demonstrates database integration for query models:

- Uses JDBC for database access
- Implements Liquibase for database migrations
- Shows how to map domain events to database operations

```java
public class CryptoMarketModel extends JdbcDatabaseModel {
    public void handle(CryptoMarketCreatedEvent event) {
        cryptoMarketRepository.save(CryptoMarket.createNew(
            event.id(),
            event.baseCrypto(),
            event.quoteCrypto(),
            // Other properties...
        ));
    }
}
```

### REST API

The application exposes REST endpoints for both commands and queries:

#### Command Endpoints

- `POST /v1/accounts`: Create a new account
- `POST /v1/wallets/{walletId}/balances`: Add a new balance to a wallet
- `POST /v1/wallets/{walletId}/balances/{currency}/credit`: Credit funds to a wallet
- `POST /v1/accounts/{accountId}/orders/buy`: Place a buy order

#### Query Endpoints

- `GET /v1/accounts/{accountId}`: Get account information
- `GET /v1/wallets/{walletId}`: Get wallet information
- `GET /v1/markets`: Get all markets
- `GET /v1/markets/{marketId}`: Get specific market information

### Testing

The application includes comprehensive tests that demonstrate testing strategies for CQRS and Event Sourcing:

- Unit tests for individual aggregates
- Integration tests for command processing
- End-to-end tests for full workflows
- Tests that verify temporal queries and event replay

The tests use TestContainers to set up Kafka, Schema Registry, and PostgreSQL for realistic testing environments.

## Technical Highlights

### Event Sourcing Implementation

- All state changes are captured as immutable events
- State is reconstructed by replaying events
- Events are stored in Kafka topics, partitioned by aggregate ID
- Snapshots are maintained in RocksDB for efficient access

### CQRS Pattern

- Clear separation between command and query responsibilities
- Command endpoints focusing on write operations
- Query endpoints optimized for read operations
- Different data models for writes and reads

### GDPR Compliance

- PII data (like names and emails) is automatically protected
- Uses `@PIIData` annotation to mark sensitive fields
- Transparent encryption during serialization

### Schema Management

- Integration with Confluent Schema Registry
- Command and event validation using JSON Schema
- Schema evolution with version tracking

### Process Management

- Complex workflows coordinated across multiple aggregates
- Stateful process tracking
- Event-driven process advancement

## Conclusion

The Crypto Trading application serves as a comprehensive demonstration of building distributed, event-driven systems with the Akces Framework. It showcases best practices for implementing CQRS and Event Sourcing patterns and demonstrates how to solve common challenges in distributed systems.

This application can be used as a reference implementation when building your own applications with the Akces Framework, offering patterns and solutions for common requirements in enterprise applications.

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/AggregateStateType.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/CommandType.java
================
public String getSchemaPrefix() {
⋮----
public boolean relaxExternalValidation() {

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/ProtocolRecordType.java
================
public sealed interface ProtocolRecordType<T> permits SchemaType, AggregateStateType {
String typeName();
⋮----
int version();
⋮----
Class<T> typeClass();

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/SchemaType.java
================
public sealed interface SchemaType<T> extends ProtocolRecordType<T> permits DomainEventType, CommandType {
String getSchemaPrefix();
⋮----
default String getSchemaName() {
return getSchemaPrefix() + typeName();
⋮----
boolean relaxExternalValidation();
⋮----
boolean external();

================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/CommandInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/DomainEventInfo.java
================


================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientController.java
================
public class AkcesClientController extends Thread implements AutoCloseable, AkcesClient, ApplicationContextAware {
private static final Logger logger = LoggerFactory.getLogger(AkcesClientController.class);
private static final TopicPartition AKCES_CONTROL_PARTITION = new TopicPartition("Akces-Control", 0);
⋮----
private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
loadSupportedDomainEvents(basePackage);
⋮----
loadSupportedDomainEvents("org.elasticsoftware.akces.errors");
⋮----
public TopicPartition getCommandResponsePartition() {
⋮----
public void run() {
try (final Consumer<String, AkcesControlRecord> controlConsumer = controlRecordConsumerFactory.createConsumer(
HostUtils.getHostName() + "-AkcesClientController-Control",
⋮----
final Consumer<String, ProtocolRecord> commandResponseConsumer = commandResponseConsumerFactory.createConsumer(
HostUtils.getHostName() + "-AkcesClientController-CommandResponses",
⋮----
final Producer<String, ProtocolRecord> producer = producerFactory.createProducer(HostUtils.getHostName() + "-AkcesClientController")) {
⋮----
partitions = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control").partitions().size();
⋮----
controlConsumer.assign(singletonList(AKCES_CONTROL_PARTITION));
⋮----
controlConsumer.seekToBeginning(singletonList(AKCES_CONTROL_PARTITION));
⋮----
int commandResponsePartitions = kafkaAdmin.describeTopics("Akces-CommandResponses").get("Akces-CommandResponses").partitions().size();
commandResponsePartition = new TopicPartition("Akces-CommandResponses",
resolveCommandResponsePartition(HostUtils.getHostName(), commandResponsePartitions));
commandResponseConsumer.assign(singletonList(commandResponsePartition));
⋮----
commandResponseConsumer.seekToEnd(singletonList(commandResponsePartition));
⋮----
process(controlConsumer, commandResponseConsumer, producer);
⋮----
commandQueue.drainTo(pendingRequests);
⋮----
pendingRequest.completableFuture().completeExceptionally(new CommandRefusedException(pendingRequest.command().getClass(), SHUTTING_DOWN));
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
private void loadSupportedDomainEvents(String basePackage) {
for (BeanDefinition beanDefinition : domainEventScanner.findCandidateComponents(basePackage)) {
⋮----
Class<? extends DomainEvent> domainEventClass = (Class<? extends DomainEvent>) Class.forName(beanDefinition.getBeanClassName());
DomainEventInfo domainEventInfo = domainEventClass.getAnnotation(DomainEventInfo.class);
TreeMap<Integer, DomainEventType<? extends DomainEvent>> versionMap = domainEventClasses.computeIfAbsent(domainEventInfo.type(), k -> new TreeMap<>());
versionMap.put(domainEventInfo.version(), new DomainEventType<>(domainEventInfo.type(), domainEventInfo.version(), domainEventClass, false, true, ErrorEvent.class.isAssignableFrom(domainEventClass), hasPIIDataAnnotation(domainEventClass)));
⋮----
private void process(Consumer<String, AkcesControlRecord> controlConsumer,
⋮----
processControlRecords(controlConsumer.poll(Duration.ofMillis(10)));
⋮----
processCommands(producer);
⋮----
processCommandResponses(commandResponseConsumer.poll(Duration.ofMillis(10)));
⋮----
logger.error("Unrecoverable exception in AkcesController", e);
⋮----
logger.error("Exception while loading Command (JSON)Schemas from SchemaRegistry", e);
⋮----
Map<TopicPartition, Long> endOffsets = controlConsumer.endOffsets(singletonList(AKCES_CONTROL_PARTITION));
ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(10));
processControlRecords(consumerRecords);
⋮----
if (consumerRecords.isEmpty() && endOffsets.getOrDefault(AKCES_CONTROL_PARTITION, 0L) <= controlConsumer.position(AKCES_CONTROL_PARTITION)) {
⋮----
private void processControlRecords(ConsumerRecords<String, AkcesControlRecord> consumerRecords) throws RestClientException, IOException {
if (!consumerRecords.isEmpty()) {
⋮----
AkcesControlRecord controlRecord = record.value();
⋮----
if (!aggregateServices.containsKey(record.key())) {
logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
⋮----
aggregateServices.put(record.key(), aggregateServiceRecord);
⋮----
logger.warn("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
⋮----
private void processCommands(Producer<String, ProtocolRecord> producer) {
⋮----
while(!commandQueue.isEmpty()) {
⋮----
CommandRequest headRequest = commandQueue.peek();
⋮----
registerCommand(
headRequest.commandType(),
headRequest.commandVersion(),
headRequest.command().getClass());
⋮----
CommandRequest commandRequest = commandQueue.poll();
⋮----
commandRequest.commandType(),
commandRequest.commandVersion(),
commandRequest.command().getClass());
⋮----
String topic = resolveTopic(
⋮----
commandRequest.command());
⋮----
CommandRecord commandRecord = new CommandRecord(
commandRequest.tenantId(),
⋮----
serialize(commandRequest.command()),
⋮----
commandRequest.command().getAggregateId(),
commandRequest.correlationId() != null ? commandRequest.correlationId() : UUID.randomUUID().toString(), commandResponsePartition.toString());
⋮----
resolvePartition(commandRecord.aggregateId()),
commandRecord.aggregateId(),
⋮----
commandRecords.put(producerRecord, commandRequest);
⋮----
if (commandRequest.completeAfterValidation()) {
commandRequest.completableFuture().complete(Collections.emptyList());
⋮----
commandRequest.completableFuture().completeExceptionally(e);
⋮----
if(!commandRecords.isEmpty()) {
⋮----
producer.beginTransaction();
for (Map.Entry<ProducerRecord<String, ProtocolRecord>, CommandRequest> entry : commandRecords.entrySet()) {
final CompletableFuture<List<DomainEvent>> completableFuture = entry.getValue().completableFuture();
final CommandRecord commandRecord = (CommandRecord) entry.getKey().value();
final Class<? extends Command> commandClass = entry.getValue().command().getClass();
if (!completableFuture.isDone()) {
KafkaSender.send(producer, entry.getKey(), (metadata, exception) -> {
⋮----
completableFuture.completeExceptionally(new CommandSendingFailedException(commandClass, exception));
⋮----
pendingCommandResponseMap.put(commandRecord.id(), new PendingCommandResponse(commandRecord, completableFuture));
⋮----
KafkaSender.send(producer, entry.getKey());
⋮----
producer.commitTransaction();
⋮----
private void processCommandResponses(ConsumerRecords<String, ProtocolRecord> consumerRecords) {
⋮----
ProtocolRecord protocolRecord = record.value();
⋮----
PendingCommandResponse pendingCommandResponse = pendingCommandResponseMap.remove(commandResponseRecord.commandId());
⋮----
logger.trace("Received CommandResponseRecord for unknown commandId: {}", commandResponseRecord.commandId());
⋮----
for (DomainEventRecord domainEventRecord : commandResponseRecord.events()) {
domainEvents.add(deserialize(domainEventRecord, commandResponseRecord.encryptionKey()));
⋮----
pendingCommandResponse.completableFuture().complete(domainEvents);
⋮----
pendingCommandResponse.completableFuture().completeExceptionally(e);
⋮----
logger.warn("Received unknown ProtocolRecord type: {}", protocolRecord.getClass().getSimpleName());
⋮----
public CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command) {
⋮----
checkRunning(command);
CommandInfo commandInfo = command.getClass().getAnnotation(CommandInfo.class);
⋮----
throw new IllegalArgumentException("Command class " + command.getClass().getName() + " is not annotated with @CommandInfo");
⋮----
commandQueue.add(new CommandRequest(
⋮----
commandInfo.type(),
commandInfo.version(),
⋮----
public void sendAndForget(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command) {
⋮----
completableFuture.get();
⋮----
if(e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public void close() throws Exception {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesClientController has been shutdown");
⋮----
logger.warn("AkcesClientController did not shutdown within 10 seconds");
⋮----
private AggregateServiceCommandType resolveCommandType(String type, int version) {
return aggregateServices.values().stream()
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), type, version))
.findFirst().flatMap(commandServiceRecord -> commandServiceRecord.supportedCommands().stream()
.filter(commandType -> commandType.typeName().equals(type) && commandType.version() == version)
.findFirst()).orElse(null);
⋮----
private AggregateServiceRecord resolveAggregateService(AggregateServiceCommandType commandType) {
⋮----
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType.typeName(), commandType.version()))
.findFirst().orElse(null);
⋮----
private String resolveTopic(String commandType, int version, Command command) {
List<AggregateServiceRecord> services = aggregateServices.values().stream()
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType, version))
.toList();
if (services.size() == 1) {
return services.getFirst().commandTopic();
} else if (services.isEmpty()) {
throw new UnroutableCommandException(command.getClass());
⋮----
private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, String commandType, int version) {
⋮----
if (supportedCommand.typeName().equals(commandType) &&
supportedCommand.version() == version) {
⋮----
public Integer resolvePartition(@Nonnull String aggregateId) {
return Math.abs(hashFunction.hashString(aggregateId, UTF_8).asInt()) % partitions;
⋮----
private Integer resolveCommandResponsePartition(String hostname, int partitions) {
return Math.abs(hashFunction.hashString(hostname, UTF_8).asInt()) % partitions;
⋮----
private void registerCommand(String type, int version, Class<? extends Command> commandClass) {
⋮----
if (!commandTypes.containsKey(commandClass)) {
⋮----
AggregateServiceCommandType commandType = resolveCommandType(type, version);
⋮----
ParsedSchema schema = schemaRegistry.validate(commandType.toLocalCommandType(commandClass));
commandSchemas.computeIfAbsent(
commandType.typeName(),
k -> new ConcurrentHashMap<>()).put(commandType.version(), schema);
logger.trace("Stored schema: {} v{}", commandType.schemaName(), commandType.version());
⋮----
commandSchemasLookup.put(commandClass, schema);
⋮----
commandTypes.put(commandClass, commandType);
⋮----
for (AggregateServiceDomainEventType domainEventType : resolveAggregateService(commandType).producedEvents()) {
processDomainEvent(commandClass, domainEventType);
⋮----
throw new UnroutableCommandException(commandClass);
⋮----
private void processDomainEvent(Class<? extends Command> commandClass, AggregateServiceDomainEventType aggregateServiceDomainEventType) throws SchemaException {
⋮----
domainEventClasses.get(aggregateServiceDomainEventType.typeName()).floorEntry(aggregateServiceDomainEventType.version()).getValue();
⋮----
domainEventSchemas.computeIfAbsent(
domainEventType.typeName(),
k -> new ConcurrentHashMap<>()).put(
domainEventType.version(),
schemaRegistry.validate(domainEventType));
logger.trace("Stored schema for: {} v{}", domainEventType.getSchemaName(), domainEventType.version());
⋮----
throw new MissingDomainEventException(
⋮----
aggregateServiceDomainEventType.typeName(),
aggregateServiceDomainEventType.version());
⋮----
private byte[] serialize(Command command) {
⋮----
ParsedSchema schema = commandSchemasLookup.get(command.getClass());
⋮----
JsonNode jsonNode = objectMapper.valueToTree(command);
jsonSchema.validate(jsonNode);
return objectMapper.writeValueAsBytes(jsonNode);
⋮----
return objectMapper.writeValueAsBytes(command);
⋮----
throw new CommandSerializationException(command.getClass(), e);
⋮----
throw new CommandValidationException(command.getClass(), e);
⋮----
private DomainEvent deserialize(DomainEventRecord der, @Nullable byte[] encryptionKey) throws IOException {
⋮----
setCurrentGDPRContext(encryptionKey != null ? new EncryptingGDPRContext(der.aggregateId(), encryptionKey, GDPRKeyUtils.isUUID(der.aggregateId())) : null);
⋮----
DomainEventType<? extends DomainEvent> domainEventType = domainEventClasses.get(der.name()).floorEntry(der.version()).getValue();
⋮----
return objectMapper.readValue(der.payload(), domainEventType.typeClass());
⋮----
resetCurrentGDPRContext();
⋮----
private void checkRunning(Command command) {
⋮----
throw new CommandRefusedException(command.getClass(), processState);
⋮----
public boolean isRunning() {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/beans/DatabaseModelEventHandlerFunctionAdapter.java
================
public class DatabaseModelEventHandlerFunctionAdapter<E extends DomainEvent>
⋮----
GDPRAnnotationUtils.hasPIIDataAnnotation(domainEventClass));
⋮----
public void init() {
⋮----
MethodHandles.Lookup lookup = MethodHandles.lookup();
MethodType methodType = MethodType.methodType(void.class, domainEventClass);
adapterMethodHandle = lookup.findVirtual(databaseModel.getClass(), adapterMethodName, methodType);
⋮----
throw new RuntimeException("Failed to find method " + adapterMethodName + " on " +
databaseModel.getClass().getName(), e);
⋮----
public void accept(@NotNull E event) {
⋮----
adapterMethodHandle.invoke(databaseModel, event);
⋮----
throw new RuntimeException("Error invoking event handler", e);
⋮----
public DomainEventType<E> getEventType() {
⋮----
public DatabaseModel getDatabaseModel() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jpa/PartitionOffsetRepository.java
================
public interface PartitionOffsetRepository extends JpaRepository<PartitionOffset, String> {
List<PartitionOffset> findByPartitionIdIn(Collection<String> partitionIds);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/beans/QueryModelEventHandlerFunctionAdapter.java
================
public class QueryModelEventHandlerFunctionAdapter<S extends QueryModelState, E extends DomainEvent>
⋮----
hasPIIDataAnnotation(domainEventClass));
⋮----
public void init() {
⋮----
MethodHandles.Lookup lookup = MethodHandles.lookup();
MethodType methodType = MethodType.methodType(stateClass, domainEventClass, stateClass);
adapterMethodHandle = lookup.findVirtual(queryModel.getClass(), adapterMethodName, methodType);
⋮----
throw new RuntimeException("Failed to find method " + adapterMethodName + " on " +
queryModel.getClass().getName(), e);
⋮----
public @NotNull S apply(@NotNull E event, S state) {
⋮----
return (S) adapterMethodHandle.invoke(queryModel, event, state);
⋮----
throw new RuntimeException("Error invoking event handler", e);
⋮----
public DomainEventType<E> getEventType() {
⋮----
public QueryModel<S> getQueryModel() {
⋮----
public boolean isCreate() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelController.java
================
public class AkcesQueryModelController extends Thread implements AutoCloseable, ApplicationContextAware, QueryModels {
private static final Logger logger = LoggerFactory.getLogger(AkcesQueryModelController.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
⋮----
private final Cache<String, CachedQueryModelState<?>> queryModelStateCache = Caffeine.newBuilder()
.maximumSize(1000)
.build();
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
this.enabledRuntimes.putAll(
applicationContext.getBeansOfType(QueryModelRuntime.class).values().stream()
.collect(Collectors.toMap(runtime -> ((QueryModelRuntime<?>) runtime).getQueryModelClass(), runtime -> runtime)));
⋮----
private <S extends QueryModelState> QueryModelRuntime<S> getEnabledRuntime(Class<? extends QueryModel<S>> modelClass) {
return (QueryModelRuntime<S>) this.enabledRuntimes.get(modelClass);
⋮----
private <S extends QueryModelState> boolean isRuntimeDisabled(Class<? extends QueryModel<S>> modelClass) {
return this.disabledRuntimes.containsKey(modelClass);
⋮----
public <S extends QueryModelState> CompletionStage<S> getHydratedState(Class<? extends QueryModel<S>> modelClass, String id) {
QueryModelRuntime<S> runtime = getEnabledRuntime(modelClass);
⋮----
CachedQueryModelState<S> cachedQueryModelState = (CachedQueryModelState<S>) queryModelStateCache.getIfPresent(runtime.getName()+"-"+id);
S currentState = cachedQueryModelState != null ? cachedQueryModelState.state() : null;
Long currentOffset = cachedQueryModelState != null ? cachedQueryModelState.offset() : null;
⋮----
commandQueue.add(new HydrationRequest<>(runtime, completableFuture, id, currentState, currentOffset));
⋮----
} else if (isRuntimeDisabled(modelClass)) {
⋮----
return CompletableFuture.failedFuture(new QueryModelExecutionDisabledException(modelClass));
⋮----
return CompletableFuture.failedFuture(new QueryModelNotFoundException(modelClass));
⋮----
public void run() {
try (final Consumer<String, ProtocolRecord> indexConsumer = consumerFactory.createConsumer(
HostUtils.getHostName() + "-AkcesQueryModelController",
⋮----
process(indexConsumer);
⋮----
logger.info("AkcesQueryModelController is shutting down");
⋮----
commandQueue.drainTo(pendingRequests);
pendingRequests.forEach(request -> request.completableFuture.completeExceptionally(
new QueryModelExecutionCancelledException(request.runtime().getQueryModelClass())));
⋮----
Iterator<HydrationExecution<?>> iterator = hydrationExecutions.values().iterator();
while (iterator.hasNext()) {
HydrationExecution<?> execution = iterator.next();
execution.completableFuture.completeExceptionally(
new QueryModelExecutionCancelledException(execution.runtime().getQueryModelClass()));
iterator.remove();
⋮----
for (GDPRContextRepository gdprContextRepository : gdprContextRepositories.values()) {
⋮----
gdprContextRepository.close();
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
private void process(Consumer<String, ProtocolRecord> indexConsumer) {
⋮----
Map<TopicPartition, HydrationExecution<?>> newExecutions = processHydrationRequests(indexConsumer);
⋮----
hydrationExecutions.putAll(newExecutions);
indexConsumer.assign(Stream.concat(hydrationExecutions.keySet().stream(), gdprKeyPartitions.stream()).toList());
⋮----
if(!newExecutions.isEmpty()){
logger.info("Processing {} new HydrationExecutions", newExecutions.size());
⋮----
indexConsumer.endOffsets(newExecutions.keySet()).forEach((partition, endOffset) ->
hydrationExecutions.computeIfPresent(partition, (topicPartition, hydrationExecution) ->
hydrationExecution.withEndOffset(endOffset)));
⋮----
newExecutions.forEach((partition, execution) -> {
if (execution.currentOffset() != null) {
indexConsumer.seek(partition, execution.currentOffset());
} else if(hydrationExecutions.get(partition).endOffset() > 0) {
⋮----
indexConsumer.seek(partition, 0);
⋮----
indexConsumer.seekToBeginning(List.of(partition));
⋮----
if(!hydrationExecutions.isEmpty()) {
logger.info("Processing HydrationExecutions {}", hydrationExecutions);
⋮----
ConsumerRecords<String, ProtocolRecord> consumerRecords = indexConsumer.poll(Duration.ofMillis(10));
if(!consumerRecords.isEmpty()) {
logger.info("Processing {}", consumerRecords.partitions());
⋮----
if(!gdprKeyPartitions.isEmpty()) {
List<TopicPartition> gdprKeyPartitions = consumerRecords.partitions().stream()
.filter(topicPartition -> topicPartition.topic().equals("Akces-GDPRKeys")).toList();
logger.info("Processing {} GDPRKeyPartitions", gdprKeyPartitions.size());
⋮----
gdprContextRepositories.get(gdprKeyPartition).process(consumerRecords.records(gdprKeyPartition));
⋮----
List<TopicPartition> indexPartitions = consumerRecords.partitions().stream()
.filter(partition -> !partition.topic().equals("Akces-GDPRKeys")).toList();
logger.info("Processing {} indexPartitions", indexPartitions.size());
⋮----
hydrationExecutions.computeIfPresent(partition,
⋮----
processHydrationExecution(
hydrationExecution.runtime().shouldHandlePIIData() ? getGDPRContextRepository(hydrationExecution.id()) : null,
⋮----
consumerRecords.records(partition)));
⋮----
Iterator<HydrationExecution<?>> itr = hydrationExecutions.values().iterator();
while (itr.hasNext()) {
HydrationExecution<?> execution = itr.next();
if (execution.endOffset() <= indexConsumer.position(execution.indexPartition())) {
logger.info(
⋮----
execution.runtime().getIndexName(),
execution.id(),
execution.runtime().getName(),
execution.indexPartition(),
execution.endOffset(),
indexConsumer.position(execution.indexPartition()));
⋮----
execution.complete();
itr.remove();
queryModelStateCache.put(
execution.runtime().getName()+"-"+execution.id(),
⋮----
execution.currentState(),
indexConsumer.position(execution.indexPartition())));
⋮----
logger.error("Unrecoverable exception in AkcesQueryModelController while {}", processState, e);
⋮----
ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = indexConsumer.poll(Duration.ofMillis(10));
⋮----
gdprContextRepositories.get(gdprKeyPartition).process(gdprKeyRecords.records(gdprKeyPartition));
⋮----
initializedEndOffsets.computeIfPresent(gdprKeyPartition, (partition, endOffset) -> {
if (endOffset <= indexConsumer.position(gdprKeyPartition)) {
⋮----
if (gdprKeyRecords.isEmpty() && initializedEndOffsets.isEmpty()) {
⋮----
Iterator<QueryModelRuntime> iterator = enabledRuntimes.values().iterator();
⋮----
QueryModelRuntime queryModelRuntime = iterator.next();
⋮----
queryModelRuntime.validateDomainEventSchemas();
logger.info("Enabling {} QueryModelRuntime", queryModelRuntime.getName());
⋮----
logger.error(
⋮----
queryModelRuntime.getName(),
⋮----
disabledRuntimes.put(queryModelRuntime.getQueryModelClass(), queryModelRuntime);
⋮----
if(enabledRuntimes.isEmpty() && !disabledRuntimes.isEmpty()) {
logger.error("No QueryModelRuntimes enabled. This is an error. Shutting down");
⋮----
} else if (enabledRuntimes.values().stream().anyMatch(QueryModelRuntime::shouldHandlePIIData)) {
⋮----
logger.info("Loading GDPR keys");
⋮----
TopicDescription controlTopicDescription = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control");
totalPartitions = controlTopicDescription.partitions().size();
⋮----
gdprKeyPartitions.add(new TopicPartition("Akces-GDPRKeys", i));
⋮----
gdprKeyPartitions.forEach(partition -> {
gdprContextRepositories.put(partition, gdprContextRepositoryFactory.create("AkcesQueryModelController",partition.partition()));
⋮----
indexConsumer.assign(gdprKeyPartitions);
⋮----
indexConsumer.seek(partition, gdprContextRepositories.get(partition).getOffset() + 1);
⋮----
initializedEndOffsets = indexConsumer.endOffsets(gdprKeyPartitions);
⋮----
private GDPRContextRepository getGDPRContextRepository(String id) {
Integer partition = Math.abs(hashFunction.hashString(id, UTF_8).asInt()) % totalPartitions;
return gdprContextRepositories.get(new TopicPartition("Akces-GDPRKeys", partition));
⋮----
private Map<TopicPartition, HydrationExecution<?>> processHydrationRequests(Consumer<String, ProtocolRecord> indexConsumer) {
⋮----
HydrationRequest request = commandQueue.poll(100, TimeUnit.MILLISECONDS);
⋮----
logger.info("Processing HydrationRequest on index {} with id {} and runtime {}", request.runtime().getIndexName(), request.id(), request.runtime().getName());
⋮----
QueryModelRuntime runtime = request.runtime();
String topicName = getIndexTopicName(runtime.getIndexName(), request.id());
if (!indexConsumer.partitionsFor(topicName).isEmpty()) {
TopicPartition indexPartition = new TopicPartition(topicName, 0);
newExecutions.put(indexPartition, new HydrationExecution<>(runtime, request.completableFuture(), request.id(), request.currentState(), request.currentOffset(), indexPartition, null));
⋮----
logger.warn("KafkaTopic {} not found for HydrationRequest on index {} with id {}", topicName, request.runtime().getIndexName(), request.id());
request.completableFuture().completeExceptionally(new QueryModelIdNotFoundException(request.runtime().getQueryModelClass(), request.id()));
⋮----
request = commandQueue.poll();
⋮----
logger.warn("Interrupted while processing HydrationRequests", e);
⋮----
Thread.currentThread().interrupt();
⋮----
private <S extends QueryModelState> HydrationExecution<S> processHydrationExecution(@Nullable GDPRContextRepository gdprContextRepository,
⋮----
GDPRContext gdprContext = gdprContextRepository.get(execution.id());
logger.info("Setting GDPRContext {} for aggregateId {}", gdprContext.getClass().getSimpleName(), execution.id());
GDPRContextHolder.setCurrentGDPRContext(gdprContext);
⋮----
records.size(),
⋮----
execution.runtime().getName());
return execution.withCurrentState(execution.runtime().apply(
records.stream().map(record -> (DomainEventRecord) record.value())
.toList(), execution.currentState()));
⋮----
logger.error("Exception while processing HydrationExecution", e);
⋮----
new QueryModelExecutionException(
⋮----
execution.runtime().getQueryModelClass(),
⋮----
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
public void close() throws Exception {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesQueryModelController has been shutdown");
⋮----
logger.warn("AkcesQueryModelController did not shutdown within 10 seconds");
⋮----
public boolean isRunning() {
⋮----
HydrationExecution<S> withEndOffset(Long endOffset) {
⋮----
HydrationExecution<S> withCurrentState(S currentState) {
⋮----
void complete() {
⋮----
completableFuture.complete(currentState);
⋮----
completableFuture.completeExceptionally(new QueryModelIdNotFoundException(runtime.getQueryModelClass(), id));

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelRuntimeFactory.java
================
public class QueryModelRuntimeFactory<S extends QueryModelState> implements FactoryBean<QueryModelRuntime<S>>, ApplicationContextAware {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
public QueryModelRuntime<S> getObject() throws Exception {
return createRuntime(queryModel);
⋮----
public Class<?> getObjectType() {
⋮----
private QueryModelRuntime<S> createRuntime(QueryModel<S> queryModel) {
⋮----
QueryModelInfo queryModelInfo = queryModel.getClass().getAnnotation(QueryModelInfo.class);
⋮----
runtimeBuilder.setStateType(new QueryModelStateType<>(
queryModelInfo.value(),
queryModelInfo.version(),
queryModel.getStateClass(),
queryModelInfo.indexName()
⋮----
throw new IllegalStateException("Class implementing Aggregate must be annotated with @QueryModelInfo");
⋮----
runtimeBuilder.setQueryModelClass((Class<? extends QueryModel<S>>) queryModel.getClass());
runtimeBuilder.setObjectMapper(objectMapper);
⋮----
applicationContext.getBeansOfType(QueryModelEventHandlerFunction.class).values().stream()
.filter(adapter -> adapter.getQueryModel().equals(queryModel))
.forEach(adapter -> {
DomainEventType<?> type = adapter.getEventType();
if (adapter.isCreate()) {
runtimeBuilder.setCreateHandler(adapter);
runtimeBuilder.addDomainEvent(type);
⋮----
runtimeBuilder.addQueryModelEventHandler(type, adapter);
⋮----
return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/database/model/DefaultJdbcModel.java
================
public class DefaultJdbcModel extends JdbcDatabaseModel {
⋮----
public void handle(AccountCreatedEvent event) {
jdbcTemplate.update("""
⋮----
event.userId(),
event.country(),
event.firstName(),
event.lastName(),
event.email()

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/database/DatabaseModelRuntimeTests.java
================
public class DatabaseModelRuntimeTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withEnv("SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL","none")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
.withDatabaseName("akces")
.withUsername("akces")
.withPassword("akces")
⋮----
.withNetworkAliases("postgresql");
⋮----
public static class ContextInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
List.of(
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
"spring.datasource.url=" + postgresql.getJdbcUrl(),
⋮----
public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<?>> domainEventClasses) {
SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static void cleanUp() throws IOException {
⋮----
if (Files.exists(Paths.get("/tmp/akces"))) {
⋮----
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
void testContextLoads() {
assertNotNull(adminClient);
assertNotNull(schemaRegistryClient);
assertNotNull(consumerFactory);
assertNotNull(objectMapper);
assertNotNull(walletController);
assertNotNull(accountController);
assertNotNull(orderProcessManagerController);
assertNotNull(akcesClientController);
assertNotNull(defaultModelController);
⋮----
while (!walletController.isRunning() ||
!accountController.isRunning() ||
!orderProcessManagerController.isRunning() ||
!akcesClientController.isRunning() ||
!defaultModelController.isRunning()) {
Thread.onSpinWait();
⋮----
public void testFindBeans() {
⋮----
assertNotNull(applicationContext);
assertEquals(1, applicationContext.getBeansOfType(DatabaseModelRuntime.class).size());
⋮----
public void testCreateAccount() throws Exception {
⋮----
CreateAccountCommand command = new CreateAccountCommand(
⋮----
akcesClientController.sendAndForget("Default", userId, command);
⋮----
Thread.sleep(1000);
⋮----
JdbcTemplate jdbcTemplate = applicationContext.getBean(JdbcTemplate.class);
Map<String, Object> result = jdbcTemplate.queryForMap(
⋮----
assertEquals(userId, result.get("user_id"));
assertEquals("NL", result.get("country"));
assertEquals("John", result.get("first_name"));
assertEquals("Doe", result.get("last_name"));
assertEquals("john.doe@example.com", result.get("email"));

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/database/DatabaseModelTestConfiguration.java
================
public class DatabaseModelTestConfiguration {
⋮----
public SpringLiquibase liquibase(DataSource dataSource) {
SpringLiquibase liquibase = new SpringLiquibase();
liquibase.setChangeLog("classpath:db/changelog/liquibase.yaml");
liquibase.setDataSource(dataSource);

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/QueryModelTestConfiguration.java
================
public class QueryModelTestConfiguration {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/AggregateStateUpcastingHandlerFunctionAdapter.java
================
public class AggregateStateUpcastingHandlerFunctionAdapter<T extends AggregateState, R extends AggregateState>
⋮----
public void init() {
⋮----
methodHandle = MethodHandles.lookup().findVirtual(
aggregate.getClass(),
⋮----
MethodType.methodType(outputStateType.typeClass(), inputStateType.typeClass()));
⋮----
throw new RuntimeException(e);
⋮----
public R apply(@NotNull T state) {
⋮----
return (R) methodHandle.invoke(aggregate, state);
⋮----
public AggregateStateType<T> getInputType() {
⋮----
public AggregateStateType<R> getOutputType() {
⋮----
public Aggregate<? extends AggregateState> getAggregate() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/DomainEventUpcastingHandlerFunctionAdapter.java
================
public class DomainEventUpcastingHandlerFunctionAdapter<T extends DomainEvent, R extends DomainEvent>
⋮----
public void init() {
⋮----
methodHandle = MethodHandles.lookup().findVirtual(
aggregate.getClass(),
⋮----
MethodType.methodType(outputEventType.typeClass(), inputEventType.typeClass()));
⋮----
throw new RuntimeException(e);
⋮----
public R apply(@NotNull T event) {
⋮----
return (R) methodHandle.invoke(aggregate, event);
⋮----
public DomainEventType<T> getInputType() {
⋮----
public DomainEventType<R> getOutputType() {
⋮----
public Aggregate<? extends AggregateState> getAggregate() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/RocksDBAggregateStateRepository.java
================
public class RocksDBAggregateStateRepository implements AggregateStateRepository {
private static final Logger log = LoggerFactory.getLogger(RocksDBAggregateStateRepository.class);
⋮----
RocksDB.loadLibrary();
final Options options = new Options();
final TransactionDBOptions transactionDBOptions = new TransactionDBOptions();
options.setCreateIfMissing(true);
⋮----
this.rocksDBDataDir = new File(baseDir, partitionId);
⋮----
Files.createDirectories(this.rocksDBDataDir.getParentFile().toPath());
Files.createDirectories(this.rocksDBDataDir.getAbsoluteFile().toPath());
db = TransactionDB.open(options, transactionDBOptions, this.rocksDBDataDir.getAbsolutePath());
⋮----
initializeOffset();
log.info("RocksDB for partition {} initialized in folder {}", partitionId, this.rocksDBDataDir.getAbsolutePath());
⋮----
throw new AggregateStateRepositoryException("Error initializing RocksDB", e);
⋮----
public void close() {
⋮----
db.syncWal();
⋮----
log.error("Error syncing WAL. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
db.close();
⋮----
private void initializeOffset() {
⋮----
byte[] offsetBytes = db.get(OFFSET);
⋮----
lastOffset = Longs.fromByteArray(offsetBytes);
⋮----
throw new AggregateStateRepositoryException("Error initializing offset", e);
⋮----
private void updateOffset(long offset) {
⋮----
log.trace("Updated offset to {}", offset);
⋮----
public long getOffset() {
⋮----
public void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture) {
checkAggregateIdType(record.aggregateId());
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
⋮----
public void commit() {
if (!transactionStateRecordMap.isEmpty()) {
⋮----
Transaction transaction = db.beginTransaction(new WriteOptions());
⋮----
for (RecordAndMetadata<?> recordAndMetadata : transactionStateRecordMap.values()) {
transaction.put(keyBytes(recordAndMetadata.record().aggregateId()), serializer.serialize(topicName, recordAndMetadata.record()));
⋮----
long offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
transaction.put(OFFSET, Longs.toByteArray(offset));
transaction.commit();
transaction.close();
updateOffset(offset);
⋮----
throw new AggregateStateRepositoryException("Error committing records", e);
⋮----
transactionStateRecordMap.clear();
⋮----
public void rollback() {
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
long offset = consumerRecords.stream()
.map(ConsumerRecord::offset)
⋮----
transaction.put(keyBytes(consumerRecord.key()), serializer.serialize(topicName, consumerRecord.value()));
⋮----
throw new AggregateStateRepositoryException("Error processing records", e);
⋮----
public AggregateStateRecord get(String aggregateId) {
checkAggregateIdType(aggregateId);
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
byte[] keyBytes = keyBytes(aggregateId);
if (db.keyExists(keyBytes)) {
⋮----
return (AggregateStateRecord) deserializer.deserialize(topicName, db.get(keyBytes));
⋮----
throw new AggregateStateRepositoryException("Problem reading record with aggregateId " + aggregateId, e);
⋮----
private byte[] keyBytes(String aggregateId) {
⋮----
UUID aggregateUUID = UUID.fromString(aggregateId);
return ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array();
⋮----
return aggregateId.getBytes(Charsets.UTF_8);
⋮----
private void checkAggregateIdType(String aggregateId) {
⋮----
UUID.fromString(aggregateId);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/AkcesAggregateController.java
================
public class AkcesAggregateController extends Thread implements AutoCloseable, ConsumerRebalanceListener, AkcesRegistry, EnvironmentAware, ApplicationContextAware {
private static final Logger logger = LoggerFactory.getLogger(AkcesAggregateController.class);
⋮----
private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
super(aggregateRuntime.getName() + "-AkcesController");
⋮----
this.executorService = Executors.newCachedThreadPool(new CustomizableThreadFactory(aggregateRuntime.getName() + "AggregatePartitionThread-"));
⋮----
public void run() {
⋮----
controlRecordConsumerFactory.createConsumer(
aggregateRuntime.getName() + "-Akces-Control",
aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-Akces-Control",
⋮----
controlConsumer.subscribe(List.of("Akces-Control"), this);
⋮----
process();
⋮----
logger.info("Closing {} AggregatePartitions", aggregatePartitions.size());
aggregatePartitions.values().forEach(aggregatePartition -> {
⋮----
aggregatePartition.close();
⋮----
logger.error("Error closing AggregatePartition " + aggregatePartition.getId(), e);
⋮----
controlConsumer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing controlConsumer", e);
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
logger.error("Error in AkcesController", e);
⋮----
private void process() {
⋮----
processControlRecords();
⋮----
logger.error("Unrecoverable exception in AkcesController", e);
⋮----
TopicDescription controlTopicDescription = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control");
partitions = controlTopicDescription.partitions().size();
replicationFactor = (short) controlTopicDescription.partitions().getFirst().replicas().size();
⋮----
for (DomainEventType<?> domainEventType : aggregateRuntime.getProducedDomainEventTypes()) {
⋮----
aggregateRuntime.registerAndValidate(domainEventType);
⋮----
if (protocolRecordTypeNotYetProduced(domainEventType, PartitionUtils::toDomainEventTopicPartition)) {
aggregateRuntime.registerAndValidate(domainEventType, true);
⋮----
logger.warn("Cannot update schema for DomainEvent {} v{} because it has already been produced",
domainEventType.typeName(), domainEventType.version());
⋮----
for (CommandType<?> commandType : aggregateRuntime.getLocalCommandTypes()) {
⋮----
aggregateRuntime.registerAndValidate(commandType);
⋮----
if (protocolRecordTypeNotYetProduced(commandType, PartitionUtils::toCommandTopicPartition)) {
aggregateRuntime.registerAndValidate(commandType, true);
⋮----
logger.warn("Cannot update schema for Command {} v{} because it has already been produced",
commandType.typeName(), commandType.version());
⋮----
publishControlRecord(partitions);
⋮----
if (!partitionsToAssign.isEmpty()) {
⋮----
controlConsumer.seekToBeginning(partitionsToAssign);
⋮----
Map<TopicPartition, Long> initializedEndOffsets = controlConsumer.endOffsets(partitionsToAssign);
⋮----
ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
while (!initializedEndOffsets.isEmpty()) {
consumerRecords.forEach(record -> {
AkcesControlRecord controlRecord = record.value();
⋮----
if (aggregateServices.putIfAbsent(record.key(), aggregateServiceRecord) == null) {
logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
⋮----
logger.info("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
⋮----
if (consumerRecords.isEmpty()) {
initializedEndOffsets.entrySet().removeIf(entry -> entry.getValue() <= controlConsumer.position(entry.getKey()));
⋮----
consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
⋮----
for (DomainEventType<?> domainEventType : aggregateRuntime.getExternalDomainEventTypes()) {
⋮----
logger.error("Error registering external domain event type: {}:{}", domainEventType.typeName(), domainEventType.version(), e);
⋮----
for (CommandType<?> commandType : aggregateRuntime.getExternalCommandTypes()) {
⋮----
logger.error("Error registering external command type: {}:{}", commandType.typeName(), commandType.version(), e);
⋮----
AggregatePartition aggregatePartition = aggregatePartitions.remove(topicPartition.partition());
⋮----
logger.info("Stopping AggregatePartition {}", aggregatePartition.getId());
⋮----
logger.error("Error closing AggregatePartition", e);
⋮----
partitionsToRevoke.clear();
⋮----
AggregatePartition aggregatePartition = new AggregatePartition(
⋮----
topicPartition.partition(),
toCommandTopicPartition(aggregateRuntime, topicPartition.partition()),
toDomainEventTopicPartition(aggregateRuntime, topicPartition.partition()),
toAggregateStateTopicPartition(aggregateRuntime, topicPartition.partition()),
toGDPRKeysTopicPartition(aggregateRuntime, topicPartition.partition()),
aggregateRuntime.getExternalDomainEventTypes(),
⋮----
aggregatePartitions.put(aggregatePartition.getId(), aggregatePartition);
logger.info("Starting AggregatePartition {}", aggregatePartition.getId());
executorService.submit(aggregatePartition);
⋮----
partitionsToAssign.clear();
⋮----
private boolean protocolRecordTypeNotYetProduced(SchemaType<?> schemaType,
⋮----
try (Consumer<String, ProtocolRecord> consumer = consumerFactory.createConsumer(
aggregateRuntime.getName() + "-Akces-Control-TypeCheck",
aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-Akces-Control-TypeCheck",
⋮----
List<TopicPartition> partitionsList = IntStream.range(0, partitions)
.mapToObj(i -> createTopicPartition.apply(aggregateRuntime, i))
.toList();
consumer.assign(partitionsList);
consumer.seekToBeginning(partitionsList);
⋮----
Map<TopicPartition, Long> endOffsets = consumer.endOffsets(partitionsList);
while (!endOffsets.isEmpty()) {
⋮----
for (ConsumerRecord<String, ProtocolRecord> record : consumer.poll(Duration.ofMillis(10))) {
if (record.value().name().equals(schemaType.typeName()) &&
record.value().version() == schemaType.version()) {
⋮----
endOffsets.entrySet().removeIf(entry -> entry.getValue() <= consumer.position(entry.getKey()));
⋮----
logger.error(
⋮----
schemaType.typeName(),
schemaType.version(),
⋮----
private void processControlRecords() {
⋮----
if (!consumerRecords.isEmpty()) {
⋮----
if (!aggregateServices.containsKey(record.key())) {
⋮----
aggregateServices.put(record.key(), aggregateServiceRecord);
⋮----
private Boolean createIndexTopic(String indexName, String indexKey) {
⋮----
kafkaAdmin.createOrModifyTopics(
createCompactedTopic(getIndexTopicName(indexName, indexKey), 1, replicationFactor));
⋮----
logger.error("Error creating index topic: {}", indexName, e);
⋮----
private void publishControlRecord(int partitions) {
String transactionalId = aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-control";
try (Producer<String, AkcesControlRecord> controlProducer = controlProducerFactory.createProducer(transactionalId)) {
⋮----
AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
aggregateRuntime.getName(),
aggregateRuntime.getName() + COMMANDS_SUFFIX,
aggregateRuntime.getName() + DOMAINEVENTS_SUFFIX,
aggregateRuntime.getAllCommandTypes().stream()
.map(commandType ->
new AggregateServiceCommandType(
commandType.typeName(),
commandType.version(),
commandType.create(),
"commands." + commandType.typeName())).toList(),
aggregateRuntime.getProducedDomainEventTypes().stream().map(domainEventType ->
new AggregateServiceDomainEventType(
domainEventType.typeName(),
domainEventType.version(),
domainEventType.create(),
domainEventType.external(),
"domainevents." + domainEventType.typeName())).toList(),
aggregateRuntime.getExternalDomainEventTypes().stream().map(externalDomainEventType ->
⋮----
externalDomainEventType.typeName(),
externalDomainEventType.version(),
externalDomainEventType.create(),
externalDomainEventType.external(),
"domainevents." + externalDomainEventType.typeName())).toList());
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, aggregateRuntime.getName(), aggregateServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
logger.error("Error publishing CommandServiceRecord", e);
⋮----
public void close() throws Exception {
logger.info("Shutting down AkcesAggregateController");
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesAggregateController has been shutdown");
⋮----
logger.warn("AkcesAggregateController did not shutdown within 10 seconds");
⋮----
public void onPartitionsRevoked(Collection<TopicPartition> topicPartitions) {
⋮----
if (!topicPartitions.isEmpty()) {
⋮----
partitionsToRevoke.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, revoking partitions: {}",
topicPartitions.stream().map(TopicPartition::partition).toList());
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, revoking partitions: {}",
⋮----
public void onPartitionsAssigned(Collection<TopicPartition> topicPartitions) {
⋮----
partitionsToAssign.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, assigning partitions : {}",
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, assigning partitions : {}",
⋮----
public CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass) {
⋮----
CommandInfo commandInfo = commandClass.getAnnotation(CommandInfo.class);
⋮----
List<AggregateServiceRecord> services = aggregateServices.values().stream()
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandInfo))
⋮----
if (services.size() == 1) {
AggregateServiceRecord aggregateServiceRecord = services.getFirst();
if (aggregateRuntime.getName().equals(aggregateServiceRecord.aggregateName())) {
⋮----
return aggregateRuntime.getLocalCommandType(commandInfo.type(), commandInfo.version());
⋮----
return new CommandType<>(commandInfo.type(),
commandInfo.version(),
⋮----
hasPIIDataAnnotation(commandClass));
⋮----
throw new IllegalStateException("Cannot determine where to send command " + commandClass.getName());
⋮----
throw new IllegalStateException("Command class " + commandClass.getName() + " is not annotated with @CommandInfo");
⋮----
private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, CommandInfo commandInfo) {
⋮----
if (supportedCommand.typeName().equals(commandInfo.type()) &&
supportedCommand.version() == commandInfo.version()) {
⋮----
private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, CommandType<?> commandType) {
⋮----
if (supportedCommand.typeName().equals(commandType.typeName()) &&
supportedCommand.version() == commandType.version()) {
⋮----
private boolean producesDomainEvent(List<AggregateServiceDomainEventType> producedEvents, DomainEventType<?> externalDomainEventType) {
⋮----
if (producedEvent.typeName().equals(externalDomainEventType.typeName()) &&
producedEvent.version() == externalDomainEventType.version()) {
⋮----
public String resolveTopic(@Nonnull Class<? extends Command> commandClass) {
return resolveTopic(resolveType(commandClass));
⋮----
public String resolveTopic(@Nonnull CommandType<?> commandType) {
⋮----
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType))
⋮----
return services.getFirst().commandTopic();
⋮----
throw new IllegalStateException("Cannot determine where to send command " + commandType.typeName() + " v" + commandType.version());
⋮----
public String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType) {
⋮----
.filter(commandServiceRecord -> producesDomainEvent(commandServiceRecord.producedEvents(), externalDomainEventType))
⋮----
return services.getFirst().domainEventTopic();
⋮----
throw new IllegalStateException("Cannot determine which service produces DomainEvent " + externalDomainEventType.typeName() + " v" + externalDomainEventType.version());
⋮----
public Integer resolvePartition(@Nonnull String aggregateId) {
return Math.abs(hashFunction.hashString(aggregateId, UTF_8).asInt()) % partitions;
⋮----
public boolean isRunning() {
return processState == RUNNING && aggregatePartitions.values().stream().allMatch(AggregatePartition::isProcessing);
⋮----
public void setEnvironment(Environment environment) {
this.forceRegisterOnIncompatible = environment.getProperty("akces.aggregate.schemas.forceRegister", Boolean.class, false);
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/Account.java
================
public final class Account implements Aggregate<AccountState> {
⋮----
public String getName() {
⋮----
public Class<AccountState> getStateClass() {
⋮----
public Stream<AccountCreatedEvent> create(CreateAccountCommand cmd, AccountState isNull) {
return Stream.of(new AccountCreatedEvent(cmd.userId(), cmd.country(), cmd.firstName(), cmd.lastName(), cmd.email()));
⋮----
public AccountCreatedEventV2 cast(AccountCreatedEvent event) {
return new AccountCreatedEventV2(event.userId(), event.country(), event.firstName(), event.lastName(), event.email(), false);
⋮----
public AccountState create(@NotNull AccountCreatedEventV2 event, AccountState isNull) {
return new AccountState(event.userId(), event.country(), event.firstName(), event.lastName(), event.email());

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/Wallet.java
================
public final class Wallet implements Aggregate<WalletStateV2> {
⋮----
public String getName() {
⋮----
public Class<WalletStateV2> getStateClass() {
⋮----
public WalletStateV2 upcast(WalletState state) {
⋮----
return new WalletStateV2(state.id(), state.balances().stream().map(
balance -> balance.reservedAmount().compareTo(BigDecimal.ZERO) > 0 ?
new WalletStateV2.Balance(balance.currency(), balance.amount(), List.of(new WalletStateV2.Reservation("v1-reservedAmount", balance.reservedAmount()))) :
new WalletStateV2.Balance(balance.currency(), balance.amount())).toList());
⋮----
public @NotNull Stream<DomainEvent> create(@NotNull CreateWalletCommand cmd, WalletStateV2 isNull) {
return Stream.of(new WalletCreatedEvent(cmd.id()), new BalanceCreatedEvent(cmd.id(), cmd.currency()));
⋮----
public @NotNull Stream<DomainEvent> create(@NotNull AccountCreatedEvent event, WalletStateV2 isNull) {
⋮----
return Stream.of(new WalletCreatedEvent(event.getAggregateId()), new BalanceCreatedEvent(event.getAggregateId(), "EUR"));
⋮----
public @NotNull Stream<DomainEvent> createBalance(@NotNull CreateBalanceCommand cmd, @NotNull WalletStateV2 currentState) {
boolean balanceExists = currentState.balances().stream()
.anyMatch(balance -> balance.currency().equals(cmd.currency()));
⋮----
return Stream.of(new BalanceAlreadyExistsErrorEvent(cmd.id(), cmd.currency()));
⋮----
return Stream.of(new BalanceCreatedEvent(cmd.id(), cmd.currency()));
⋮----
public Stream<DomainEvent> credit(@NotNull CreditWalletCommand cmd, @NotNull WalletStateV2 currentState) {
WalletStateV2.Balance balance = currentState.balances().stream().filter(b -> b.currency().equals(cmd.currency())).findFirst().orElse(null);
⋮----
return Stream.of(new InvalidCurrencyErrorEvent(cmd.id(), cmd.currency()));
⋮----
if (cmd.amount().compareTo(BigDecimal.ZERO) < 0) {
⋮----
return Stream.of(new InvalidAmountErrorEvent(cmd.id(), cmd.currency()));
⋮----
return Stream.of(new WalletCreditedEvent(currentState.id(), cmd.currency(), cmd.amount(), balance.amount().add(cmd.amount())));
⋮----
public Stream<DomainEvent> makeReservation(ReserveAmountCommand command, WalletStateV2 state) {
WalletStateV2.Balance balance = state.balances().stream().filter(b -> b.currency().equals(command.currency())).findFirst().orElse(null);
⋮----
return Stream.of(new InvalidCurrencyErrorEvent(command.userId(), command.currency(), command.referenceId()));
⋮----
if (command.amount().compareTo(BigDecimal.ZERO) < 0) {
⋮----
return Stream.of(new InvalidAmountErrorEvent(command.userId(), command.currency()));
⋮----
if (balance.getAvailableAmount().compareTo(command.amount()) >= 0) {
return Stream.of(new AmountReservedEvent(command.userId(), command.currency(), command.amount(), command.referenceId()));
⋮----
return Stream.of(new InsufficientFundsErrorEvent(command.userId(), command.currency(), balance.getAvailableAmount(), command.amount(), command.referenceId()));
⋮----
public @NotNull WalletStateV2 create(@NotNull WalletCreatedEvent event, WalletStateV2 isNull) {
return new WalletStateV2(event.id(), new ArrayList<>());
⋮----
public @NotNull WalletStateV2 createBalance(@NotNull BalanceCreatedEvent event, WalletStateV2 state) {
List<WalletStateV2.Balance> balances = new ArrayList<>(state.balances());
balances.add(new WalletStateV2.Balance(event.currency(), BigDecimal.ZERO));
return new WalletStateV2(state.id(), balances);
⋮----
public @NotNull WalletStateV2 credit(@NotNull WalletCreditedEvent event, @NotNull WalletStateV2 state) {
return new WalletStateV2(state.id(), state.balances().stream().map(b -> {
if (b.currency().equals(event.currency())) {
return new WalletStateV2.Balance(b.currency(), b.amount().add(event.amount()));
⋮----
}).toList());
⋮----
public @NotNull WalletStateV2 reserveAmount(@NotNull AmountReservedEvent event, @NotNull WalletStateV2 state) {
⋮----
List<WalletStateV2.Reservation> updatedReservations = new ArrayList<>(b.reservations());
updatedReservations.add(new WalletStateV2.Reservation(event.referenceId(), event.amount()));
return new WalletStateV2.Balance(b.currency(), b.amount(), updatedReservations);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/JsonSchemaTests.java
================
public class JsonSchemaTests {
private static @NotNull SchemaGenerator createSchemaGenerator() {
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
return new SchemaGenerator(configBuilder.build());
⋮----
public void testSchemaCompatibility() throws IOException {
SchemaGenerator generator = createSchemaGenerator();
⋮----
ObjectNode schemaV1 = generator.generateSchema(AccountCreatedEvent.class);
ObjectNode schemaV2 = generator.generateSchema(AccountCreatedEventV2.class);
ObjectNode schemaV3 = generator.generateSchema(AccountCreatedEventV3.class);
⋮----
JsonSchema schema1 = new JsonSchema(schemaV1);
JsonSchema schema2 = new JsonSchema(schemaV2);
JsonSchema schema3 = new JsonSchema(schemaV3);
⋮----
List<Difference> differencesV2 = SchemaDiff.compare(schema1.rawSchema(), schema2.rawSchema())
.stream().filter(diff ->
!SchemaDiff.COMPATIBLE_CHANGES.contains(diff.getType()) &&
!Difference.Type.REQUIRED_PROPERTY_ADDED_TO_UNOPEN_CONTENT_MODEL.equals(diff.getType())).toList();
⋮----
assertEquals(0, differencesV2.size());
⋮----
List<Difference> differencesV3 = SchemaDiff.compare(schema2.rawSchema(), schema3.rawSchema())
⋮----
assertEquals(0, differencesV3.size());
⋮----
schema1.validate(schema1.toJson(new AccountCreatedEvent("1", "Musk", AccountTypeV1.PREMIUM)));
⋮----
schema2.validate(schema2.toJson(new AccountCreatedEventV2("1", "Musk", AccountTypeV2.PREMIUM, "Elon", "US")));
⋮----
schema3.validate(schema3.toJson(new AccountCreatedEventV3("1", "Musk", AccountTypeV2.GOLD, "Elon", "NL", "Amsterdam")));
⋮----
public void testNullableString() throws IOException {
⋮----
JsonNode schema = generator.generateSchema(InvalidAmountErrorEvent.class);
JsonSchema jsonSchema = new JsonSchema(schema);
⋮----
jsonSchema.validate(jsonSchema.toJson(new InvalidAmountErrorEvent(UUID.randomUUID().toString(), "USD")));
⋮----
public void testNullForNotNullField() {
⋮----
ValidationException exception = Assertions.assertThrows(ValidationException.class, () -> {
jsonSchema.validate(jsonSchema.toJson(new InvalidAmountErrorEvent(UUID.randomUUID().toString(), null)));
⋮----
assertEquals("#/currency", exception.getPointerToViolation());
assertEquals("#/currency: expected type: String, found: Null", exception.getMessage());
⋮----
public void testCommandWithBigDecimal() throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
⋮----
JsonNode schema = generator.generateSchema(CreditWalletCommand.class);
System.out.println(schema);
⋮----
JsonNode jsonNode = objectMapper.valueToTree(new CreditWalletCommand("de6f87c6-0e4a-4f20-8c06-659fe5bcb7bc", "USD", new BigDecimal("100.00"), null));
jsonSchema.validate(jsonNode);
⋮----
String serialized = objectMapper.writeValueAsString(jsonNode);
System.out.println(serialized);
⋮----
public void testDeepEquals() throws JsonProcessingException {
⋮----
Assertions.assertEquals(registeredSchemaString, localSchemaString);
⋮----
JsonSchema localSchema = new JsonSchema(objectMapper.readValue(localSchemaString, Schema.class));
JsonSchema registeredSchema = new JsonSchema(objectMapper.readValue(registeredSchemaString, Schema.class));
⋮----
Assertions.assertTrue(registeredSchema.deepEquals(localSchema));
⋮----
public void renderSchemas() {
⋮----
System.out.println(generator.generateSchema(org.elasticsoftware.akcestest.aggregate.account.CreateAccountCommand.class));
System.out.println(generator.generateSchema(org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent.class));
System.out.println(generator.generateSchema(org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEventV2.class));
System.out.println(generator.generateSchema(org.elasticsoftware.akcestest.aggregate.orders.BuyOrderCreatedEvent.class));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/AggregateServiceApplicationTests.java
================
public class AggregateServiceApplicationTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withEnv("SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL","none")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void cleanUp() throws IOException {
if (Files.exists(Paths.get("/tmp/akces"))) {
⋮----
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
public void testAggregateLoading() {
⋮----
Assertions.assertNotNull(applicationContext.getBean("AccountAggregateRuntimeFactory"));
Assertions.assertNotNull(akcesAggregateController);
Assertions.assertNotNull(consumerFactory);
Assertions.assertNotNull(producerFactory);
Assertions.assertNotNull(controlConsumerFactory);
Assertions.assertThrows(NoSuchBeanDefinitionException.class, () -> applicationContext.getBean("WalletAggregateRuntimeFactory"));
Assertions.assertThrows(NoSuchBeanDefinitionException.class, () -> applicationContext.getBean("OrderProcessManagerAggregateRuntimeFactory"));
⋮----
Consumer<String, AkcesControlRecord> controlConsumer = controlConsumerFactory.createConsumer("Test-AkcesControl", "test-akces-control");
⋮----
controlConsumer.subscribe(List.of("Akces-Control"));
controlConsumer.poll(Duration.ofMillis(1000));
controlConsumer.seekToBeginning(controlConsumer.assignment());
⋮----
ConsumerRecords<String, AkcesControlRecord> controlRecords = new ConsumerRecords<>(Collections.emptyMap());
while (controlRecords.isEmpty()) {
controlRecords = controlConsumer.poll(Duration.ofMillis(1000));
⋮----
controlConsumer.close();
⋮----
while (!akcesAggregateController.isRunning()) {
Thread.onSpinWait();
⋮----
public static class Initializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
SchemaRegistryClient src = new CachedSchemaRegistryClient("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), 100);
prepareExternalSchemas(src, List.of(AccountCreatedEvent.class));
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/RuntimeTests.java
================
public class RuntimeTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withEnv("SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL","none")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void prepareExternalServices(String bootstrapServers) {
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(new ObjectMapper());
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
⋮----
List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
List.of());
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", aggregateServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
public static void cleanUp() throws IOException {
⋮----
if (Files.exists(Paths.get("/tmp/akces"))) {
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.filter(File::isDirectory)
.forEach(file -> System.out.println(file.getAbsolutePath()));
⋮----
.forEach(File::delete);
⋮----
public static Stream<TopicPartition> generateTopicPartitions(String topic, int partitions) {
return IntStream.range(0, partitions)
.mapToObj(i -> new TopicPartition(topic, i));
⋮----
public static <C extends Command> void prepareOldCommandSchemas(SchemaRegistryClient src) {
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
src.register("commands.CreateWallet",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.CreateWalletCommand.class), List.of(), Map.of(), 1),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
public static <D extends DomainEvent> void prepareOldDomainEventSchemas(SchemaRegistryClient src) {
⋮----
src.register("domainevents.BalanceCreated",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.BalanceCreatedEvent.class), List.of(), Map.of(), 1),
⋮----
src.register("domainevents.BuyOrderPlaced",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.BuyOrderPlacedEvent.class), List.of(), Map.of(), 1),
⋮----
src.register("domainevents.WalletCredited",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.WalletCreditedEvent.class), List.of(), Map.of(), 1),
⋮----
public void testKafkaAdminClient() {
assertNotNull(adminClient);
⋮----
adminClient.describeTopics(
⋮----
assertNotNull(topics);
assertFalse(topics.isEmpty());
⋮----
public void createSchemas() throws RestClientException, IOException {
while (!walletAggregateController.isRunning() ||
!accountAggregateController.isRunning() ||
!orderProcessManagerAggregateController.isRunning() ||
!akcesClient.isRunning()) {
Thread.onSpinWait();
⋮----
System.out.println(schemaRegistryClient.getAllSubjects());
⋮----
public void testAkcesControl() throws JsonProcessingException {
assertNotNull(walletAggregateController);
assertNotNull(accountAggregateController);
assertNotNull(orderProcessManagerAggregateController);
assertNotNull(akcesClient);
⋮----
Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test");
Consumer<String, AkcesControlRecord> controlConsumer = controlConsumerFactory.createConsumer("Test-AkcesControl", "test-akces-control");
⋮----
TopicPartition controlParition = new TopicPartition("Akces-Control", 0);
controlConsumer.assign(List.of(controlParition));
controlConsumer.seekToBeginning(controlConsumer.assignment());
Map<TopicPartition, Long> endOffsets = controlConsumer.endOffsets(controlConsumer.assignment());
⋮----
while (endOffsets.getOrDefault(controlParition, 0L) > controlConsumer.position(controlParition)) {
ConsumerRecords<String, AkcesControlRecord> controlRecords = controlConsumer.poll(Duration.ofMillis(1000));
if (!controlRecords.isEmpty()) {
for (ConsumerRecord<String, AkcesControlRecord> record : controlRecords.records(controlParition)) {
System.out.println(objectMapper.writeValueAsString(record.value()));
⋮----
controlConsumer.close();
⋮----
while (!walletAggregateController.isRunning()) {
⋮----
CreateWalletCommand command = new CreateWalletCommand(userId, "USD");
CommandRecord commandRecord = new CommandRecord(null, "CreateWallet", 1, objectMapper.writeValueAsBytes(command), PayloadEncoding.JSON, command.getAggregateId(), null, null);
String topicName = walletAggregateController.resolveTopic(command.getClass());
int partition = walletAggregateController.resolvePartition(command.getAggregateId());
⋮----
testProducer.beginTransaction();
testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
testProducer.commitTransaction();
⋮----
TopicPartition aggregateStatePartition = new TopicPartition("Wallet-AggregateState", partition);
TopicPartition domainEventsPartition = new TopicPartition("Wallet-DomainEvents", partition);
⋮----
testConsumer.assign(List.of(aggregateStatePartition, domainEventsPartition));
⋮----
testConsumer.seekToBeginning(testConsumer.assignment());
ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
while (records.isEmpty()) {
⋮----
records = testConsumer.poll(Duration.ofMillis(250));
⋮----
assertFalse(records.isEmpty());
⋮----
CreditWalletCommand creditCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("100.00"));
CommandRecord creditCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(creditCommand), PayloadEncoding.JSON, creditCommand.getAggregateId(), null, null);
⋮----
testProducer.send(new ProducerRecord<>(topicName, partition, creditCommandRecord.aggregateId(), creditCommandRecord));
⋮----
CreditWalletCommand invalidCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("-100.00"));
CommandRecord invalidCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(invalidCommand), PayloadEncoding.JSON, invalidCommand.getAggregateId(), null, null);
⋮----
testProducer.send(new ProducerRecord<>(topicName, partition, invalidCommandRecord.aggregateId(), invalidCommandRecord));
⋮----
assertTrue(records.records(aggregateStatePartition).isEmpty());
⋮----
assertEquals(1, records.records(domainEventsPartition).size());
⋮----
DomainEventRecord protocolRecord = (DomainEventRecord) records.records(domainEventsPartition).getFirst().value();
assertEquals("InvalidAmountError", protocolRecord.name());
⋮----
testConsumer.close();
testProducer.close();
⋮----
public void testBatchedCommands() throws JsonProcessingException {
⋮----
List<String> userIds = List.of(
⋮----
Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
⋮----
Map<TopicPartition, Long> endOffsets = testConsumer.endOffsets(
Stream.concat(
generateTopicPartitions("Wallet-AggregateState", 3),
generateTopicPartitions("Wallet-DomainEvents", 3))
.toList());
⋮----
testConsumer.subscribe(List.of("Wallet-AggregateState", "Wallet-DomainEvents"), new ConsumerRebalanceListener() {
⋮----
public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
⋮----
public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
partitions.forEach(partition -> testConsumer.seek(partition, endOffsets.get(partition)));
⋮----
while (allRecords.size() < 40) {
records.forEach(record -> allRecords.add(record.value()));
⋮----
assertEquals(40, allRecords.size());
⋮----
public void testCreateViaExternalDomainEvent() throws JsonProcessingException {
⋮----
CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Fahim", "Zuijderwijk", "FahimZuijderwijk@jourrapide.com");
CommandRecord commandRecord = new CommandRecord(
⋮----
objectMapper.writeValueAsBytes(command),
⋮----
command.getAggregateId(),
⋮----
while (allRecords.size() < 4) {
⋮----
assertEquals(4, allRecords.size());
⋮----
public void testGDPREncryption() throws IOException {
⋮----
generateTopicPartitions("Account-AggregateState", 3),
generateTopicPartitions("Account-DomainEvents", 3))
⋮----
testConsumer.subscribe(List.of("Account-AggregateState", "Account-DomainEvents"), new ConsumerRebalanceListener() {
⋮----
while (allRecords.size() < 2) {
⋮----
assertEquals(2, allRecords.size());
⋮----
assertInstanceOf(DomainEventRecord.class, allRecords.get(1));
DomainEventRecord domainEventRecord = (DomainEventRecord) allRecords.get(1);
AccountCreatedEvent accountCreatedEvent = objectMapper.readValue(domainEventRecord.payload(), AccountCreatedEvent.class);
assertNotEquals("Fahim", accountCreatedEvent.firstName());
assertNotEquals("Zuijderwijk", accountCreatedEvent.lastName());
assertNotEquals("FahimZuijderwijk@jourrapide.com", accountCreatedEvent.email());
AggregateStateRecord stateRecord = (AggregateStateRecord) allRecords.getFirst();
AccountState accountState = objectMapper.readValue(stateRecord.payload(), AccountState.class);
assertNotEquals("Fahim", accountState.firstName());
assertNotEquals("Zuijderwijk", accountState.lastName());
assertNotEquals("FahimZuijderwijk@jourrapide.com", accountState.email());
⋮----
public void testDomainEventIndexing() throws IOException {
⋮----
TopicDescription topicDescription = getTopicDescription("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex");
⋮----
Thread.sleep(1000);
⋮----
Thread.currentThread().interrupt();
⋮----
topicDescription = getTopicDescription("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex");
⋮----
testConsumer.assign(generateTopicPartitions("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex", 1).toList());
⋮----
List<String> names = allRecords.stream().map(ProtocolRecord::name).toList();
⋮----
assertEquals("AccountCreated", names.getFirst());
assertTrue(names.indexOf("WalletCreated") < names.indexOf("BalanceCreated"));
assertTrue(names.contains("UserOrderProcessesCreated"));
⋮----
public void testDomainEventIndexingWithErrorEvents() throws IOException {
⋮----
TopicDescription topicDescription = getTopicDescription("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex");
⋮----
topicDescription = getTopicDescription("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex");
⋮----
testConsumer.assign(generateTopicPartitions("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex", 1).toList());
⋮----
while (allRecords.size() < 3) {
⋮----
assertEquals(3, allRecords.size());
⋮----
assertEquals("WalletCreated", allRecords.getFirst().name());
assertEquals("BalanceCreated", allRecords.get(1).name());
assertEquals("WalletCredited", allRecords.getLast().name());
⋮----
public void testWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
List<DomainEvent> result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);
⋮----
Assertions.assertNotNull(result);
Assertions.assertEquals(2, result.size());
assertInstanceOf(WalletCreatedEvent.class, result.getFirst());
assertInstanceOf(BalanceCreatedEvent.class, result.getLast());
⋮----
public void testOrderFlowWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Bella", "Fowler", "bella.fowler@example.com");
⋮----
Assertions.assertEquals(1, result.size());
assertInstanceOf(AccountCreatedEvent.class, result.getFirst());
⋮----
akcesClient.sendAndForget("TEST_TENANT", new CreditWalletCommand(userId, "EUR", new BigDecimal("100.00")));
⋮----
PlaceBuyOrderCommand orderCommand = new PlaceBuyOrderCommand(userId, new FxMarket("USDEUR", "USD", "EUR"), new BigDecimal("90.00"), new BigDecimal("1.05"), "trade-1");
result = akcesClient.send("TEST_TENANT", orderCommand).toCompletableFuture().get(10, TimeUnit.SECONDS);
⋮----
assertInstanceOf(BuyOrderCreatedEvent.class, result.getFirst());
⋮----
public void testAggregateAlreadyExistsErrorWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
CreateAccountCommand command = new CreateAccountCommand(userId, "USA", "Angelo", "Plummer", "AngeloRPlummer@rhyta.com");
⋮----
result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);
⋮----
assertInstanceOf(AggregateAlreadyExistsErrorEvent.class, result.getFirst());
⋮----
public TopicDescription getTopicDescription(String topic) {
⋮----
return adminClient.describeTopics(topic).get(topic);
⋮----
if (e.getCause().getClass().equals(ExecutionException.class) &&
e.getCause().getCause().getClass().equals(UnknownTopicOrPartitionException.class)) {
⋮----
public static class DataSourceInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
SchemaRegistryClient src = new CachedSchemaRegistryClient("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), 100);
prepareExternalSchemas(src, List.of(AccountCreatedEvent.class));
prepareOldCommandSchemas(src);
prepareOldDomainEventSchemas(src);
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceDomainEventType.java
================
public <E extends DomainEvent> DomainEventType<E> toLocalDomainEventType(Class<E> typeClass, boolean error) {
return new DomainEventType<>(typeName, version, typeClass, create, external, error, hasPIIDataAnnotation(typeClass));

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/DomainEventType.java
================
public String getSchemaPrefix() {
⋮----
public boolean relaxExternalValidation() {

================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/AggregateInfo.java
================


================
File: main/runtime/src/main/java/org/elasticsoftware/akces/aggregate/AggregateRuntime.java
================
public interface AggregateRuntime {
⋮----
String getName();
⋮----
Class<? extends Aggregate<?>> getAggregateClass();
⋮----
void handleCommandRecord(CommandRecord commandRecord,
⋮----
void handleExternalDomainEventRecord(DomainEventRecord eventRecord,
⋮----
Collection<DomainEventType<?>> getAllDomainEventTypes();
⋮----
Collection<DomainEventType<?>> getProducedDomainEventTypes();
⋮----
Collection<DomainEventType<?>> getExternalDomainEventTypes();
⋮----
Collection<CommandType<?>> getAllCommandTypes();
⋮----
Collection<CommandType<?>> getLocalCommandTypes();
⋮----
Collection<CommandType<?>> getExternalCommandTypes();
⋮----
CommandType<?> getLocalCommandType(String type, int version);
⋮----
void registerAndValidate(DomainEventType<?> domainEventType, boolean forceRegisterOnIncompatible) throws SchemaException;
⋮----
default void registerAndValidate(DomainEventType<?> domainEventType) throws SchemaException {
registerAndValidate(domainEventType, false);
⋮----
void registerAndValidate(CommandType<?> commandType,  boolean forceRegisterOnIncompatible) throws SchemaException;
⋮----
default void registerAndValidate(CommandType<?> commandType) throws SchemaException {
registerAndValidate(commandType, false);
⋮----
Command materialize(CommandType<?> commandType, CommandRecord commandRecord) throws IOException;
⋮----
byte[] serialize(Command command) throws SerializationException;
⋮----
boolean shouldGenerateGDPRKey(CommandRecord commandRecord);
⋮----
boolean shouldGenerateGDPRKey(DomainEventRecord eventRecord);
⋮----
boolean requiresGDPRContext(DomainEventRecord eventRecord);
⋮----
boolean requiresGDPRContext(CommandRecord eventRecord);
⋮----
boolean shouldHandlePIIData();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/CommandHandlerFunctionAdapter.java
================
public class CommandHandlerFunctionAdapter<S extends AggregateState, C extends Command, E extends DomainEvent>
⋮----
public void init() {
⋮----
adapterMethodHandle = MethodHandles.lookup().findVirtual(
aggregate.getClass(),
⋮----
MethodType.methodType(Stream.class, commandType.typeClass(), stateClass));
⋮----
throw new RuntimeException(e);
⋮----
public Stream<E> apply(C command, S state) {
⋮----
return (Stream<E>) adapterMethodHandle.invoke(aggregate, command, state);
⋮----
public boolean isCreate() {
return commandType.create();
⋮----
public CommandType<C> getCommandType() {
⋮----
public Aggregate<S> getAggregate() {
⋮----
public List<DomainEventType<E>> getProducedDomainEventTypes() {
⋮----
public List<DomainEventType<E>> getErrorEventTypes() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/EventHandlerFunctionAdapter.java
================
public class EventHandlerFunctionAdapter<S extends AggregateState, InputEvent extends DomainEvent, E extends DomainEvent> implements EventHandlerFunction<S, InputEvent, E> {
⋮----
public void init() {
⋮----
methodHandle = MethodHandles.lookup().findVirtual(
aggregate.getClass(),
⋮----
MethodType.methodType(Stream.class, domainEventType.typeClass(), stateClass));
⋮----
throw new RuntimeException(e);
⋮----
public Stream<E> apply(@NotNull InputEvent event, S state) {
⋮----
return (Stream<E>) methodHandle.invoke(aggregate, event, state);
⋮----
public DomainEventType<InputEvent> getEventType() {
⋮----
public Aggregate<S> getAggregate() {
⋮----
public boolean isCreate() {
return domainEventType.create();
⋮----
public List<DomainEventType<E>> getProducedDomainEventTypes() {
⋮----
public List<DomainEventType<E>> getErrorEventTypes() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/EventSourcingHandlerFunctionAdapter.java
================
public class EventSourcingHandlerFunctionAdapter<S extends AggregateState, E extends DomainEvent> implements EventSourcingHandlerFunction<S, E> {
⋮----
public void init() {
⋮----
methodHandle = MethodHandles.lookup().findVirtual(
aggregate.getClass(),
⋮----
MethodType.methodType(stateClass, domainEventType.typeClass(), stateClass));
⋮----
throw new RuntimeException(e);
⋮----
public @NotNull S apply(@NotNull E event, S state) {
⋮----
return (S) methodHandle.invoke(aggregate, event, state);
⋮----
public DomainEventType<E> getEventType() {
⋮----
public Aggregate<S> getAggregate() {
⋮----
public boolean isCreate() {
return domainEventType.create();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartition.java
================
public class AggregatePartition implements Runnable, AutoCloseable, CommandBus {
private static final Logger logger = LoggerFactory.getLogger(AggregatePartition.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
⋮----
this.stateRepository = stateRepositoryFactory.create(runtime, id);
⋮----
this.gdprContextRepository = gdprContextRepositoryFactory.create(runtime.getName(), id);
⋮----
public Integer getId() {
⋮----
public void run() {
⋮----
this.aggregatePartitionThread = Thread.currentThread();
⋮----
AggregatePartitionCommandBus.registerCommandBus(this);
logger.info("Starting AggregatePartition {} of {}Aggregate", id, runtime.getName());
this.consumer = consumerFactory.createConsumer(
runtime.getName() + "Aggregate-partition-" + id,
runtime.getName() + "Aggregate-partition-" + id + "-" + HostUtils.getHostName(),
⋮----
this.producer = producerFactory.createProducer(runtime.getName() + "Aggregate-partition-" + id + "-" + HostUtils.getHostName());
⋮----
externalDomainEventTypes.forEach(domainEventType -> {
String topic = ackesRegistry.resolveTopic(domainEventType);
externalEventPartitions.add(new TopicPartition(topic, id));
⋮----
consumer.assign(Stream.concat(Stream.concat(
Stream.of(commandPartition, domainEventPartition, statePartition), runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty()),
externalEventPartitions.stream())
.toList());
logger.info("Assigned partitions {} for AggregatePartition {} of {}Aggregate", consumer.assignment(), id, runtime.getName());
⋮----
process();
⋮----
logger.info("Shutting down AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
logger.error("Unexpected error in AggregatePartition {} of {}Aggregate", id, runtime.getName(), t);
⋮----
consumer.close(Duration.ofSeconds(5));
producer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing consumer/producer", e);
⋮----
stateRepository.close();
⋮----
logger.error("Error closing state repository", e);
⋮----
gdprContextRepository.close();
⋮----
logger.error("Error closing gdpr context repository", e);
⋮----
AggregatePartitionCommandBus.registerCommandBus(null);
⋮----
logger.info("Finished Shutting down AggregatePartition {} of {}Aggregate", id, runtime.getName());
shutdownLatch.countDown();
⋮----
public void close() throws InterruptedException {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AggregatePartition={} has been shutdown", id);
⋮----
logger.warn("AggregatePartition={} did not shutdown within 10 seconds", id);
⋮----
public void send(Command command) {
⋮----
if (Thread.currentThread() != aggregatePartitionThread) {
throw new IllegalStateException("send() can only be called from the AggregatePartition thread");
⋮----
CommandType<?> commandType = ackesRegistry.resolveType(command.getClass());
⋮----
runtime.registerAndValidate(commandType);
⋮----
logger.error("Problem registering command {}", commandType.typeName(), e);
⋮----
throw new RuntimeException(e);
⋮----
String topic = ackesRegistry.resolveTopic(commandType);
⋮----
CommandRecord commandRecord = new CommandRecord(
⋮----
commandType.typeName(),
commandType.version(),
runtime.serialize(command),
⋮----
command.getAggregateId(),
⋮----
Integer partition = ackesRegistry.resolvePartition(command.getAggregateId());
KafkaSender.send(producer, new ProducerRecord<>(topic, partition, commandRecord.id(), commandRecord));
⋮----
private void send(ProtocolRecord protocolRecord) {
⋮----
logger.trace("Sending AggregateStateRecord with id {} to {}", asr.aggregateId(), statePartition);
⋮----
Future<RecordMetadata> result = KafkaSender.send(producer, new ProducerRecord<>(statePartition.topic(), statePartition.partition(), asr.aggregateId(), asr));
⋮----
stateRepository.prepare(asr, result);
⋮----
logger.trace("Sending DomainEventRecord {}:{} with id {} to {}", der.name(), der.version(), der.id(), domainEventPartition);
KafkaSender.send(producer, new ProducerRecord<>(domainEventPartition.topic(), domainEventPartition.partition(), der.id(), der));
⋮----
logger.trace("Sending GDPRKeyRecord with id {} to {}", gkr.aggregateId(), gdprKeyPartition);
Future<RecordMetadata> result = KafkaSender.send(producer, new ProducerRecord<>(gdprKeyPartition.topic(), gdprKeyPartition.partition(), gkr.aggregateId(), gkr));
gdprContextRepository.prepare(gkr, result);
⋮----
throw new IllegalArgumentException("""
⋮----
private void index(DomainEventRecord der, IndexParams params) {
// send to the index topic
String topicName = getIndexTopicName(params.indexName(), params.indexKey());
if (params.createIndex()) {
if (consumer.partitionsFor(topicName).isEmpty()) {
if (indexTopicCreator.apply(params.indexName(), params.indexKey())) {
logger.info("Creating DomainEventIndex topic {}", topicName);
⋮----
logger.trace("Indexing DomainEventRecord {}:{} with id {} to topic {}", der.name(), der.version(), der.id(), topicName + "-0");
⋮----
KafkaSender.send(producer, new ProducerRecord<>(topicName, 0, der.id(), der));
⋮----
private void setupGDPRContext(String tenantId, String aggregateId, boolean createIfMissing) {
⋮----
if (!gdprContextRepository.exists(aggregateId) && createIfMissing) {
logger.trace("Generating GDPR key for aggregate {}", aggregateId);
⋮----
GDPRKeyRecord gdprKeyRecord = new GDPRKeyRecord(
⋮----
GDPRKeyUtils.createKey().getEncoded());
⋮----
send(gdprKeyRecord);
⋮----
GDPRContextHolder.setCurrentGDPRContext(gdprContextRepository.get(aggregateId));
⋮----
private void tearDownGDPRContext() {
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
private void handleCommand(CommandRecord commandRecord) {
⋮----
final List<DomainEventRecord> responseRecords = commandRecord.replyToTopicPartition() != null ? new ArrayList<>() : null;
⋮----
send(pr);
⋮----
responseRecords.add(der);
⋮----
if(runtime.requiresGDPRContext(commandRecord)) {
setupGDPRContext(commandRecord.tenantId(), commandRecord.aggregateId(), runtime.shouldGenerateGDPRKey(commandRecord));
⋮----
logger.trace("Handling CommandRecord with type {}", commandRecord.name());
runtime.handleCommandRecord(commandRecord, protocolRecordConsumer, this::index, () -> stateRepository.get(commandRecord.aggregateId()));
⋮----
CommandResponseRecord crr = new CommandResponseRecord(
commandRecord.tenantId(),
commandRecord.aggregateId(),
commandRecord.correlationId(),
commandRecord.id(),
⋮----
getCurrentGDPRContext() != null ? getCurrentGDPRContext().getEncryptionKey() : null);
TopicPartition replyToTopicPartition = PartitionUtils.parseReplyToTopicPartition(commandRecord.replyToTopicPartition());
logger.trace("Sending CommandResponseRecord with commandId {} to {}", crr.commandId(), replyToTopicPartition);
KafkaSender.send(producer, new ProducerRecord<>(replyToTopicPartition.topic(), replyToTopicPartition.partition(), crr.commandId(), crr));
⋮----
logger.error("Error handling command", e);
⋮----
tearDownGDPRContext();
⋮----
private void handleExternalEvent(DomainEventRecord eventRecord) {
⋮----
logger.trace("Handling DomainEventRecord with type {} as External Event", eventRecord.name());
⋮----
if(runtime.requiresGDPRContext(eventRecord)) {
setupGDPRContext(eventRecord.tenantId(), eventRecord.aggregateId(), runtime.shouldGenerateGDPRKey(eventRecord));
⋮----
runtime.handleExternalDomainEventRecord(eventRecord,
⋮----
() -> stateRepository.get(eventRecord.aggregateId()),
⋮----
logger.error("Error handling external event", e);
⋮----
private void process() {
⋮----
ConsumerRecords<String, ProtocolRecord> allRecords = consumer.poll(Duration.ofMillis(10));
if (!allRecords.isEmpty()) {
processRecords(allRecords);
⋮----
ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = consumer.poll(Duration.ofMillis(10));
gdprContextRepository.process(gdprKeyRecords.records(gdprKeyPartition));
⋮----
if (gdprKeyRecords.isEmpty() && initializedEndOffsets.getOrDefault(gdprKeyPartition, 0L) <= consumer.position(gdprKeyPartition)) {
⋮----
if (initializedEndOffsets.getOrDefault(statePartition, 0L) == 0L) {
⋮----
commitInitialOffsetsIfNecessary();
logger.info("No state found in Kafka for AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
consumer.resume(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());
⋮----
logger.info("Loading state for AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
consumer.resume(singletonList(statePartition));
⋮----
consumer.pause(singletonList(gdprKeyPartition));
⋮----
ConsumerRecords<String, ProtocolRecord> stateRecords = consumer.poll(Duration.ofMillis(10));
stateRepository.process(stateRecords.records(statePartition));
⋮----
if (stateRecords.isEmpty() && initializedEndOffsets.getOrDefault(statePartition, 0L) <= consumer.position(statePartition)) {
⋮----
consumer.resume(Stream.concat(
Stream.concat(Stream.of(commandPartition, domainEventPartition),runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty()),
externalEventPartitions.stream()).toList());
⋮----
logger.info(
⋮----
runtime.getName(),
runtime.shouldHandlePIIData() ? "Handle PII Data" : "Not Handle PII Data");
⋮----
long stateRepositoryOffset = stateRepository.getOffset();
⋮----
runtime.getName());
consumer.seek(statePartition, stateRepository.getOffset() + 1);
⋮----
consumer.seekToBeginning(singletonList(statePartition));
⋮----
if(runtime.shouldHandlePIIData()) {
⋮----
long gdprKeyRepositoryOffset = gdprContextRepository.getOffset();
⋮----
consumer.seek(gdprKeyPartition, gdprContextRepository.getOffset() + 1);
⋮----
consumer.seekToBeginning(singletonList(gdprKeyPartition));
⋮----
initializedEndOffsets = consumer.endOffsets(List.of(gdprKeyPartition, statePartition));
logger.info("Loading GDPR Keys for AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
consumer.pause(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());
⋮----
initializedEndOffsets = consumer.endOffsets(List.of(statePartition));
⋮----
logger.error("Fatal error during " + processState + " phase, shutting down AggregatePartition " + id + " of " + runtime.getName() + "Aggregate", e);
⋮----
private void commitInitialOffsetsIfNecessary() {
⋮----
String autoOffsetResetConfig = (String) Optional.ofNullable(consumerFactory.getConfigurationProperties()
.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG)).orElse("latest");
if ("latest".equals(autoOffsetResetConfig)) {
List<TopicPartition> topicPartitions = Stream.concat(Stream.of(commandPartition, domainEventPartition, statePartition), externalEventPartitions.stream()).toList();
final Map<TopicPartition, Long> beginningOffsets = consumer.beginningOffsets(topicPartitions);
final Map<TopicPartition, OffsetAndMetadata> committedOffsets = consumer.committed(new HashSet<>(topicPartitions));
⋮----
committedOffsets.forEach((topicPartition, offsetAndMetadata) -> {
⋮----
logger.info("TopicPartition[{}] has no committed offsets, will commit offset {} to avoid " +
"skipping records", topicPartition, beginningOffsets.getOrDefault(topicPartition, 0L));
uncommittedTopicPartitions.put(topicPartition, new OffsetAndMetadata(beginningOffsets.getOrDefault(topicPartition, 0L)));
⋮----
if (!uncommittedTopicPartitions.isEmpty()) {
producer.beginTransaction();
producer.sendOffsetsToTransaction(uncommittedTopicPartitions, consumer.groupMetadata());
producer.commitTransaction();
⋮----
private void processRecords(ConsumerRecords<String, ProtocolRecord> allRecords) {
⋮----
if (logger.isTraceEnabled()) {
logger.trace("Processing {} records in a single transaction", allRecords.count());
logger.trace("Processing {} gdpr key records", allRecords.records(gdprKeyPartition).size());
logger.trace("Processing {} command records", allRecords.records(commandPartition).size());
if (!externalEventPartitions.isEmpty()) {
logger.trace("Processing {} external event records", externalEventPartitions.stream()
.map(externalEventPartition -> allRecords.records(externalEventPartition).size())
.mapToInt(Integer::intValue).sum());
⋮----
logger.trace("Processing {} state records", allRecords.records(statePartition).size());
logger.trace("Processing {} internal event records", allRecords.records(domainEventPartition).size());
⋮----
List<ConsumerRecord<String, ProtocolRecord>> gdprKeyRecords = allRecords.records(gdprKeyPartition);
if (!gdprKeyRecords.isEmpty()) {
gdprContextRepository.process(gdprKeyRecords);
offsets.put(gdprKeyPartition, gdprKeyRecords.getLast().offset());
⋮----
.forEach(externalEventPartition -> allRecords.records(externalEventPartition)
.forEach(eventRecord -> {
handleExternalEvent((DomainEventRecord) eventRecord.value());
offsets.put(externalEventPartition, eventRecord.offset());
⋮----
allRecords.records(commandPartition)
.forEach(commandRecord -> {
handleCommand((CommandRecord) commandRecord.value());
offsets.put(commandPartition, commandRecord.offset());
⋮----
List<ConsumerRecord<String, ProtocolRecord>> stateRecords = allRecords.records(statePartition);
if (!stateRecords.isEmpty()) {
stateRepository.process(stateRecords);
offsets.put(statePartition, stateRecords.getLast().offset());
⋮----
allRecords.records(domainEventPartition)
.forEach(domainEventRecord -> offsets.put(domainEventPartition, domainEventRecord.offset()));
⋮----
producer.sendOffsetsToTransaction(offsets.entrySet().stream()
.collect(Collectors.toMap(Map.Entry::getKey, e -> new OffsetAndMetadata(e.getValue() + 1))),
consumer.groupMetadata());
⋮----
stateRepository.commit();
⋮----
gdprContextRepository.commit();
⋮----
producer.abortTransaction();
rollbackConsumer(allRecords);
stateRepository.rollback();
gdprContextRepository.rollback();
⋮----
private void rollbackConsumer(ConsumerRecords<String, ProtocolRecord> consumerRecords) {
consumerRecords.partitions().forEach(topicPartition -> {
⋮----
consumerRecords.records(topicPartition).stream().map(ConsumerRecord::offset).min(Long::compareTo)
.ifPresent(offset -> consumer.seek(topicPartition, offset));
⋮----
public boolean isProcessing() {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/WalletTests.java
================
public class WalletTests {
⋮----
public void testFindBeans() {
assertEquals(4, applicationContext.getBeansOfType(CommandHandlerFunction.class).size());
assertEquals(1, applicationContext.getBeansOfType(EventHandlerFunction.class).size());
assertEquals(4, applicationContext.getBeansOfType(EventSourcingHandlerFunction.class).size());
assertEquals(1, applicationContext.getBeansOfType(UpcastingHandlerFunction.class).size());
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_create_CreateWallet_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_credit_CreditWallet_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_makeReservation_ReserveAmount_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_createBalance_CreateBalance_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_eh_create_AccountCreated_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_create_WalletCreated_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_createBalance_BalanceCreated_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_credit_WalletCredited_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_suh_upcast_Wallet_1_to_2"));
⋮----
public void testValidateDomainEventsWithMissingExternalDomainEventSchema() throws Exception {
AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);
assertThrows(SchemaNotFoundException.class, () -> {
for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
walletAggregate.registerAndValidate(domainEventType);
⋮----
System.out.println(schemaRegistryClient.getAllSubjects());
⋮----
public void testValidateDomainEvents() throws Exception {
⋮----
schemaRegistryClient.register("domainevents.AccountCreated",
schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, AccountCreatedEvent.class, true, true, false, true)),
⋮----
public void testValidateDomainEventsWithExistingSchemas() throws Exception {
⋮----
schemaRegistryClient.register("domainevents.WalletCreated",
schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCreated", 1, WalletCreatedEvent.class, true, false, false, false)),
⋮----
schemaRegistryClient.register("domainevents.WalletCredited",
schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCredited", 1, WalletCreditedEvent.class, false, false, false, false)),
⋮----
public void testValidateDomainEventsWithExistingSchemasAndExternalEventSubset() throws Exception {
⋮----
schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false, false)),
⋮----
public void testValidateDomainEventsWithExistingSchemasAndInvalidExternalEvent() throws Exception {
⋮----
schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, false, false, false)),
⋮----
Assertions.assertThrows(IncompatibleSchemaException.class, () ->
walletAggregate.registerAndValidate(new DomainEventType<>("AccountCreated", 1, InvalidAccountCreatedEvent.class, true, true, false, false)));
⋮----
public void testRegisterAndValidateMultipleVersionsOfEvent() throws Exception {
⋮----
walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 2, AccountCreatedEventV2.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false, false));
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas("domainevents.TestAccountCreated", false, false);
assertEquals(3, registeredSchemas.size());
⋮----
public void testRegisterAndValidateMultipleVersionsOfEventWithSkippedVersion() throws Exception {
⋮----
walletAggregate.registerAndValidate(new DomainEventType<>("AnotherTestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false, false));
Assertions.assertThrows(InvalidSchemaVersionException.class, () ->
walletAggregate.registerAndValidate(new DomainEventType<>("AnotherTestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false, false)));
⋮----
public void testRegisterAndValidateMultipleVersionsOfEventWithNonCompatibleEvent() throws Exception {
⋮----
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 2, AccountCreatedEventV2.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false, false));
SchemaNotBackwardsCompatibleException exception = Assertions.assertThrows(SchemaNotBackwardsCompatibleException.class, () -> {
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 4, NotCompatibleAccountCreatedEventV4.class, true, false, false, false));
⋮----
assertEquals("Schema not backwards compatible with previous version: 3", exception.getMessage());
⋮----
public void testCreateWalletByCommand() throws Exception {
⋮----
walletAggregate.handleCommandRecord(
new CommandRecord(
⋮----
objectMapper.writeValueAsBytes(
new CreateWalletCommand(aggregateId, "EUR")),
⋮----
(eventRecord, index) -> indexedEvents.add(eventRecord),
⋮----
assertEquals(4, producedRecords.size());
AggregateStateRecord actualRecord = (AggregateStateRecord) producedRecords.getFirst();
AggregateStateRecord expectedRecord = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(new WalletStateV2(aggregateId, new ArrayList<>())),
⋮----
assertEquals(expectedRecord.generation(), actualRecord.generation());
assertEquals(expectedRecord.aggregateId(), actualRecord.aggregateId());
assertEquals(expectedRecord.correlationId(), actualRecord.correlationId());
assertArrayEquals(expectedRecord.payload(), actualRecord.payload());
assertEquals(expectedRecord.encoding(), actualRecord.encoding());
assertEquals(expectedRecord.name(), actualRecord.name());
assertEquals(expectedRecord.version(), actualRecord.version());
⋮----
DomainEventRecord actual = (DomainEventRecord) producedRecords.get(1);
⋮----
assertEquals(1, actual.generation());
assertEquals(aggregateId, actual.aggregateId());
assertEquals(correlationId, actual.correlationId());
assertArrayEquals(objectMapper.writeValueAsBytes(new WalletCreatedEvent(aggregateId)), actual.payload());
assertEquals(PayloadEncoding.JSON, actual.encoding());
assertEquals("WalletCreated", actual.name());
assertEquals(1, actual.version());
⋮----
actualRecord = (AggregateStateRecord) producedRecords.get(2);
expectedRecord = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(new WalletStateV2(aggregateId, List.of(new WalletStateV2.Balance("EUR", BigDecimal.ZERO)))),
⋮----
actual = (DomainEventRecord) producedRecords.get(3);
⋮----
assertEquals(2, actual.generation());
⋮----
assertArrayEquals(objectMapper.writeValueAsBytes(new BalanceCreatedEvent(aggregateId, "EUR")), actual.payload());
⋮----
assertEquals("BalanceCreated", actual.name());
⋮----
public void testIndexWalletEventsFromCommand() throws Exception {
⋮----
assertEquals(2, indexedEvents.size());
DomainEventRecord actual = indexedEvents.getFirst();
⋮----
actual = indexedEvents.get(1);
⋮----
public void testCreateWalletByExternalDomainEvent() throws Exception {
⋮----
walletAggregate.handleExternalDomainEventRecord(
new DomainEventRecord(
⋮----
new AccountCreatedEvent(aggregateId, "NL", "7hdU_mfA_bvkRRgCekTZ0A==", "ioxbJd-hSLj6KNJpdYzN4g==", "6KLIDo3Ii2d-oVZtiv1h3OYNgW5lXYAnCnxPK2fprUU=")),
⋮----
public void testIndexWalletEventsByExternalDomainEvent() throws Exception {
⋮----
public void testWalletCreatedWithWalletStateV1andUpdatedWithWalletStateV2() throws Exception {
⋮----
schemaRegistryClient.register("domainevents.BalanceCreated",
schemaRegistry.generateJsonSchema(new DomainEventType<>("BalanceCreated", 1, BalanceCreatedEvent.class, false, false, false, false)),
⋮----
AggregateStateRecord v1StateRecord = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(new WalletState(aggregateId, List.of(new WalletState.Balance("EUR", BigDecimal.ZERO)))),
⋮----
new CreateBalanceCommand(aggregateId, "ETH")),
⋮----
assertEquals(2, producedRecords.size());
⋮----
AggregateStateRecord expectedStateRecord = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(new WalletStateV2(aggregateId, List.of(
⋮----
AggregateStateRecord actualStateRecord = (AggregateStateRecord) producedRecords.getFirst();
assertEquals(expectedStateRecord.generation(), actualStateRecord.generation());
assertEquals(expectedStateRecord.aggregateId(), actualStateRecord.aggregateId());
assertEquals(expectedStateRecord.correlationId(), actualStateRecord.correlationId());
assertEquals(expectedStateRecord.encoding(), actualStateRecord.encoding());
assertEquals(expectedStateRecord.version(), actualStateRecord.version());
assertEquals(expectedStateRecord.tenantId(), actualStateRecord.tenantId());
assertEquals(expectedStateRecord.name(), actualStateRecord.name());
assertArrayEquals(expectedStateRecord.payload(), actualStateRecord.payload());

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/AkcesClientTests.java
================
public class AkcesClientTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withEnv("SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL","none")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static <C extends Command> void prepareCommandSchemas(String url, List<Class<C>> commandClasses) {
SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
CommandInfo info = commandClass.getAnnotation(CommandInfo.class);
src.register("commands." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(commandClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<D>> domainEventClasses) {
⋮----
DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
⋮----
public static void prepareExternalServices(String bootstrapServers) {
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(new ObjectMapper());
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
⋮----
List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
List.of());
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", aggregateServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
public void testUnroutableCommandWithSendAndForget() {
Assertions.assertNotNull(akcesClient);
⋮----
while (!akcesClient.isRunning()) {
Thread.onSpinWait();
⋮----
Assertions.assertThrows(UnroutableCommandException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new UnroutableCommand(UUID.randomUUID().toString())));
⋮----
public void testUnroutableCommandWithSend() {
⋮----
CompletionStage<List<DomainEvent>> result =  akcesClient.send("TEST_TENANT", new UnroutableCommand(UUID.randomUUID().toString()));
⋮----
ExecutionException executionException = Assertions.assertThrows(ExecutionException.class, () -> result.toCompletableFuture().get());
Assertions.assertInstanceOf(UnroutableCommandException.class, executionException.getCause());
⋮----
public void testInvalidCommandWithSendAndForget() {
⋮----
Assertions.assertThrows(IllegalArgumentException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new InvalidCommand(UUID.randomUUID().toString())));
⋮----
public void testInvalidCommandWithSend() {
⋮----
Assertions.assertThrows(IllegalArgumentException.class, () -> akcesClient.send("TEST_TENANT", new InvalidCommand(UUID.randomUUID().toString())));
⋮----
public void testValidationErrorWithSendAndForget() {
⋮----
Assertions.assertThrows(CommandValidationException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new CreateAccountCommand(UUID.randomUUID().toString(), "NL", "Aike", "Christianen", null)));
⋮----
public void testValidationErrorWithSend() {
⋮----
CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT", new CreateAccountCommand(UUID.randomUUID().toString(), "NL", "Aike", "Christianen", null));
⋮----
Assertions.assertInstanceOf(CommandValidationException.class, executionException.getCause());
⋮----
public void testSendCommand() throws InterruptedException, JsonProcessingException {
⋮----
CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT",
new CreateAccountCommand(userId,
⋮----
Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
⋮----
TopicPartition commandResponsesPartition = akcesClient.getCommandResponsePartition();
TopicPartition commandPartition = new TopicPartition("Account-Commands", akcesClient.resolvePartition(userId));
testConsumer.assign(List.of(commandPartition));
testConsumer.seekToBeginning(List.of(commandPartition));
ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
⋮----
while (allRecords.isEmpty()) {
records.forEach(record -> allRecords.add(record.value()));
⋮----
records = testConsumer.poll(Duration.ofMillis(250));
⋮----
Assertions.assertEquals(1, allRecords.size());
CommandRecord cr = (CommandRecord) allRecords.getFirst();
⋮----
testProducer.beginTransaction();
List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", "Aike", "Christianen", "aike.christianen@gmail.com")), PayloadEncoding.JSON, userId, null, 0));
CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, null);
⋮----
testProducer.send(new ProducerRecord<>(commandResponsesPartition.topic(), commandResponsesPartition.partition(), crr.commandId(), crr));
testProducer.commitTransaction();
⋮----
CountDownLatch waitLatch = new CountDownLatch(1);
result.whenComplete((s, throwable) -> waitLatch.countDown());
Assertions.assertTrue(waitLatch.await(10, TimeUnit.SECONDS));
Assertions.assertTrue(result.toCompletableFuture().isDone());
Assertions.assertFalse(result.toCompletableFuture().isCompletedExceptionally());
Assertions.assertNotNull(result.toCompletableFuture().getNow(null));
Assertions.assertEquals(1, result.toCompletableFuture().getNow(null).size());
AccountCreatedEvent ace = (AccountCreatedEvent) result.toCompletableFuture().getNow(null).getFirst();
Assertions.assertEquals(userId, ace.userId());
Assertions.assertEquals("NL", ace.country());
Assertions.assertEquals("Aike", ace.firstName());
Assertions.assertEquals("Christianen", ace.lastName());
Assertions.assertEquals("aike.christianen@gmail.com", ace.email());
⋮----
public void testSendCommandWithCorrelationId() throws InterruptedException, JsonProcessingException {
⋮----
String correlationId = UUID.randomUUID().toString();
⋮----
new CreateAccountCommand(
⋮----
Assertions.assertEquals(correlationId, cr.correlationId());
⋮----
public void testGDPRDecryption() throws InterruptedException, JsonProcessingException {
⋮----
SecretKeySpec secretKeySpec = GDPRKeyUtils.createKey();
EncryptingGDPRContext gdprContext = new EncryptingGDPRContext(userId, secretKeySpec.getEncoded(), GDPRKeyUtils.isUUID(userId));
String encryptedFirstName = gdprContext.encrypt("Aike");
String encryptedLastName = gdprContext.encrypt("Christianen");
String encryptedEmail = gdprContext.encrypt("aike.christianen@gmail.com");
⋮----
List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", encryptedFirstName, encryptedLastName, encryptedEmail)), PayloadEncoding.JSON, userId, null, 0));
CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, secretKeySpec.getEncoded());
⋮----
public static class ContextInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
prepareCommandSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), List.of(CreateAccountCommand.class));
prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), List.of(AccountCreatedEvent.class));
prepareExternalServices(kafka.getBootstrapServers());
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/AggregateBeanFactoryPostProcessor.java
================
public class AggregateBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
private static final Logger logger = LoggerFactory.getLogger(AggregateBeanFactoryPostProcessor.class);
public static final List<DomainEventType<? extends DomainEvent>> COMMAND_HANDLER_CREATE_SYSTEM_ERRORS = List.of(
⋮----
public static final List<DomainEventType<? extends DomainEvent>> COMMAND_HANDLER_SYSTEM_ERRORS = List.of(
⋮----
public static final List<DomainEventType<? extends DomainEvent>> EVENT_HANDLER_CREATE_SYSTEM_ERRORS = List.of(
⋮----
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
⋮----
logger.info("Processing Aggregate beans");
Arrays.asList(beanFactory.getBeanNamesForAnnotation(AggregateInfo.class)).forEach(beanName -> {
logger.info("Processing Aggregate bean {}", beanName);
BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
⋮----
final Class<? extends Aggregate<?>> aggregateClass = (Class<? extends Aggregate<?>>) Class.forName(bd.getBeanClassName());
AggregateInfo aggregateInfo = aggregateClass.getAnnotation(AggregateInfo.class);
AggregateValidator validator = new AggregateValidator(aggregateClass, aggregateInfo.stateClass());
List<Method> commandHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(CommandHandler.class))
.toList();
List<Method> eventHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(EventHandler.class))
⋮----
List<Method> eventSourcingHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(EventSourcingHandler.class))
⋮----
List<Method> eventBridgeHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(EventBridgeHandler.class))
⋮----
List<Method> upcastingHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(UpcastingHandler.class))
⋮----
commandHandlers.forEach(commandHandlerMethod ->
processCommandHandler(beanName,
aggregateInfo.stateClass(),
⋮----
eventHandlers.forEach(eventHandlerMethod ->
processEventHandler(beanName,
⋮----
eventSourcingHandlers.forEach(eventSourcingHandlerMethod ->
processEventSourcingHandler(beanName,
⋮----
eventBridgeHandlers.forEach(eventBridgeHandlerMethod ->
processEventBridgeHandler(beanName,
⋮----
upcastingHandlers.forEach(upcastingHandlerMethod ->
processUpcastingHandler(beanName, aggregateClass, upcastingHandlerMethod, bdr, validator));
⋮----
validator.validate();
⋮----
throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
⋮----
throw new ApplicationContextException("Invalid Aggregate configuration for bean " + beanName, e);
⋮----
bdr.registerBeanDefinition(beanName + "AggregateRuntimeFactory",
BeanDefinitionBuilder.genericBeanDefinition(AggregateRuntimeFactory.class)
.addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
.addConstructorArgReference("aggregateServiceSchemaRegistry")
.addConstructorArgReference(beanName)
.getBeanDefinition());
⋮----
if (beanFactory.containsBeanDefinition("aggregateServiceConsumerFactory") &&
beanFactory.containsBeanDefinition("aggregateServiceProducerFactory") &&
beanFactory.containsBeanDefinition("aggregateServiceControlProducerFactory") &&
beanFactory.containsBeanDefinition("aggregateServiceAggregateStateRepositoryFactory")) {
bdr.registerBeanDefinition(beanName + "AkcesController",
BeanDefinitionBuilder.genericBeanDefinition(AkcesAggregateController.class)
.addConstructorArgReference("aggregateServiceConsumerFactory")
.addConstructorArgReference("aggregateServiceProducerFactory")
.addConstructorArgReference("aggregateServiceControlConsumerFactory")
.addConstructorArgReference("aggregateServiceControlProducerFactory")
.addConstructorArgReference("aggregateServiceAggregateStateRepositoryFactory")
.addConstructorArgReference("aggregateServiceGDPRContextRepositoryFactory")
.addConstructorArgReference(beanName + "AggregateRuntimeFactory")
⋮----
.addConstructorArgReference("aggregateServiceKafkaAdmin")
.setInitMethodName("start")
.setDestroyMethodName("close")
⋮----
throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
⋮----
private void processEventSourcingHandler(String aggregateBeanName,
⋮----
EventSourcingHandler eventSourcingHandler = eventSourcingHandlerMethod.getAnnotation(EventSourcingHandler.class);
if (eventSourcingHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(eventSourcingHandlerMethod.getParameterTypes()[0]) &&
stateClass.equals(eventSourcingHandlerMethod.getParameterTypes()[1]) &&
stateClass.equals(eventSourcingHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = eventSourcingHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_esh_" + eventSourcingHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
Class<? extends DomainEvent> domainEventClass = (Class<? extends DomainEvent>) eventSourcingHandlerMethod.getParameterTypes()[0];
⋮----
eventInfo.type(),
eventInfo.version(),
⋮----
eventSourcingHandler.create(),
⋮----
hasPIIDataAnnotation(domainEventClass));
bdr.registerBeanDefinition(beanName,
BeanDefinitionBuilder.genericBeanDefinition(EventSourcingHandlerFunctionAdapter.class)
.addConstructorArgReference(aggregateBeanName)
.addConstructorArgValue(eventSourcingHandlerMethod.getName())
.addConstructorArgValue(domainEventType)
.addConstructorArgValue(stateClass)
.setInitMethodName("init")
⋮----
validator.detectEventSourcingHandler(domainEventType);
⋮----
throw new ApplicationContextException("Invalid EventSourcingHandler method signature: " + eventSourcingHandlerMethod);
⋮----
private void processEventHandler(String aggregateBeanName,
⋮----
EventHandler eventHandler = eventHandlerMethod.getAnnotation(EventHandler.class);
if (eventHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(eventHandlerMethod.getParameterTypes()[0]) &&
stateClass.equals(eventHandlerMethod.getParameterTypes()[1]) &&
Stream.class.isAssignableFrom(eventHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = eventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_eh_" + eventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
Class<? extends DomainEvent> inputEventClass = (Class<? extends DomainEvent>) eventHandlerMethod.getParameterTypes()[0];
⋮----
eventHandler.create(),
⋮----
ErrorEvent.class.isAssignableFrom(inputEventClass),
hasPIIDataAnnotation(inputEventClass));
List<DomainEventType<?>> producedDomainEventTypes = generateDomainEventTypes(eventHandler.produces(), eventHandler.create());
List<DomainEventType<?>> errorEventTypes = generateEventHandlerErrorEventTypes(eventHandler.errors(), eventHandler.create());
⋮----
BeanDefinitionBuilder.genericBeanDefinition(EventHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(eventHandlerMethod.getName())
.addConstructorArgValue(inputEventType)
⋮----
.addConstructorArgValue(producedDomainEventTypes)
.addConstructorArgValue(errorEventTypes)
⋮----
validator.detectEventHandler(inputEventType, producedDomainEventTypes, errorEventTypes);
⋮----
throw new ApplicationContextException("Invalid EventHandler method signature: " + eventHandlerMethod);
⋮----
private void processCommandHandler(String aggregateBeanName,
⋮----
CommandHandler commandHandler = commandHandlerMethod.getAnnotation(CommandHandler.class);
if (commandHandlerMethod.getParameterCount() == 2 &&
Command.class.isAssignableFrom(commandHandlerMethod.getParameterTypes()[0]) &&
stateClass.equals(commandHandlerMethod.getParameterTypes()[1]) &&
Stream.class.isAssignableFrom(commandHandlerMethod.getReturnType())) {
CommandInfo commandInfo = commandHandlerMethod.getParameterTypes()[0].getAnnotation(CommandInfo.class);
⋮----
String beanName = aggregateBeanName + "_ch_" + commandHandlerMethod.getName() + "_" + commandInfo.type() + "_" + commandInfo.version();
Class<? extends Command> commandClass = (Class<? extends Command>) commandHandlerMethod.getParameterTypes()[0];
⋮----
commandInfo.type(),
commandInfo.version(),
⋮----
commandHandler.create(),
⋮----
hasPIIDataAnnotation(commandClass));
List<DomainEventType<?>> producedDomainEventTypes = generateDomainEventTypes(commandHandler.produces(), commandHandler.create());
List<DomainEventType<?>> errorEventTypes = generateCommandHandlerErrorEventTypes(commandHandler.errors(), commandHandler.create());
⋮----
BeanDefinitionBuilder.genericBeanDefinition(CommandHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(commandHandlerMethod.getName())
.addConstructorArgValue(commandType)
⋮----
.setInitMethodName("init").getBeanDefinition()
⋮----
validator.detectCommandHandler(commandType, producedDomainEventTypes, errorEventTypes);
⋮----
throw new ApplicationContextException("Invalid CommandHandler method signature: " + commandHandlerMethod);
⋮----
private void processEventBridgeHandler(String aggregateBeanName,
⋮----
if (eventBridgeHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(eventBridgeHandlerMethod.getParameterTypes()[0]) &&
eventBridgeHandlerMethod.getParameterTypes()[1].equals(CommandBus.class) &&
void.class.equals(eventBridgeHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = eventBridgeHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_ebh_" + eventBridgeHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
Class<? extends DomainEvent> domainEventClass = (Class<? extends DomainEvent>) eventBridgeHandlerMethod.getParameterTypes()[0];
⋮----
ErrorEvent.class.isAssignableFrom(domainEventClass),
⋮----
BeanDefinitionBuilder.genericBeanDefinition(EventBridgeHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(eventBridgeHandlerMethod.getName())
⋮----
validator.detectEventBridgeHandler(inputEventType);
⋮----
throw new ApplicationContextException("Invalid EventBridgeHandler method signature: " + eventBridgeHandlerMethod);
⋮----
private void processUpcastingHandler(String aggregateBeanName,
⋮----
if (upcastingHandlerMethod.getParameterCount() == 1 &&
DomainEvent.class.isAssignableFrom(upcastingHandlerMethod.getParameterTypes()[0]) &&
DomainEvent.class.isAssignableFrom(upcastingHandlerMethod.getReturnType())) {
⋮----
Class<? extends DomainEvent> inputEventClass = (Class<? extends DomainEvent>) upcastingHandlerMethod.getParameterTypes()[0];
Class<? extends DomainEvent> outputEventClass = (Class<? extends DomainEvent>) upcastingHandlerMethod.getReturnType();
⋮----
DomainEventInfo inputEventInfo = inputEventClass.getAnnotation(DomainEventInfo.class);
⋮----
throw new IllegalArgumentException("Input event class " + inputEventClass.getName() +
⋮----
DomainEventInfo outputEventInfo = outputEventClass.getAnnotation(DomainEventInfo.class);
⋮----
throw new IllegalArgumentException("Output event class " + outputEventClass.getName() +
⋮----
if (!inputEventInfo.type().equals(outputEventInfo.type())) {
throw new IllegalArgumentException("Input event type " + inputEventInfo.type() +
" does not match output event type " + outputEventInfo.type());
⋮----
if(outputEventInfo.version() - inputEventInfo.version() != 1) {
throw new IllegalArgumentException("Output event version " + outputEventInfo.version() +
" must be one greater than input event version " + inputEventInfo.version());
⋮----
String beanName = aggregateBeanName + "_duh_" + upcastingHandlerMethod.getName() +
"_" + inputEventInfo.type() + "_" + inputEventInfo.version() +
"_to_" + outputEventInfo.version();
⋮----
boolean externalEvent = Arrays.stream(aggregateClass.getMethods())
⋮----
.map(method -> method.getParameterTypes()[0])
.noneMatch(eventClass -> eventClass.getAnnotation(DomainEventInfo.class).type().equals(outputEventInfo.type()));
⋮----
inputEventInfo.type(),
inputEventInfo.version(),
⋮----
outputEventInfo.type(),
outputEventInfo.version(),
⋮----
ErrorEvent.class.isAssignableFrom(outputEventClass),
hasPIIDataAnnotation(outputEventClass));
⋮----
BeanDefinitionBuilder.genericBeanDefinition(DomainEventUpcastingHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(upcastingHandlerMethod.getName())
⋮----
.addConstructorArgValue(outputEventType)
⋮----
validator.detectUpcastingHandler(inputEventType, outputEventType);
⋮----
} else if (upcastingHandlerMethod.getParameterCount() == 1 &&
AggregateState.class.isAssignableFrom(upcastingHandlerMethod.getParameterTypes()[0]) &&
AggregateState.class.isAssignableFrom(upcastingHandlerMethod.getReturnType())) {
⋮----
Class<? extends AggregateState> inputStateClass = (Class<? extends AggregateState>) upcastingHandlerMethod.getParameterTypes()[0];
Class<? extends AggregateState> outputStateClass = (Class<? extends AggregateState>) upcastingHandlerMethod.getReturnType();
⋮----
AggregateStateInfo inputStateInfo = inputStateClass.getAnnotation(AggregateStateInfo.class);
⋮----
throw new IllegalArgumentException("Input state class " + inputStateClass.getName() +
⋮----
AggregateStateInfo outputStateInfo = outputStateClass.getAnnotation(AggregateStateInfo.class);
⋮----
throw new IllegalArgumentException("Output state class " + outputStateClass.getName() +
⋮----
throw new IllegalArgumentException("Aggregate class " + aggregateClass.getName() +
⋮----
if (!inputStateInfo.type().equals(outputStateInfo.type())) {
throw new IllegalArgumentException("Input state type " + inputStateInfo.type() +
" does not match output state type " + outputStateInfo.type());
⋮----
if(outputStateInfo.version() - inputStateInfo.version() != 1) {
throw new IllegalArgumentException("Output state version " + outputStateInfo.version() +
" must be one greater than input state version " + inputStateInfo.version());
⋮----
String beanName = aggregateBeanName + "_suh_" + upcastingHandlerMethod.getName() +
"_" + inputStateInfo.type() + "_" + inputStateInfo.version() +
"_to_" + outputStateInfo.version();
⋮----
inputStateInfo.type(),
inputStateInfo.version(),
⋮----
aggregateInfo.generateGDPRKeyOnCreate(),
aggregateInfo.indexed(),
aggregateInfo.indexName(),
hasPIIDataAnnotation(inputStateClass)
⋮----
outputStateInfo.type(),
outputStateInfo.version(),
⋮----
hasPIIDataAnnotation(outputStateClass)
⋮----
BeanDefinitionBuilder.genericBeanDefinition(AggregateStateUpcastingHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(inputStateType)
.addConstructorArgValue(outputStateType)
⋮----
validator.detectUpcastingHandler(inputStateType, outputStateType);
⋮----
throw new ApplicationContextException("Invalid UpcastingHandler method signature: " + upcastingHandlerMethod);
⋮----
private List<DomainEventType<?>> generateDomainEventTypes(Class<? extends DomainEvent>[] domainEventClasses,
⋮----
return Arrays.stream(domainEventClasses).map(eventClass -> {
DomainEventInfo eventInfo = eventClass.getAnnotation(DomainEventInfo.class);
return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, isCreate, false, false, hasPIIDataAnnotation(eventClass));
}).collect(Collectors.toList());
⋮----
private List<DomainEventType<?>> generateEventHandlerErrorEventTypes(Class<? extends DomainEvent>[] domainEventClasses, boolean isCreate) {
Stream<DomainEventType<? extends DomainEvent>> systemErrorEvents = (isCreate) ? EVENT_HANDLER_CREATE_SYSTEM_ERRORS.stream() : Stream.empty();
return Stream.concat(Arrays.stream(domainEventClasses).map(eventClass -> {
⋮----
return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, false, false, true, hasPIIDataAnnotation(eventClass));
}), systemErrorEvents).collect(Collectors.toList());
⋮----
private List<DomainEventType<?>> generateCommandHandlerErrorEventTypes(Class<? extends DomainEvent>[] domainEventClasses, boolean isCreate) {
Stream<DomainEventType<? extends DomainEvent>> systemErrorEvents = (isCreate) ? COMMAND_HANDLER_CREATE_SYSTEM_ERRORS.stream() : COMMAND_HANDLER_SYSTEM_ERRORS.stream();
⋮----
public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
logger.info("Processing Aggregate beans for AOT");
⋮----
public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/EventBridgeHandlerFunctionAdapter.java
================
public class EventBridgeHandlerFunctionAdapter<S extends AggregateState, E extends DomainEvent> implements EventBridgeHandlerFunction<S, E> {
⋮----
public void init() {
⋮----
methodHandle = MethodHandles.lookup().findVirtual(
aggregate.getClass(),
⋮----
MethodType.methodType(void.class, domainEventType.typeClass(), CommandBus.class));
⋮----
throw new RuntimeException(e);
⋮----
public void apply(@NotNull E event, CommandBus commandBus) {
⋮----
methodHandle.invoke(aggregate, event, commandBus);
⋮----
public DomainEventType<E> getEventType() {
⋮----
public Aggregate<S> getAggregate() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregateRuntimeFactory.java
================
public class AggregateRuntimeFactory<S extends AggregateState> implements FactoryBean<AggregateRuntime>, ApplicationContextAware {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
public AggregateRuntime getObject(){
return createRuntime(aggregate);
⋮----
public Class<?> getObjectType() {
⋮----
private KafkaAggregateRuntime createRuntime(Aggregate<S> aggregate) {
⋮----
AggregateInfo aggregateInfo = aggregate.getClass().getAnnotation(AggregateInfo.class);
⋮----
throw new IllegalStateException("Class implementing Aggregate must be annotated with @AggregateInfo");
⋮----
AggregateStateInfo aggregateStateInfo = aggregateInfo.stateClass().getAnnotation(AggregateStateInfo.class);
⋮----
throw new IllegalStateException("Aggregate state class " + aggregateInfo.stateClass().getName() +
⋮----
runtimeBuilder.setStateType(new AggregateStateType<>(
aggregateInfo.value(),
aggregateStateInfo.version(),
aggregateInfo.stateClass(),
aggregateInfo.generateGDPRKeyOnCreate(),
aggregateInfo.indexed(),
aggregateInfo.indexName(),
hasPIIDataAnnotation(aggregateInfo.stateClass())))
.setAggregateClass((Class<? extends Aggregate<?>>) aggregate.getClass())
.setObjectMapper(objectMapper)
.setGenerateGDPRKeyOnCreate(aggregateInfo.generateGDPRKeyOnCreate());
⋮----
applicationContext.getBeansOfType(CommandHandlerFunction.class).values().stream()
⋮----
.filter(adapter -> adapter.getAggregate().equals(aggregate))
.forEach(adapter -> {
CommandType<?> type = adapter.getCommandType();
if (adapter.isCreate()) {
⋮----
.setCommandCreateHandler(adapter)
.addCommand(type);
⋮----
.addCommandHandler(type, adapter)
⋮----
for (Object producedDomainEventType : adapter.getProducedDomainEventTypes()) {
runtimeBuilder.addDomainEvent((DomainEventType<?>) producedDomainEventType);
⋮----
for (Object errorEventType : adapter.getErrorEventTypes()) {
runtimeBuilder.addDomainEvent((DomainEventType<?>) errorEventType);
⋮----
applicationContext.getBeansOfType(EventHandlerFunction.class).values().stream()
⋮----
DomainEventType<?> type = adapter.getEventType();
⋮----
.setEventCreateHandler(adapter)
.addDomainEvent(type);
⋮----
.addExternalEventHandler(type, adapter)
⋮----
applicationContext.getBeansOfType(EventSourcingHandlerFunction.class).values().stream()
⋮----
.setEventSourcingCreateHandler(adapter)
⋮----
.addEventSourcingHandler(type, adapter)
⋮----
applicationContext.getBeansOfType(EventBridgeHandlerFunction.class).values().stream()
⋮----
.addEventBridgeHandler(type, adapter)
⋮----
applicationContext.getBeansOfType(UpcastingHandlerFunction.class).values().stream()
⋮----
if(adapter.getInputType() instanceof AggregateStateType<?> stateType) {
⋮----
.addStateUpcastingHandler(stateType, adapter);
} else if(adapter.getInputType() instanceof DomainEventType<?> eventType) {
⋮----
.addEventUpcastingHandler(eventType, adapter)
.addDomainEvent(eventType);
⋮----
return runtimeBuilder.setSchemaRegistry(schemaRegistry).validateAndBuild();

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/QueryModelRuntimeTests.java
================
public class QueryModelRuntimeTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withEnv("SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL","none")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<?>> domainEventClasses) {
SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static void cleanUp() throws IOException {
⋮----
if (Files.exists(Paths.get("/tmp/akces"))) {
⋮----
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
void testContextLoads() {
assertNotNull(adminClient);
assertNotNull(schemaRegistryClient);
assertNotNull(consumerFactory);
assertNotNull(objectMapper);
assertNotNull(walletController);
assertNotNull(accountController);
assertNotNull(orderProcessManagerController);
assertNotNull(akcesClientController);
assertNotNull(akcesQueryModelController);
⋮----
while (!walletController.isRunning() ||
!accountController.isRunning() ||
!orderProcessManagerController.isRunning() ||
!akcesClientController.isRunning() ||
!akcesQueryModelController.isRunning()) {
Thread.onSpinWait();
⋮----
public void testFindBeans() {
⋮----
assertNotNull(applicationContext);
assertEquals(4, applicationContext.getBeansOfType(QueryModelEventHandlerFunction.class).size());
assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_create_WalletCreated_1"));
assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_createBalance_BalanceCreated_1"));
assertNotNull(applicationContext.getBean("AccountQueryModel_qmeh_create_AccountCreated_1"));
assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_creditWallet_WalletCredited_1"));
assertEquals(2, applicationContext.getBeansOfType(QueryModelRuntimeFactory.class).size());
assertNotNull(applicationContext.getBean("WalletQueryModelQueryModelRuntime"));
assertNotNull(applicationContext.getBean("AccountQueryModelQueryModelRuntime"));
assertEquals(2, applicationContext.getBeansOfType(QueryModelRuntime.class).size());
⋮----
public void testWithUnknownId() throws InterruptedException, TimeoutException {
⋮----
CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(
WalletQueryModel.class, "unknown-id").toCompletableFuture();
assertNotNull(stateFuture);
ExecutionException exception = assertThrows(ExecutionException.class, stateFuture::get);
assertInstanceOf(QueryModelIdNotFoundException.class, exception.getCause());
⋮----
public void testWithUnknownIdAgainAndInsureNotCreated() throws InterruptedException, TimeoutException {
⋮----
CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, "unknown-id")
.toCompletableFuture();
⋮----
public void testRegisteredSchemas() throws RestClientException, IOException {
⋮----
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas("domainevents.WalletCreated", false, false);
assertFalse(registeredSchemas.isEmpty());
assertEquals(1, schemaRegistryClient.getVersion("domainevents.WalletCreated", registeredSchemas.getFirst()));
⋮----
registeredSchemas = schemaRegistryClient.getSchemas("domainevents.AccountCreated", false, false);
⋮----
assertEquals(1, schemaRegistryClient.getVersion("domainevents.AccountCreated", registeredSchemas.getFirst()));
⋮----
public void testCreateAndQueryWalletQueryModel() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
CreateWalletCommand createWalletCommand = new CreateWalletCommand(userId, "BTC");
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createWalletCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));
Assertions.assertNotNull(result);
Assertions.assertEquals(2, result.size());
assertInstanceOf(WalletCreatedEvent.class, result.getFirst());
assertInstanceOf(BalanceCreatedEvent.class, result.getLast());
⋮----
CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
WalletQueryModelState state = assertDoesNotThrow(() -> stateFuture.get(10, TimeUnit.SECONDS));
assertNotNull(state);
⋮----
public void testCreateAndQueryWalletQueryModelWithMultipleConcurrentRequests() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
List<String> userIds = List.of(
⋮----
List<CompletableFuture<List<DomainEvent>>> futures = userIds.stream()
.map(userId -> akcesClientController.send("TEST_TENANT", new CreateWalletCommand(userId, "BTC"))
.toCompletableFuture())
.toList();
⋮----
CompletableFuture<Void> allOf = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
allOf.get(10, TimeUnit.SECONDS);
⋮----
List<DomainEvent> result = future.get();
assertNotNull(result);
assertEquals(2, result.size());
assertInstanceOf(WalletCreatedEvent.class, result.get(0));
assertInstanceOf(BalanceCreatedEvent.class, result.get(1));
⋮----
List<CompletableFuture<WalletQueryModelState>> stateFutures = userIds.stream()
.map(userId -> akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
CompletableFuture<Void> allOfStates = CompletableFuture.allOf(stateFutures.toArray(new CompletableFuture[0]));
allOfStates.get(10, TimeUnit.SECONDS);
⋮----
WalletQueryModelState state = stateFuture.get();
⋮----
public void testAccountQueryModel() {
⋮----
CreateAccountCommand createAccountCommand = new CreateAccountCommand(userId, "US", "John", "Doe", "john.doe@example.com");
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createAccountCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));
⋮----
Assertions.assertEquals(1, result.size());
assertInstanceOf(AccountCreatedEvent.class, result.getFirst());
⋮----
CompletableFuture<AccountQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
⋮----
AccountQueryModelState state = assertDoesNotThrow(() -> stateFuture.get(10, TimeUnit.SECONDS));
⋮----
assertThat(state.country()).isEqualTo("US");
assertThat(state.firstName()).isEqualTo("John");
assertThat(state.lastName()).isEqualTo("Doe");
assertThat(state.email()).isEqualTo("john.doe@example.com");
⋮----
public void testAccountQueryModelUsingCache() {
⋮----
CompletableFuture<AccountQueryModelState> stateFuture1 = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
⋮----
assertNotNull(stateFuture1);
AccountQueryModelState state1 = assertDoesNotThrow(() -> stateFuture1.get(10, TimeUnit.SECONDS));
assertNotNull(state1);
assertThat(state1.country()).isEqualTo("US");
assertThat(state1.firstName()).isEqualTo("John");
assertThat(state1.lastName()).isEqualTo("Doe");
assertThat(state1.email()).isEqualTo("john.doe@example.com");
⋮----
CompletableFuture<AccountQueryModelState> stateFuture2 = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
⋮----
assertNotNull(stateFuture2);
AccountQueryModelState state2 = assertDoesNotThrow(() -> stateFuture2.get(10, TimeUnit.SECONDS));
assertNotNull(state2);
assertThat(state2.country()).isEqualTo("US");
assertThat(state2.firstName()).isEqualTo("John");
assertThat(state2.lastName()).isEqualTo("Doe");
assertThat(state2.email()).isEqualTo("john.doe@example.com");
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture);
WalletQueryModelState walletState = assertDoesNotThrow(() -> walletStateFuture.get(10, TimeUnit.SECONDS));
assertNotNull(walletState);
assertEquals(1, walletState.balances().size());
assertEquals("EUR", walletState.balances().getFirst().currency());
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture2 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture2);
WalletQueryModelState walletState2 = assertDoesNotThrow(() -> walletStateFuture2.get(10, TimeUnit.SECONDS));
assertNotNull(walletState2);
assertEquals(1, walletState2.balances().size());
assertEquals("EUR", walletState2.balances().getFirst().currency());
⋮----
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreateBalanceCommand(userId, "BTC"))
.toCompletableFuture().get(10, TimeUnit.SECONDS));
⋮----
assertInstanceOf(BalanceCreatedEvent.class, result.getFirst());
assertEquals("BTC", ((BalanceCreatedEvent) result.getFirst()).currency());
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture3 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture3);
WalletQueryModelState walletState3 = assertDoesNotThrow(() -> walletStateFuture3.get(10, TimeUnit.SECONDS));
assertNotNull(walletState3);
assertEquals(2, walletState3.balances().size());
assertEquals("EUR", walletState3.balances().getFirst().currency());
assertEquals("BTC", walletState3.balances().getLast().currency());
⋮----
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreateBalanceCommand(userId, "ETH"))
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture4 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture4);
WalletQueryModelState walletState4 = assertDoesNotThrow(() -> walletStateFuture4.get(10, TimeUnit.SECONDS));
assertNotNull(walletState4);
assertEquals(3, walletState4.balances().size());
assertEquals("EUR", walletState4.balances().get(0).currency());
assertEquals("BTC", walletState4.balances().get(1).currency());
assertEquals("ETH", walletState4.balances().get(2).currency());
⋮----
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreditWalletCommand(userId, "EUR", new BigDecimal("1000.00")))
⋮----
assertInstanceOf(WalletCreditedEvent.class, result.getFirst());
assertEquals("EUR", ((WalletCreditedEvent) result.getFirst()).currency());
assertEquals(new BigDecimal("1000.00"), ((WalletCreditedEvent) result.getFirst()).amount());
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture5 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture5);
WalletQueryModelState walletState5 = assertDoesNotThrow(() -> walletStateFuture5.get(10, TimeUnit.SECONDS));
assertNotNull(walletState5);
assertEquals(3, walletState5.balances().size());
assertEquals("EUR", walletState5.balances().getFirst().currency());
assertEquals(new BigDecimal("1000.00"), walletState5.balances().getFirst().amount());
⋮----
public static class ContextInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
List.of(
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)
⋮----
public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
⋮----
controlProducer.commitTransaction();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/KafkaAggregateRuntime.java
================
public class KafkaAggregateRuntime implements AggregateRuntime {
private static final Logger log = LoggerFactory.getLogger(KafkaAggregateRuntime.class);
⋮----
public String getName() {
return stateType.typeName();
⋮----
public Class<? extends Aggregate<?>> getAggregateClass() {
⋮----
public void handleCommandRecord(CommandRecord commandRecord,
⋮----
CommandType<?> commandType = getCommandType(commandRecord);
⋮----
if (commandType.create()) {
⋮----
if (stateRecordSupplier.get() != null) {
log.warn("Command {} wants to create a {} Aggregate with id {}, but the state already exists. Generating a AggregateAlreadyExistsError",
commandRecord.name(),
getName(),
commandRecord.aggregateId());
aggregateAlreadyExists(commandRecord, protocolRecordConsumer);
⋮----
handleCreateCommand(commandType, commandRecord, protocolRecordConsumer, domainEventIndexer);
⋮----
if (commandHandlers.containsKey(commandType)) {
handleCommand(commandType, commandRecord, protocolRecordConsumer, domainEventIndexer, stateRecordSupplier);
⋮----
commandExecutionError(commandRecord, protocolRecordConsumer, "No handler found for command " + commandRecord.name());
⋮----
log.error("Exception while handling command, sending CommandExecutionError", t);
commandExecutionError(commandRecord, protocolRecordConsumer, t);
⋮----
private void aggregateAlreadyExists(ProtocolRecord commandOrDomainEventRecord,
⋮----
AggregateAlreadyExistsErrorEvent errorEvent = new AggregateAlreadyExistsErrorEvent(commandOrDomainEventRecord.aggregateId(), this.getName());
DomainEventType<?> type = getDomainEventType(AggregateAlreadyExistsErrorEvent.class);
DomainEventRecord eventRecord = new DomainEventRecord(
commandOrDomainEventRecord.tenantId(),
type.typeName(),
type.version(),
serialize(errorEvent),
getEncoding(type),
errorEvent.getAggregateId(),
commandOrDomainEventRecord.correlationId(),
⋮----
protocolRecordConsumer.accept(eventRecord);
⋮----
private void commandExecutionError(CommandRecord commandRecord,
⋮----
commandExecutionError(commandRecord, protocolRecordConsumer, exception.getMessage());
⋮----
CommandExecutionErrorEvent errorEvent = new CommandExecutionErrorEvent(
commandRecord.aggregateId(),
this.getName(),
⋮----
commandRecord.tenantId(),
⋮----
commandRecord.correlationId(),
⋮----
private CommandType<?> getCommandType(CommandRecord commandRecord) {
return commandTypes.getOrDefault(commandRecord.name(), emptyList()).stream()
.filter(ct -> ct.version() == commandRecord.version())
.findFirst().orElseThrow(RuntimeException::new);
⋮----
private void indexDomainEventIfRequired(DomainEventRecord domainEventRecord,
⋮----
if (stateType.indexed()) {
domainEventIndexer.accept(domainEventRecord, new IndexParams(stateType.indexName(), state.getIndexKey(), createIndex));
⋮----
private void handleCreateCommand(CommandType<?> commandType,
⋮----
Command command = materialize(commandType, commandRecord);
⋮----
Stream<DomainEvent> domainEvents = commandCreateHandler.apply(command, null);
⋮----
Iterator<DomainEvent> itr = domainEvents.iterator();
DomainEvent domainEvent = itr.next();
⋮----
DomainEventType<?> domainEventType = getDomainEventType(domainEvent.getClass());
⋮----
AggregateState state = (createStateHandler.getEventType().equals(domainEventType)) ?
createStateHandler.apply(domainEvent, null) :
createStateHandler.apply(upcast(domainEvent, domainEventType), null);
⋮----
AggregateStateRecord stateRecord = new AggregateStateRecord(
⋮----
stateType.typeName(),
stateType.version(),
serialize(state),
getEncoding(stateType),
state.getAggregateId(),
⋮----
protocolRecordConsumer.accept(stateRecord);
⋮----
domainEventType.typeName(),
domainEventType.version(),
serialize(domainEvent),
getEncoding(domainEventType),
domainEvent.getAggregateId(),
⋮----
stateRecord.generation());
⋮----
indexDomainEventIfRequired(eventRecord, state, domainEventIndexer, true);
⋮----
while (itr.hasNext()) {
DomainEvent nextDomainEvent = itr.next();
currentStateRecord = processDomainEvent(
⋮----
private void handleCommand(CommandType<?> commandType,
⋮----
AggregateStateRecord currentStateRecord = stateRecordSupplier.get();
AggregateState currentState = materialize(currentStateRecord);
Stream<DomainEvent> domainEvents = commandHandlers.get(commandType).apply(command, currentState);
for (DomainEvent domainEvent : domainEvents.toList())
currentStateRecord = processDomainEvent(commandRecord.correlationId(),
⋮----
private void handleCreateEvent(DomainEventType<?> eventType,
⋮----
DomainEvent externalEvent = materialize(eventType, domainEventRecord);
⋮----
Stream<DomainEvent> domainEvents = eventCreateHandler.apply(externalEvent, null);
⋮----
AggregateState state = createStateHandler.apply(domainEvent, null);
⋮----
domainEventRecord.tenantId(),
⋮----
domainEventRecord.correlationId(),
⋮----
DomainEventType<?> type = getDomainEventType(domainEvent.getClass());
⋮----
private void handleEvent(DomainEventType<?> eventType,
⋮----
Stream<DomainEvent> domainEvents = eventHandlers.get(eventType).apply(externalEvent, currentState);
⋮----
private void handleBridgedEvent(DomainEventType<?> eventType,
⋮----
eventBridgeHandlers.get(eventType).apply(externalEvent, commandBus);
⋮----
private AggregateStateRecord processDomainEvent(String correlationId,
⋮----
AggregateState nextState = eventSourcingHandlers.containsKey(domainEventType) ?
eventSourcingHandlers.get(domainEventType).apply(domainEvent, currentState) :
eventSourcingHandlers.get(getDomainEventType(upcast(domainEvent, domainEventType).getClass()))
.apply(upcast(domainEvent, domainEventType), currentState);
⋮----
AggregateStateRecord nextStateRecord = new AggregateStateRecord(
currentStateRecord.tenantId(),
⋮----
serialize(nextState),
⋮----
currentStateRecord.aggregateId(),
⋮----
currentStateRecord.generation() + 1L);
protocolRecordConsumer.accept(nextStateRecord);
⋮----
nextStateRecord.generation());
⋮----
indexDomainEventIfRequired(eventRecord, nextState, domainEventIndexer, false);
⋮----
public void handleExternalDomainEventRecord(DomainEventRecord eventRecord,
⋮----
DomainEventType<?> eventRecordDomainEventType = getDomainEventType(eventRecord);
⋮----
DomainEventType<?> actualDomainEventType = upcastedType(eventRecordDomainEventType);
if (eventCreateHandler != null && eventCreateHandler.getEventType().equals(upcastedType(actualDomainEventType))) {
⋮----
log.warn("External DomainEvent {} wants to create a {} Aggregate with id {}, but the state already exists. Generate a AggregateAlreadyExistsError",
eventRecord.name(),
⋮----
eventRecord.aggregateId());
aggregateAlreadyExists(eventRecord, protocolRecordConsumer);
⋮----
handleCreateEvent(eventRecordDomainEventType, eventRecord, protocolRecordConsumer, domainEventIndexer);
⋮----
if (eventHandlers.containsKey(actualDomainEventType)) {
handleEvent(eventRecordDomainEventType, eventRecord, protocolRecordConsumer, domainEventIndexer, stateRecordSupplier);
} else if(eventBridgeHandlers.containsKey(actualDomainEventType)) {
handleBridgedEvent(eventRecordDomainEventType, eventRecord, commandBus);
⋮----
private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {
⋮----
return domainEvents.entrySet().stream()
.filter(entry -> entry.getValue().external())
.filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
.filter(entry -> entry.getValue().version() <= eventRecord.version())
.max(Comparator.comparingInt(entry -> entry.getValue().version()))
.map(Map.Entry::getValue).orElse(null);
⋮----
public Collection<DomainEventType<?>> getAllDomainEventTypes() {
return this.domainEvents.values();
⋮----
public Collection<DomainEventType<?>> getProducedDomainEventTypes() {
return this.domainEvents.values().stream().filter(domainEventType -> !domainEventType.external()).collect(Collectors.toSet());
⋮----
public Collection<DomainEventType<?>> getExternalDomainEventTypes() {
return this.domainEvents.values().stream().filter(DomainEventType::external).collect(Collectors.toSet());
⋮----
public Collection<CommandType<?>> getAllCommandTypes() {
return this.commandTypes.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());
⋮----
public Collection<CommandType<?>> getLocalCommandTypes() {
return this.commandTypes.values().stream().flatMap(Collection::stream).filter(commandType -> !commandType.external()).collect(Collectors.toSet());
⋮----
public Collection<CommandType<?>> getExternalCommandTypes() {
return this.commandTypes.values().stream().flatMap(Collection::stream).filter(CommandType::external).collect(Collectors.toSet());
⋮----
private DomainEventType<?> getDomainEventType(Class<?> domainEventClass) {
return domainEvents.get(domainEventClass);
⋮----
public CommandType<?> getLocalCommandType(String type, int version) {
return commandTypes.getOrDefault(type, Collections.emptyList()).stream()
.filter(commandType -> commandType.version() == version)
.findFirst()
.orElseThrow(() -> new IllegalArgumentException("No CommandType found for type " + type + " and version " + version));
⋮----
public boolean shouldGenerateGDPRKey(CommandRecord commandRecord) {
return getCommandType(commandRecord).create() && generateGDPRKeyOnCreate;
⋮----
public boolean shouldGenerateGDPRKey(DomainEventRecord eventRecord) {
return Optional.ofNullable(getDomainEventType(eventRecord))
.map(domainEventType -> domainEventType.create() && generateGDPRKeyOnCreate).orElse(false);
⋮----
public boolean requiresGDPRContext(DomainEventRecord eventRecord) {
⋮----
DomainEventType<?> domainEventType = getDomainEventType(eventRecord);
⋮----
return domainEventType.piiData() || (!eventBridgeHandlers.containsKey(domainEventType) && stateType.piiData());
⋮----
public boolean requiresGDPRContext(CommandRecord commandRecord) {
return this.stateType.piiData() || getCommandType(commandRecord).piiData();
⋮----
public boolean shouldHandlePIIData() {
⋮----
public void registerAndValidate(DomainEventType<?> domainEventType, boolean forceRegisterOnIncompatible) throws SchemaException {
⋮----
JsonSchema localSchema = schemaRegistry.registerAndValidate(domainEventType, forceRegisterOnIncompatible);
⋮----
domainEventSchemas.put(domainEventType.typeClass(), localSchema);
⋮----
public void registerAndValidate(CommandType<?> commandType,  boolean forceRegisterOnIncompatible) throws SchemaException {
if (!commandSchemas.containsKey(commandType.typeClass())) {
JsonSchema localSchema = schemaRegistry.registerAndValidate(commandType, forceRegisterOnIncompatible);
commandSchemas.put(commandType.typeClass(), localSchema);
if (commandType.external()) addCommand(commandType);
⋮----
public Command materialize(CommandType<?> type, CommandRecord commandRecord) throws IOException {
return objectMapper.readValue(commandRecord.payload(), type.typeClass());
⋮----
private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
DomainEvent domainEvent = objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
⋮----
if(eventUpcastingHandlers.containsKey(domainEventType)) {
return upcast(domainEvent,  domainEventType);
⋮----
private DomainEvent upcast(DomainEvent domainEvent, DomainEventType<?> domainEventType) {
final UpcastingHandlerFunction<DomainEvent, DomainEvent, DomainEventType<DomainEvent>, DomainEventType<DomainEvent>> upcastingHandlerFunction = eventUpcastingHandlers.get(domainEventType);
DomainEvent upcastedEvent = upcastingHandlerFunction.apply(domainEvent);
⋮----
if(eventUpcastingHandlers.containsKey(upcastingHandlerFunction.getOutputType())) {
return upcast(upcastedEvent, upcastingHandlerFunction.getOutputType());
⋮----
private DomainEventType<?> upcastedType(DomainEventType<?> type) {
return eventUpcastingHandlers.containsKey(type) ? upcastedType(eventUpcastingHandlers.get(type).getOutputType()) : type;
⋮----
private AggregateState materialize(AggregateStateRecord stateRecord) throws IOException {
AggregateStateType<?> stateType = getAggregateStateType(stateRecord);
AggregateState state = objectMapper.readValue(stateRecord.payload(), getAggregateStateType(stateRecord).typeClass());
⋮----
if(!stateType.equals(this.stateType)) {
return upcast(state, stateType);
⋮----
private AggregateState upcast(AggregateState state, AggregateStateType<?> inputStateType) {
final UpcastingHandlerFunction<AggregateState, AggregateState, AggregateStateType<AggregateState>, AggregateStateType<AggregateState>> upcastingHandlerFunction = stateUpcastingHandlers.get(inputStateType);
AggregateState upcastedState = upcastingHandlerFunction.apply(state);
if(!upcastingHandlerFunction.getOutputType().equals(stateType)) {
return upcast(upcastedState, upcastingHandlerFunction.getOutputType());
⋮----
private byte[] serialize(AggregateState state) throws IOException {
return objectMapper.writeValueAsBytes(state);
⋮----
private byte[] serialize(DomainEvent domainEvent) throws SerializationException {
JsonNode jsonNode = objectMapper.convertValue(domainEvent, JsonNode.class);
⋮----
domainEventSchemas.get(domainEvent.getClass()).validate(jsonNode);
return objectMapper.writeValueAsBytes(jsonNode);
⋮----
throw new SerializationException("Validation Failed while Serializing DomainEventClass " + domainEvent.getClass().getName(), e);
⋮----
throw new SerializationException("Serialization Failed while Serializing DomainEventClass " + domainEvent.getClass().getName(), e);
⋮----
public byte[] serialize(Command command) throws SerializationException {
JsonNode jsonNode = objectMapper.convertValue(command, JsonNode.class);
⋮----
commandSchemas.get(command.getClass()).validate(jsonNode);
return objectMapper.writeValueAsBytes(command);
⋮----
throw new SerializationException("Validation Failed while Serializing CommandClass " + command.getClass().getName(), e);
⋮----
throw new SerializationException("Serialization Failed while Serializing CommandClass " + command.getClass().getName(), e);
⋮----
private PayloadEncoding getEncoding(CommandType<?> type) {
⋮----
private PayloadEncoding getEncoding(DomainEventType<?> type) {
⋮----
private PayloadEncoding getEncoding(AggregateStateType<?> type) {
⋮----
private AggregateStateType<?> getAggregateStateType(AggregateStateRecord record) {
⋮----
if(stateType.typeName().equals(record.name()) && stateType.version() == record.version()) {
⋮----
return stateUpcastingHandlers.keySet().stream()
.filter(aggregateStateType -> aggregateStateType.version() == record.version())
.findAny().orElseThrow(() -> new IllegalStateException("Aggregate state type for " + record.name() + " with version " + record.version() +" does not exist"));
⋮----
private void addCommand(CommandType<?> commandType) {
this.commandTypes.computeIfAbsent(commandType.typeName(), typeName -> new ArrayList<>()).add(commandType);
⋮----
public static class Builder {
⋮----
public Builder setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
⋮----
public Builder setObjectMapper(ObjectMapper objectMapper) {
⋮----
public Builder setStateType(AggregateStateType<?> stateType) {
⋮----
public Builder setAggregateClass(Class<? extends Aggregate<?>> aggregateClass) {
⋮----
public Builder setCommandCreateHandler(CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler) {
⋮----
public Builder setEventCreateHandler(EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler) {
⋮----
public Builder setEventSourcingCreateHandler(EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler) {
⋮----
public Builder addDomainEvent(DomainEventType<?> domainEvent) {
this.domainEvents.put(domainEvent.typeClass(), domainEvent);
⋮----
public Builder addCommand(CommandType<?> commandType) {
this.commandTypes.computeIfAbsent(commandType.typeName(), s -> new ArrayList<>()).add(commandType);
⋮----
public Builder addCommandHandler(CommandType<?> commandType, CommandHandlerFunction<AggregateState, Command, DomainEvent> commandHandler) {
this.commandHandlers.put(commandType, commandHandler);
⋮----
public Builder addExternalEventHandler(DomainEventType<?> eventType, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventHandler) {
this.eventHandlers.put(eventType, eventHandler);
⋮----
public Builder addEventSourcingHandler(DomainEventType<?> eventType, EventSourcingHandlerFunction<AggregateState, DomainEvent> eventSourcingHandler) {
this.eventSourcingHandlers.put(eventType, eventSourcingHandler);
⋮----
public Builder addEventBridgeHandler(DomainEventType<?> eventType, EventBridgeHandlerFunction<AggregateState, DomainEvent> eventBridgeHandler) {
this.eventBridgeHandlers.put(eventType, eventBridgeHandler);
⋮----
public Builder addStateUpcastingHandler(AggregateStateType<?> inputType,
⋮----
this.stateUpcastingHandlers.put(inputType, upcastingHandler);
⋮----
public Builder addEventUpcastingHandler(DomainEventType<?> inputType,
⋮----
this.eventUpcastingHandlers.put(inputType, upcastingHandler);
⋮----
public Builder setGenerateGDPRKeyOnCreate(boolean generateGDPRKeyOnCreate) {
⋮----
private Builder validate() {
⋮----
throw new IllegalStateException("No create handler (either from command or event) configured");
⋮----
List<DomainEventType<?>> eventsWithoutHandlers = domainEvents.values().stream()
.filter(domainEventType -> !domainEventType.external())
.filter(domainEventType ->
!eventSourcingHandlers.containsKey(domainEventType) &&
!createStateHandler.getEventType().equals(domainEventType) &&
!eventUpcastingHandlers.containsKey(domainEventType))
⋮----
.filter(domainEventType -> !ErrorEvent.class.isAssignableFrom(domainEventType.typeClass()))
.toList();
⋮----
if (!eventsWithoutHandlers.isEmpty()) {
throw new IllegalStateException("The following domain events are missing EventSourcingHandlers: " +
eventsWithoutHandlers.stream()
.map(type -> type.typeName() + " v" + type.version())
.collect(Collectors.joining(", ")));
⋮----
Map<String, List<DomainEventType<?>>> eventsByName = domainEvents.values().stream()
⋮----
.collect(Collectors.groupingBy(DomainEventType::typeName));
⋮----
for (Map.Entry<String, List<DomainEventType<?>>> entry : eventsByName.entrySet()) {
List<DomainEventType<?>> versions = entry.getValue();
⋮----
versions.sort(Comparator.comparingInt(DomainEventType::version));
⋮----
if (versions.getFirst().version() != 1) {
throw new IllegalStateException("Event type " + versions.getFirst().typeName() +
⋮----
if (versions.size() <= 1) continue;
⋮----
for (int i = 0; i < versions.size() - 1; i++) {
DomainEventType<?> current = versions.get(i);
DomainEventType<?> next = versions.get(i + 1);
if (next.version() - current.version() > 1) {
throw new IllegalStateException("Gap detected in versions for event type " +
current.typeName() + ": missing version(s) between v" +
current.version() + " and v" + next.version());
⋮----
DomainEventType<?> olderVersion = versions.get(i);
if (!eventUpcastingHandlers.containsKey(olderVersion) && !eventSourcingHandlers.containsKey(olderVersion)) {
throw new IllegalStateException("Missing handler for " +
olderVersion.typeName() + " v" + olderVersion.version() +
⋮----
List<AggregateStateType<?>> stateTypes = new ArrayList<>(stateUpcastingHandlers.keySet());
stateTypes.add(stateType);
Map<String, List<AggregateStateType<?>>> statesByName = stateTypes.stream()
.collect(Collectors.groupingBy(AggregateStateType::typeName));
⋮----
for (Map.Entry<String, List<AggregateStateType<?>>> entry : statesByName.entrySet()) {
List<AggregateStateType<?>> versions = entry.getValue();
⋮----
versions.sort(Comparator.comparingInt(AggregateStateType::version));
⋮----
throw new IllegalStateException("State type " + versions.getFirst().typeName() +
⋮----
if (olderVersion.version() < latestVersion.version() &&
!stateUpcastingHandlers.containsKey(olderVersion)) {
throw new IllegalStateException("Missing upcasting handler for " +
⋮----
List<DomainEventType<?>> nonExternalEventsWithHandlers = eventHandlers.keySet().stream()
.filter(eventType -> !eventType.external())
⋮----
if (!nonExternalEventsWithHandlers.isEmpty()) {
throw new IllegalStateException("Event handlers found for non-external events: " +
nonExternalEventsWithHandlers.stream()
⋮----
public KafkaAggregateRuntime build() {
⋮----
final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation) ||
commandTypes.values().stream().flatMap(List::stream).map(CommandType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
⋮----
return new KafkaAggregateRuntime(
⋮----
public KafkaAggregateRuntime validateAndBuild() {
return validate().build();

================
File: FRAMEWORK_OVERVIEW.md
================
# Comprehensive Analysis of the Akces Framework - Updated Overview

## Introduction

The Akces Framework is a sophisticated event sourcing and CQRS (Command Query Responsibility Segregation) implementation built on Apache Kafka. It provides a comprehensive infrastructure for building distributed, event-driven applications with a clear separation between write and read concerns.

## Core Purpose and Values

Akces addresses several key challenges in distributed systems:

1. **Event Sourcing Implementation**: Provides a complete event sourcing framework where all changes to application state are captured as an immutable sequence of events.

2. **CQRS Architecture**: Enforces clean separation between command (write) and query (read) responsibilities for better scalability and performance.

3. **Partition-Based Scalability**: Leverages Kafka's partitioning for horizontal scaling of aggregates, allowing applications to scale with increasing load.

4. **Privacy By Design**: Built-in GDPR compliance through transparent encryption of personally identifiable information (PII).

5. **Schema Evolution**: Sophisticated schema management with backward compatibility checks to support evolving domain models.

6. **Process Management**: First-class support for process managers to orchestrate multi-step business processes across aggregates.

## Architecture Overview

Akces is organized into five main modules, each with distinct responsibilities:

### 1. API Module (`akces-api`)
Defines the core interfaces and annotations that make up the programming model:
- `Aggregate` and `AggregateState` interfaces
- Command and event interfaces with marker annotations
- Handler annotations (`@CommandHandler`, `@EventHandler`, etc.)
- Query model interfaces and annotations

### 2. Shared Module (`akces-shared`)
Contains common utilities and shared functionality:
- Protocol record definitions for Kafka communication
- GDPR compliance utilities with encryption/decryption
- Schema registry integration for JSON schema validation
- Serialization/deserialization support with Protocol Buffers
- RocksDB utilities for efficient state management

### 3. Runtime Module (`akces-runtime`)
Implements the core event sourcing infrastructure:
- Aggregate runtime for processing commands and events
- State repositories (RocksDB-based and in-memory)
- Command handling pipeline with validation
- Event sourcing mechanics for state reconstruction
- Kafka partition management for distributed processing

### 4. Client Module (`akces-client`)
Provides client-side functionality for command submission:
- Command sending with synchronous and asynchronous APIs
- Service discovery for routing commands to the correct aggregate
- Schema validation and compatibility checking
- Command response handling

### 5. Query Support Module (`akces-query-support`)
Implements the query side of CQRS:
- Query model runtime for maintaining read models
- Database model support (JDBC, JPA) for persistence
- Event handling for updating query models
- State hydration for efficient retrieval
- Caching for improved read performance

## Key Components and Patterns

### Aggregate Pattern

At the core of Akces is the concept of aggregates, which are domain entities that:
- Encapsulate business logic within consistency boundaries
- Respond to commands by validating and processing them
- Emit domain events representing facts that have occurred
- Maintain state through event sourcing mechanisms

```java
@AggregateInfo(value = "Wallet", version = 1)
public final class Wallet implements Aggregate<WalletState> {
    @CommandHandler(create = true)
    public Stream<DomainEvent> create(CreateWalletCommand cmd, WalletState isNull) {
        return Stream.of(new WalletCreatedEvent(cmd.id()));
    }
    
    @EventSourcingHandler(create = true)
    public WalletState create(WalletCreatedEvent event, WalletState isNull) {
        return new WalletState(event.id(), new ArrayList<>());
    }
}
```

### Command Handling

Commands are processed through a pipeline that:
1. Validates the command structure using JSON Schema
2. Routes the command to the appropriate aggregate partition
3. Processes the command to produce domain events
4. Applies those events to update the aggregate state
5. Persists both events and updated state to Kafka topics

```java
@CommandInfo(type = "CreateWallet", version = 1)
public record CreateWalletCommand(@AggregateIdentifier String id, String currency) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### Event Sourcing

The event sourcing mechanism:
1. Captures all state changes as immutable events in Kafka
2. Stores events in partitioned topics for efficient processing
3. Rebuilds aggregate state by replaying events
4. Uses RocksDB to maintain efficient state snapshots
5. Handles event upcasting for schema evolution

```java
@DomainEventInfo(type = "WalletCreated", version = 1)
public record WalletCreatedEvent(@AggregateIdentifier String id) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### GDPR Compliance

Akces provides sophisticated GDPR compliance through:
1. `@PIIData` annotation to mark sensitive fields
2. Transparent encryption/decryption during serialization
3. Key management through dedicated Kafka topics
4. Context-based encryption to secure personal data
5. Separation of encryption keys from data for better security

```java
public record AccountState(
    @AggregateIdentifier String userId,
    String country,
    @PIIData String firstName,
    @PIIData String lastName,
    @PIIData String email
) implements AggregateState {
    @Override
    public String getAggregateId() {
        return userId();
    }
}
```

### Query Models

For efficient reads, Akces implements:
1. Query models updated via domain events
2. State hydration from event streams
3. Caching mechanisms for efficient access
4. Projection-based read models optimized for specific query patterns

```java
@QueryModelInfo(value = "WalletQuery", version = 1, indexName = "Wallets")
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
    @QueryModelEventHandler(create = true)
    public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
        return new WalletQueryModelState(event.id(), List.of());
    }
    
    @QueryModelEventHandler
    public WalletQueryModelState creditWallet(WalletCreditedEvent event, WalletQueryModelState state) {
        return new WalletQueryModelState(
            state.walletId(),
            state.balances().stream()
                .map(balance -> {
                    if (balance.currency().equals(event.currency())) {
                        return new WalletQueryModelState.Balance(
                            balance.currency(),
                            balance.amount().add(event.amount()),
                            balance.reservedAmount()
                        );
                    }
                    return balance;
                })
                .toList()
        );
    }
}
```

### Database Models

For integration with traditional databases:
1. Database models updated from domain events
2. Support for JDBC and JPA persistence
3. Transactional updates with exactly-once semantics
4. Partition-aware offset tracking for reliable processing

```java
@DatabaseModelInfo(value = "WalletDatabase", version = 1)
public class WalletDatabaseModel extends JdbcDatabaseModel {
    @DatabaseModelEventHandler
    public void handle(WalletCreatedEvent event) {
        jdbcTemplate.update("INSERT INTO wallets (wallet_id) VALUES (?)", event.id());
    }
    
    @DatabaseModelEventHandler
    public void handle(WalletCreditedEvent event) {
        jdbcTemplate.update(
            "UPDATE wallet_balances SET amount = ? WHERE wallet_id = ? AND currency = ?",
            event.newBalance(),
            event.id(),
            event.currency()
        );
    }
}
```

### Process Managers

For coordinating complex workflows:
1. Process managers to orchestrate multi-step processes
2. Process state tracking to maintain workflow state
3. Event-driven process advancement
4. Error handling and compensation logic

```java
@AggregateInfo(value = "OrderProcessManager", version = 1)
public class OrderProcessManager implements ProcessManager<OrderProcessManagerState, OrderProcess> {
    @CommandHandler
    public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {
        String orderId = UUID.randomUUID().toString();
        
        // Reserve funds first - send command to Wallet aggregate
        getCommandBus().send(new ReserveAmountCommand(
            state.userId(),
            command.market().quoteCurrency(),
            command.quantity().multiply(command.limitPrice()),
            orderId
        ));
        
        return Stream.of(new BuyOrderCreatedEvent(
            state.userId(),
            orderId,
            command.market(),
            command.quantity(),
            command.limitPrice(),
            command.clientReference()
        ));
    }
    
    @EventHandler
    public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {
        if (state.hasAkcesProcess(event.referenceId())) {
            OrderProcess process = state.getAkcesProcess(event.referenceId());
            return Stream.of(new BuyOrderPlacedEvent(
                state.userId(),
                process.orderId(),
                process.market(),
                process.quantity(),
                process.limitPrice()
            ));
        }
        return Stream.empty();
    }
}
```

## Technical Implementation Details

### Partition-Based Processing

Akces utilizes Kafka's partitioning for scalability:
- Aggregates are distributed across partitions based on their ID
- Each partition is processed independently by a dedicated thread
- Partitions can be rebalanced across nodes dynamically
- State is maintained efficiently per partition using RocksDB

### Transactional Processing

Commands are processed transactionally:
- Kafka transactions ensure atomic updates
- State updates are coordinated with event publishing
- Exactly-once semantics are preserved
- Failures result in transaction rollbacks
- Consistent offset management for reliable processing

### Schema Management

Schema evolution is handled through:
- Schema registry integration (Confluent Schema Registry)
- JSON Schema validation for commands and events
- Automatic schema compatibility checks
- Version management for backward compatibility
- Support for upcasting to handle schema changes

### State Management

Aggregate state is managed efficiently:
- RocksDB for persistent state storage with high performance
- Transactional state updates with rollback capability
- Optimistic concurrency control
- In-memory caching for performance
- State snapshots to avoid full event replay

## Key Innovations

1. **Integrated GDPR Compliance**: Built-in support for handling personal data with transparent encryption, making compliance easier.

2. **Event Indexing**: Automatic indexing of events for efficient temporal queries and state reconstruction across aggregates.

3. **Flexible Deployment Models**: Support for different deployment topologies from monolithic to fully distributed microservices.

4. **RocksDB Integration**: Efficient state storage using RocksDB for high performance with durability.

5. **Process Manager Support**: First-class support for process managers to handle complex workflows across aggregate boundaries.

6. **Schema Evolution**: Sophisticated handling of schema changes with backward compatibility checks and upcasting.

7. **Annotation-Based Programming Model**: Intuitive, declarative programming model that reduces boilerplate code.

## Advantages Over Similar Frameworks

Compared to other event sourcing frameworks like Axon or EventStore:

1. **Kafka Foundation**: Built on Kafka for enterprise-grade scalability, reliability, and throughput.

2. **Privacy by Design**: First-class GDPR compliance baked into the core rather than as an afterthought.

3. **Schema Evolution**: Sophisticated schema management with backward compatibility checks and automatic validation.

4. **Programming Model**: Clean, annotation-based programming model that minimizes boilerplate and follows domain-driven design principles.

5. **Complete CQRS Stack**: Full support for both command and query sides with multiple implementation options.

6. **Partition-Based Scalability**: Leverages Kafka's partitioning for horizontal scaling without complex configuration.

## Usage Scenarios

Akces is well-suited for:

1. **Financial Systems**: Where audit trails, transaction integrity, and high throughput are critical requirements.

2. **Customer Data Platforms**: Where GDPR compliance is essential and personal data needs protection.

3. **Distributed Commerce Systems**: With complex workflows across services and high scalability needs.

4. **High-Scale Event-Driven Systems**: Requiring reliable, high-throughput event processing.

5. **Systems with Complex Temporal Requirements**: Needing historical state reconstruction and time-based queries.

6. **Microservice Architectures**: Where clear boundaries and eventual consistency are appropriate design choices.

## Limitations and Considerations

1. **Kafka Dependency**: Requires a well-configured Kafka cluster, which adds operational complexity.

2. **Learning Curve**: Event sourcing and CQRS patterns require a mindset shift for teams used to traditional CRUD.

3. **Eventual Consistency**: Query models may lag behind command processing, requiring careful design for user experience.

4. **Infrastructure Complexity**: Requires Schema Registry and additional components for full functionality.

5. **Performance Considerations**: Event replaying can be resource-intensive for aggregates with many events.

## Production Readiness

The framework includes several features that make it production-ready:

1. **Resilience**: Automatic recovery from failures through Kafka's reliability mechanisms.

2. **Observability**: Comprehensive logging and metrics for monitoring system behavior.

3. **Configuration Flexibility**: Extensive configuration options for tuning performance.

4. **Testing Support**: Built-in utilities for testing aggregates and command handlers.

5. **Deployment Options**: Support for containerized deployment in modern cloud environments.

## Conclusion

The Akces Framework provides a comprehensive solution for building event-sourced, CQRS-based applications with a focus on scalability, privacy, and developer experience. Its clean programming model, combined with sophisticated runtime components, addresses many common challenges in distributed systems development.

The framework's integration with Kafka provides a reliable foundation for high-throughput event processing, while its schema management and GDPR compliance features address important enterprise concerns. The partition-based processing model enables horizontal scaling, making it suitable for applications with demanding performance requirements.

By providing a complete implementation of event sourcing and CQRS patterns, Akces enables developers to focus on domain logic rather than infrastructure concerns, ultimately leading to more maintainable, scalable, and secure distributed applications.

================
File: README.md
================
# Akces Framework

## Overview

Akces is a powerful CQRS (Command Query Responsibility Segregation) and Event Sourcing framework built on Apache Kafka. It provides a comprehensive infrastructure for building distributed, event-driven applications with a clear separation between write operations (commands) and read operations (queries).

The framework implements the full event sourcing pattern, capturing all changes to application state as a sequence of events. These events serve as the system of record and can be replayed to reconstruct the state at any point in time, providing a complete audit trail and enabling temporal queries.

Akces leverages Kafka's distributed architecture for reliable event storage and processing, making it highly scalable and resilient. It also provides built-in support for privacy protection (GDPR compliance), schema evolution, and efficient state management.

## Core Concepts

- **Aggregates**: Domain entities that encapsulate business logic and maintain consistency boundaries
- **Commands**: Requests to perform actions that change the state of an aggregate
- **Domain Events**: Immutable records of facts that have occurred, representing state changes
- **Command Handlers**: Process commands and produce events 
- **Event Sourcing Handlers**: Apply events to update aggregate state
- **Query Models**: Read-optimized projections of aggregate state
- **Database Models**: Persistent storage of aggregate data optimized for queries
- **Process Managers**: Coordinate workflows across multiple aggregates

## Key Features

### Command Processing

- **Command Bus**: Distribute commands to appropriate aggregates
- **Command Validation**: Automatic schema-based validation using JSON Schema
- **Command Routing**: Intelligent routing based on aggregate IDs
- **Transactional Processing**: Atomic processing with Kafka transactions

### Event Sourcing

- **Event Store**: Kafka-based storage for all domain events
- **State Reconstruction**: Rebuild aggregate state by replaying events
- **Event Handlers**: React to events to trigger additional processes
- **Event Bridging**: Connect events from one aggregate to commands on another

### Aggregate Management

- **Partition-Based Processing**: Scale horizontally through Kafka partitioning
- **State Snapshots**: Efficient state storage using RocksDB
- **Aggregate Lifecycle**: Manage aggregate creation and updates
- **Event Indexing**: Index events for efficient retrieval

### Query Support

- **Query Models**: Build specialized read models from events
- **State Hydration**: Efficiently load and cache query model state
- **Database Integration**: Support for both JDBC and JPA database models
- **Event-Driven Updates**: Keep read models in sync with write models

### Privacy & GDPR

- **PII Data Protection**: Automatic encryption of personal data
- **Transparent Handling**: Annotation-based marking of sensitive fields
- **Key Management**: Secure handling of encryption keys
- **Context-Aware Processing**: Apply encryption based on context

### Schema Management

- **Schema Registry Integration**: Work with Confluent Schema Registry
- **Schema Evolution**: Support versioning and evolution of schemas
- **Compatibility Checking**: Ensure backward compatibility
- **Automatic Generation**: Generate JSON schemas from command and event classes

### Process Managers

- **Orchestration**: Manage complex workflows across multiple aggregates
- **Stateful Processing**: Maintain process state through events
- **Event-Driven Flow**: React to events to advance processes
- **Error Handling**: Built-in compensation logic for failures

## Architecture

Akces is organized into several Maven modules:

- **api**: Core interfaces and annotations defining the programming model
- **runtime**: Implementation of event sourcing and command handling
- **shared**: Common utilities, serialization, and GDPR functionality
- **client**: Client library for sending commands and processing responses
- **query-support**: Support for query models and database models

## Getting Started

### Prerequisites

- Java 21 or higher
- Apache Kafka 3.x with KRaft mode enabled
- Confluent Schema Registry
- Maven 3.6 or higher

### Maven Dependencies

Add the following to your `pom.xml`:

```xml
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-api</artifactId>
    <version>0.9.0</version>
</dependency>

<!-- For command senders -->
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-client</artifactId>
    <version>0.9.0</version>
</dependency>

<!-- For aggregate services -->
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-runtime</artifactId>
    <version>0.9.0</version>
</dependency>

<!-- For query models and database models -->
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-query-support</artifactId>
    <version>0.9.0</version>
</dependency>
```

### Configuration

Configure the framework in your `application.yaml`:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      enable-auto-commit: false
      isolation-level: read_committed
      max-poll-records: 500
      heartbeat-interval: 2000
      auto-offset-reset: latest
      properties:
        max.poll.interval.ms: 10000
        session.timeout.ms: 30000
        partition.assignment.strategy: org.apache.kafka.clients.consumer.CooperativeStickyAssignor
    producer:
      acks: all
      retries: 2147483647
      properties:
        linger.ms: 0
        retry.backoff.ms: 0
        enable.idempotence: true
        max.in.flight.requests.per.connection: 1

akces:
  schemaregistry:
    url: http://localhost:8081
  rocksdb:
    baseDir: /tmp/akces
```

## Usage Examples

### Defining an Aggregate

```java
@AggregateInfo(value = "Wallet", version = 1, indexed = true, indexName = "Wallets")
public final class Wallet implements Aggregate<WalletState> {
    @Override
    public Class<WalletState> getStateClass() {
        return WalletState.class;
    }

    @CommandHandler(create = true, produces = {WalletCreatedEvent.class, BalanceCreatedEvent.class})
    public Stream<DomainEvent> create(CreateWalletCommand cmd, WalletState isNull) {
        return Stream.of(new WalletCreatedEvent(cmd.id()), 
                         new BalanceCreatedEvent(cmd.id(), cmd.currency()));
    }

    @EventSourcingHandler(create = true)
    public WalletState create(WalletCreatedEvent event, WalletState isNull) {
        return new WalletState(event.id(), new ArrayList<>());
    }
    
    @EventSourcingHandler
    public WalletState createBalance(BalanceCreatedEvent event, WalletState state) {
        List<WalletState.Balance> balances = new ArrayList<>(state.balances());
        balances.add(new WalletState.Balance(event.currency(), BigDecimal.ZERO));
        return new WalletState(state.id(), balances);
    }
    
    @CommandHandler(produces = {WalletCreditedEvent.class})
    public Stream<DomainEvent> credit(CreditWalletCommand cmd, WalletState currentState) {
        WalletState.Balance balance = currentState.balances().stream()
                .filter(b -> b.currency().equals(cmd.currency()))
                .findFirst()
                .orElse(null);
                
        if (balance == null) {
            return Stream.of(new InvalidCurrencyErrorEvent(cmd.id(), cmd.currency()));
        }
        
        if (cmd.amount().compareTo(BigDecimal.ZERO) < 0) {
            return Stream.of(new InvalidAmountErrorEvent(cmd.id(), cmd.currency()));
        }
        
        return Stream.of(new WalletCreditedEvent(currentState.id(), 
                                               cmd.currency(), 
                                               cmd.amount(), 
                                               balance.amount().add(cmd.amount())));
    }
}
```

### Defining the Aggregate State

```java
public record WalletState(String id, List<Balance> balances) implements AggregateState {
    @Override
    public String getAggregateId() {
        return id();
    }

    public record Balance(String currency, BigDecimal amount, BigDecimal reservedAmount) {
        public Balance(String currency, BigDecimal amount) {
            this(currency, amount, BigDecimal.ZERO);
        }

        public BigDecimal getAvailableAmount() {
            return amount.subtract(reservedAmount);
        }
    }
}
```

### Creating Commands

```java
@CommandInfo(type = "CreateWallet", version = 1)
public record CreateWalletCommand(
    @AggregateIdentifier 
    @NotNull String id, 
    
    @NotNull String currency
) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@CommandInfo(type = "CreditWallet", version = 1)
public record CreditWalletCommand(
    @AggregateIdentifier 
    @NotNull String id,
    
    @NotNull String currency,
    
    @NotNull BigDecimal amount
) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### Creating Events

```java
@DomainEventInfo(type = "WalletCreated", version = 1)
public record WalletCreatedEvent(
    @AggregateIdentifier 
    @NotNull String id
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@DomainEventInfo(type = "BalanceCreated", version = 1)
public record BalanceCreatedEvent(
    @AggregateIdentifier 
    @NotNull String id, 
    
    @NotNull String currency
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@DomainEventInfo(type = "WalletCredited", version = 1)
public record WalletCreditedEvent(
    @AggregateIdentifier 
    @NotNull String id,
    
    @NotNull String currency,
    
    @NotNull BigDecimal amount,
    
    @NotNull BigDecimal newBalance
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### Error Events

```java
@DomainEventInfo(type = "InvalidCurrencyError", version = 1)
public record InvalidCurrencyErrorEvent(
    @AggregateIdentifier 
    @NotNull String walletId,
    
    @NotNull String currency
) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return walletId();
    }
}

@DomainEventInfo(type = "InvalidAmountError", version = 1)
public record InvalidAmountErrorEvent(
    @AggregateIdentifier 
    @NotNull String walletId,
    
    @NotNull String currency
) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return walletId();
    }
}
```

### Sending Commands

```java
@Service
public class WalletService {
    private final AkcesClient akcesClient;
    
    @Autowired
    public WalletService(AkcesClient akcesClient) {
        this.akcesClient = akcesClient;
    }
    
    public String createWallet(String currency) {
        String walletId = UUID.randomUUID().toString();
        CreateWalletCommand command = new CreateWalletCommand(walletId, currency);
        
        // Send command and wait for response
        List<DomainEvent> events = akcesClient.send("DEFAULT_TENANT", command)
            .toCompletableFuture()
            .join();
            
        // Check for success
        if (events.stream().anyMatch(e -> e instanceof ErrorEvent)) {
            throw new RuntimeException("Failed to create wallet");
        }
        
        return walletId;
    }
    
    public void creditWallet(String walletId, String currency, BigDecimal amount) {
        CreditWalletCommand command = new CreditWalletCommand(walletId, currency, amount);
        
        try {
            // Send command without waiting for response
            akcesClient.sendAndForget("DEFAULT_TENANT", command);
        } catch (CommandRefusedException e) {
            // Handle specific command exceptions
            throw new RuntimeException("Command refused: " + e.getMessage());
        } catch (CommandValidationException e) {
            throw new RuntimeException("Invalid command: " + e.getMessage());
        }
    }
}
```

### Creating a Query Model

```java
@QueryModelInfo(value = "WalletQuery", version = 1, indexName = "Wallets")
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
    @Override
    public Class<WalletQueryModelState> getStateClass() {
        return WalletQueryModelState.class;
    }
    
    @Override
    public String getIndexName() {
        return "Wallets";
    }

    @QueryModelEventHandler(create = true)
    public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
        return new WalletQueryModelState(event.id(), List.of());
    }
    
    @QueryModelEventHandler
    public WalletQueryModelState createBalance(BalanceCreatedEvent event, WalletQueryModelState state) {
        List<WalletQueryModelState.Balance> balances = new ArrayList<>(state.balances());
        balances.add(new WalletQueryModelState.Balance(event.currency(), BigDecimal.ZERO));
        return new WalletQueryModelState(state.walletId(), balances);
    }
    
    @QueryModelEventHandler
    public WalletQueryModelState creditWallet(WalletCreditedEvent event, WalletQueryModelState state) {
        return new WalletQueryModelState(
            state.walletId(),
            state.balances().stream()
                .map(balance -> {
                    if (balance.currency().equals(event.currency())) {
                        return new WalletQueryModelState.Balance(
                            balance.currency(),
                            balance.amount().add(event.amount()),
                            balance.reservedAmount()
                        );
                    }
                    return balance;
                })
                .toList()
        );
    }
}

public record WalletQueryModelState(String walletId, List<Balance> balances) implements QueryModelState {
    @Override
    public String getIndexKey() {
        return walletId();
    }
    
    public record Balance(String currency, BigDecimal amount, BigDecimal reservedAmount) {
        public Balance(String currency, BigDecimal amount) {
            this(currency, amount, BigDecimal.ZERO);
        }
        
        public BigDecimal getAvailableAmount() {
            return amount.subtract(reservedAmount);
        }
    }
}
```

### Querying a Model

```java
@RestController
@RequestMapping("/wallets")
public class WalletController {
    private final QueryModels queryModels;
    
    @Autowired
    public WalletController(QueryModels queryModels) {
        this.queryModels = queryModels;
    }
    
    @GetMapping("/{walletId}")
    public ResponseEntity<WalletQueryModelState> getWallet(@PathVariable String walletId) {
        try {
            WalletQueryModelState wallet = queryModels.getHydratedState(WalletQueryModel.class, walletId)
                .toCompletableFuture()
                .get(5, TimeUnit.SECONDS);
                
            return ResponseEntity.ok(wallet);
        } catch (QueryModelIdNotFoundException e) {
            return ResponseEntity.notFound().build();
        } catch (Exception e) {
            return ResponseEntity.status(500).build();
        }
    }
}
```

### Creating a Database Model

```java
@DatabaseModelInfo(value = "WalletDB", version = 1)
public class WalletDatabaseModel extends JdbcDatabaseModel {

    @Autowired
    public WalletDatabaseModel(JdbcTemplate jdbcTemplate, PlatformTransactionManager transactionManager) {
        super(jdbcTemplate, transactionManager);
    }
    
    @DatabaseModelEventHandler
    public void handle(WalletCreatedEvent event) {
        jdbcTemplate.update(
            "INSERT INTO wallets (wallet_id, created_at) VALUES (?, NOW())",
            event.id()
        );
    }
    
    @DatabaseModelEventHandler
    public void handle(BalanceCreatedEvent event) {
        jdbcTemplate.update(
            "INSERT INTO wallet_balances (wallet_id, currency, amount, reserved_amount) VALUES (?, ?, 0, 0)",
            event.id(),
            event.currency()
        );
    }
    
    @DatabaseModelEventHandler
    public void handle(WalletCreditedEvent event) {
        jdbcTemplate.update(
            "UPDATE wallet_balances SET amount = ? WHERE wallet_id = ? AND currency = ?",
            event.newBalance(),
            event.id(),
            event.currency()
        );
    }
}
```

### GDPR and PII Data

Akces provides built-in support for handling personal identifiable information (PII):

```java
@AggregateStateInfo(value = "UserState", version = 1)
public record UserState(
    @AggregateIdentifier 
    String userId,
    
    String country,
    
    @PIIData 
    String firstName,
    
    @PIIData 
    String lastName,
    
    @PIIData 
    String email
) implements AggregateState {
    @Override
    public String getAggregateId() {
        return userId();
    }
}
```

With this annotation, the framework automatically:
- Encrypts PII data before storing it
- Decrypts PII data when loading it
- Manages encryption keys securely
- Ensures only authorized access to decrypted data

### Process Managers

For coordinating complex workflows across multiple aggregates:

```java
@AggregateInfo(value = "OrderProcessManager", version = 1)
public class OrderProcessManager implements ProcessManager<OrderProcessManagerState, OrderProcess> {
    
    @Override
    public Class<OrderProcessManagerState> getStateClass() {
        return OrderProcessManagerState.class;
    }
    
    @EventHandler(create = true)
    public Stream<UserOrderProcessesCreatedEvent> create(AccountCreatedEvent event, OrderProcessManagerState isNull) {
        return Stream.of(new UserOrderProcessesCreatedEvent(event.userId()));
    }
    
    @EventSourcingHandler(create = true)
    public OrderProcessManagerState create(UserOrderProcessesCreatedEvent event, OrderProcessManagerState isNull) {
        return new OrderProcessManagerState(event.userId());
    }
    
    @CommandHandler
    public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {
        String orderId = UUID.randomUUID().toString();
        
        // Reserve funds first - send command to Wallet aggregate
        getCommandBus().send(new ReserveAmountCommand(
            state.userId(),
            command.market().quoteCurrency(),
            command.quantity().multiply(command.limitPrice()),
            orderId
        ));
        
        // Create order record
        return Stream.of(new BuyOrderCreatedEvent(
            state.userId(),
            orderId,
            command.market(),
            command.quantity(),
            command.limitPrice(),
            command.clientReference()
        ));
    }
    
    @EventHandler
    public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {
        if (state.hasAkcesProcess(event.referenceId())) {
            OrderProcess process = state.getAkcesProcess(event.referenceId());
            return Stream.of(new BuyOrderPlacedEvent(
                state.userId(),
                process.orderId(),
                process.market(),
                process.quantity(),
                process.limitPrice()
            ));
        }
        return Stream.empty();
    }
    
    @EventHandler
    public Stream<DomainEvent> handle(InsufficientFundsErrorEvent errorEvent, OrderProcessManagerState state) {
        if (state.hasAkcesProcess(errorEvent.referenceId())) {
            return Stream.of(state.getAkcesProcess(errorEvent.referenceId()).handle(errorEvent));
        }
        return Stream.empty();
    }
}
```

## Schema Evolution

Akces supports evolving your domain model over time:

```java
// Original version
@DomainEventInfo(type = "AccountCreated", version = 1)
public record AccountCreatedEvent(
    @AggregateIdentifier String userId,
    String country,
    String firstName,
    String lastName,
    String email
) implements DomainEvent { 
    @Override
    public String getAggregateId() {
        return userId();
    }
}

// New version with additional field
@DomainEventInfo(type = "AccountCreated", version = 2)
public record AccountCreatedEventV2(
    @AggregateIdentifier String userId,
    String country,
    String firstName,
    String lastName,
    String email,
    Boolean twoFactorEnabled
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

// The upcasting handler
@UpcastingHandler
public AccountCreatedEventV2 cast(AccountCreatedEvent event) {
    return new AccountCreatedEventV2(
        event.userId(), 
        event.country(), 
        event.firstName(), 
        event.lastName(), 
        event.email(), 
        false // Default value for new field
    );
}
```

## Running the Framework

### Aggregate Service

```java
@SpringBootApplication
public class AggregateServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(AggregateServiceApplication.class, args);
    }
}
```

### Query Service

```java
@SpringBootApplication
public class QueryServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(QueryServiceApplication.class, args);
    }
}
```

### Client Application

```java
@SpringBootApplication
public class ClientApplication {
    public static void main(String[] args) {
        SpringApplication.run(ClientApplication.class, args);
    }
}
```

## Benefits of Using Akces

- **Scalability**: Built on Kafka for horizontal scaling across multiple nodes
- **Reliability**: Event sourcing ensures data integrity and complete audit trails
- **Flexibility**: Clean separation of commands and queries with CQRS
- **Performance**: Efficient state management with RocksDB and optimized query models
- **Security**: Built-in GDPR compliance with transparent PII handling
- **Evolution**: Schema evolution with backward compatibility checks
- **Developer Experience**: Intuitive annotation-based programming model
- **Observability**: Complete visibility into all commands and events

## License

Apache License 2.0

## Release Process

This project uses the Maven Release Plugin and GitHub Actions to create releases.
Run `mvn release:prepare release:perform && git push` to select the version to be released and create a VCS tag.

GitHub Actions will start [the build process](https://github.com/elasticsoftwarefoundation/akces-framework/actions/workflows/maven-publish.yml).

If successful, the build will be automatically published to [Github Packages](https://maven.pkg.github.com/elasticsoftwarefoundation/akces-framework/).

================
File: main/api/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.9.1-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-api</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: API</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testng</groupId>
            <artifactId>testng</artifactId>
            <scope>test</scope>
        </dependency>







    </dependencies>

    <build>
        <plugins>
        </plugins>
    </build>
</project>

================
File: main/client/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.9.1-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Client</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-client</artifactId>
    <packaging>jar</packaging>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
    </dependencies>

</project>

================
File: main/query-support/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.9.1-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Query Support</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-query-support</artifactId>
    <packaging>jar</packaging>

    <properties>

    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-jdbc</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.persistence</groupId>
            <artifactId>jakarta.persistence-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.data</groupId>
            <artifactId>spring-data-jpa</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>

        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>postgresql</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.postgresql</groupId>
            <artifactId>postgresql</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.liquibase</groupId>
            <artifactId>liquibase-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-jdbc</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-runtime</artifactId>
            <classifier>tests</classifier>
            <type>test-jar</type>
            <version>${project.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-client</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-runtime</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
    </dependencies>

</project>

================
File: main/runtime/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.9.1-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-runtime</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: Runtime</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>com.google.protobuf</groupId>
            <artifactId>protobuf-java</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.semver4j</groupId>
            <artifactId>semver4j</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-client</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <executions>
                    <execution>
                        <goals>
                            <goal>test-jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>

================
File: main/shared/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.9.1-SNAPSHOT</version>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Shared Classes</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-shared</artifactId>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.semver4j</groupId>
            <artifactId>semver4j</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.annotation</groupId>
            <artifactId>jakarta.annotation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.rocksdb</groupId>
            <artifactId>rocksdbjni</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.ben-manes.caffeine</groupId>
            <artifactId>caffeine</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

================
File: main/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-parent</artifactId>
        <version>0.9.1-SNAPSHOT</version>
    </parent>

    <artifactId>akces-framework-main</artifactId>
    <packaging>pom</packaging>

    <name>Elastic Software Foundation :: Akces :: Main module</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <protobuf.version>4.30.0</protobuf.version>
        <log4j.version>2.24.3</log4j.version>
        <jackson.version>2.18.3</jackson.version>
        <slf4j.version>2.0.17</slf4j.version>
        <javassist.version>3.30.2-GA</javassist.version>
        <lz4.version>1.8.0</lz4.version>
        <kafka.version>3.8.1</kafka.version>
        <java-uuid-generator.version>5.1.0</java-uuid-generator.version>
        <rocksdb.version>9.10.0</rocksdb.version>
        <semver4j.version>5.6.0</semver4j.version>
        <auto-service.version>1.1.1</auto-service.version>

        <testng-version>7.11.0</testng-version>
        <awaitility.version>4.3.0</awaitility.version>
        <persistence-api.version>2.2</persistence-api.version>
        <plexus-utils.version>4.0.2</plexus-utils.version>
        <netty.version>4.1.119.Final</netty.version>
        <micrometer.version>1.14.4</micrometer.version>
        <fast-uuid.version>0.2.0</fast-uuid.version>
        <confluent.version>7.9.0</confluent.version>
        <victools.version>4.37.0</victools.version>
        <joda-time.version>2.13.1</joda-time.version>
        <logback.version>1.5.17</logback.version>
        <testcontainers.version>1.20.6</testcontainers.version>
        <compile-testing.version>0.21.0</compile-testing.version>
    </properties>

    <scm>
        <connection>scm:git:git@github.com:elasticsoftwarefoundation/elasticactors.git</connection>
        <developerConnection>scm:git:git@github.com:elasticsoftwarefoundation/elasticactors.git
        </developerConnection>
        <url>https://github.com/elasticsoftwarefoundation/elasticactors</url>
        <tag>v0.3.4</tag>
    </scm>

    <distributionManagement>
        <repository>
            <id>github</id>
            <name>GitHub Packages</name>
            <url>https://maven.pkg.github.com/elasticsoftwarefoundation/akces-framework</url>
        </repository>








    </distributionManagement>

    <repositories>
        <repository>
            <id>central</id>
            <url>https://repo.maven.apache.org/maven2</url>
        </repository>
        <repository>
            <id>Confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
    </repositories>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>com.fasterxml.jackson</groupId>
                <artifactId>jackson-bom</artifactId>
                <version>${jackson.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>io.netty</groupId>
                <artifactId>netty-bom</artifactId>
                <version>${netty.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>com.google.guava</groupId>
                <artifactId>guava-bom</artifactId>
                <version>${guava.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>io.micrometer</groupId>
                <artifactId>micrometer-bom</artifactId>
                <version>${micrometer.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>jakarta.validation</groupId>
                <artifactId>jakarta.validation-api</artifactId>
                <version>3.1.1</version>
            </dependency>
            <dependency>
                <groupId>jakarta.inject</groupId>
                <artifactId>jakarta.inject-api</artifactId>
                <version>2.0.1</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-generator</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-module-jakarta-validation</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-module-jackson</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>commons-validator</groupId>
                <artifactId>commons-validator</artifactId>
                <version>1.9.0</version>
            </dependency>
            <dependency>
                <groupId>org.apache.commons</groupId>
                <artifactId>commons-compress</artifactId>
                <version>1.27.1</version>
            </dependency>
            <dependency>
                <groupId>commons-io</groupId>
                <artifactId>commons-io</artifactId>
                <version>2.18.0</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-json-serializer</artifactId>
                <version>${confluent.version}</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-json-schema-serializer</artifactId>
                <version>${confluent.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>joda-time</groupId>
                        <artifactId>joda-time</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.jetbrains.kotlin</groupId>
                        <artifactId>kotlin-scripting-compiler-embeddable</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.jetbrains</groupId>
                        <artifactId>annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.scala-lang</groupId>
                        <artifactId>scala-library</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>commons-validator</groupId>
                        <artifactId>commons-validator</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.checkerframework</groupId>
                        <artifactId>checker-qual</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.guava</groupId>
                        <artifactId>guava</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>joda-time</groupId>
                <artifactId>joda-time</artifactId>
                <version>${joda-time.version}</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-protobuf-serializer</artifactId>
                <version>${confluent.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.jetbrains.kotlin</groupId>
                        <artifactId>*</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.errorprone</groupId>
                        <artifactId>error_prone_annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.apache.commons</groupId>
                        <artifactId>commons-compress</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.checkerframework</groupId>
                        <artifactId>checker-qual</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.squareup.okio</groupId>
                        <artifactId>okio-jvm</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.j2objc</groupId>
                        <artifactId>j2objc-annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.protobuf</groupId>
                        <artifactId>protobuf-java</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.lz4</groupId>
                <artifactId>lz4-java</artifactId>
                <version>${lz4.version}</version>
            </dependency>
            <dependency>
                <groupId>com.eatthepath</groupId>
                <artifactId>fast-uuid</artifactId>
                <version>${fast-uuid.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-beans</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-context</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.aspectj</groupId>
                <artifactId>aspectjrt</artifactId>
                <version>${aspectj.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-tx</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-core</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-context-support</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-api</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>jcl-over-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>jul-to-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>log4j-over-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>ch.qos.logback</groupId>
                <artifactId>logback-classic</artifactId>
                <version>${logback.version}</version>
            </dependency>
            <dependency>
                <groupId>ch.qos.logback</groupId>
                <artifactId>logback-core</artifactId>
                <version>${logback.version}</version>
            </dependency>
            <dependency>
                <groupId>org.javassist</groupId>
                <artifactId>javassist</artifactId>
                <version>${javassist.version}</version>
            </dependency>
            <dependency>
                <groupId>com.fasterxml.uuid</groupId>
                <artifactId>java-uuid-generator</artifactId>
                <version>${java-uuid-generator.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-webmvc</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-aspects</artifactId>
                <version>${spring.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.aspectj</groupId>
                        <artifactId>aspectjweaver</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-aop</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>javax.persistence</groupId>
                <artifactId>javax.persistence-api</artifactId>
                <version>${persistence-api.version}</version>
            </dependency>
            <dependency>
                <groupId>com.google.protobuf</groupId>
                <artifactId>protobuf-java</artifactId>
                <version>${protobuf.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-clients</artifactId>
                <version>${kafka.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-streams</artifactId>
                <version>${kafka.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.rocksdb</groupId>
                        <artifactId>rocksdbjni</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.awaitility</groupId>
                <artifactId>awaitility</artifactId>
                <version>${awaitility.version}</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.plexus</groupId>
                <artifactId>plexus-utils</artifactId>
                <version>${plexus-utils.version}</version>
            </dependency>
            <dependency>
                <groupId>javax.annotation</groupId>
                <artifactId>javax.annotation-api</artifactId>
                <version>${javax-annotation-api.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testng</groupId>
                <artifactId>testng</artifactId>
                <version>${testng-version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.yaml</groupId>
                        <artifactId>snakeyaml</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.mockito</groupId>
                <artifactId>mockito-core</artifactId>
                <version>${mockito.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-test</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>kafka</artifactId>
                <version>${testcontainers.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>junit-jupiter</artifactId>
                <version>${testcontainers.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.ben-manes.caffeine</groupId>
                <artifactId>caffeine</artifactId>
                <version>${caffeine.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>com.google.errorprone</groupId>
                        <artifactId>error_prone_annotations</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.semver4j</groupId>
                <artifactId>semver4j</artifactId>
                <version>${semver4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-simple</artifactId>
                <version>${slf4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.apache.logging.log4j</groupId>
                <artifactId>log4j-core</artifactId>
                <version>${log4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>${spring-boot.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
            <dependency>
                <groupId>org.rocksdb</groupId>
                <artifactId>rocksdbjni</artifactId>
                <version>${rocksdb.version}</version>
            </dependency>
            <dependency>
                <groupId>com.google.testing.compile</groupId>
                <artifactId>compile-testing</artifactId>
                <version>${compile-testing.version}</version>
            </dependency>
            <dependency>
                <groupId>com.google.auto.service</groupId>
                <artifactId>auto-service</artifactId>
                <version>${auto-service.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <modules>
        <module>api</module>
        <module>runtime</module>
        <module>client</module>
        <module>shared</module>
        <module>query-support</module>
        <module>eventcatalog</module>
    </modules>

    <build>
        <resources>
            <resource>
                <filtering>false</filtering>
                <directory>${basedir}/src/main/resources</directory>
                <includes>
                    <include>*.xml</include>
                    <include>*.yaml</include>
                    <include>*.properties</include>
                    <include>META-INF/*</include>
                    <include>META-INF/services/*</include>
                    <include>META-INF/spring/*</include>
                    <include>protobuf/*.proto</include>
                </includes>
            </resource>
        </resources>
        <testResources>
            <testResource>
                <filtering>true</filtering>
                <directory>${basedir}/src/test/resources</directory>
                <includes>
                    <include>*.xml</include>
                    <include>*.properties</include>
                    <include>**/*.yaml</include>
                    <include>META-INF/*</include>
                </includes>
            </testResource>
        </testResources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>



================================================================
End of Codebase
================================================================
