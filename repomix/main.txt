This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where comments have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: *.md, main/**/*.java, main/**/*.xml, main/**/*.properties, main/**/*.proto, main/**/*.imports, main/**/*.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Content has been compressed - code blocks are separated by ⋮---- delimiter

Additional Info:
----------------

================================================================
Directory Structure
================================================================
main/
  api/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                aggregate/
                  Aggregate.java
                  AggregateState.java
                  AggregateStateType.java
                  CommandHandlerFunction.java
                  CommandType.java
                  DomainEventType.java
                  EventBridgeHandlerFunction.java
                  EventHandlerFunction.java
                  EventSourcingHandlerFunction.java
                  SchemaType.java
                annotations/
                  AggregateIdentifier.java
                  AggregateInfo.java
                  AggregateStateInfo.java
                  CommandHandler.java
                  CommandInfo.java
                  DatabaseModelEventHandler.java
                  DatabaseModelInfo.java
                  DomainEventInfo.java
                  EventBridgeHandler.java
                  EventHandler.java
                  EventSourcingHandler.java
                  PIIData.java
                  QueryModelEventHandler.java
                  QueryModelInfo.java
                  QueryModelStateInfo.java
                commands/
                  Command.java
                  CommandBus.java
                  CommandBusHolder.java
                events/
                  DomainEvent.java
                  ErrorEvent.java
                processmanager/
                  AkcesProcess.java
                  ProcessManager.java
                  ProcessManagerState.java
                  UnknownAkcesProcessException.java
                query/
                  DatabaseModel.java
                  DatabaseModelEventHandlerFunction.java
                  QueryModel.java
                  QueryModelEventHandlerFunction.java
                  QueryModelState.java
                  QueryModelStateType.java
                AkcesException.java
    pom.xml
  client/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                client/
                  AkcesClient.java
                  AkcesClientAutoConfiguration.java
                  AkcesClientCommandException.java
                  AkcesClientController.java
                  AkcesClientControllerState.java
                  CommandRefusedException.java
                  CommandSendingFailedException.java
                  CommandSerializationException.java
                  CommandServiceApplication.java
                  CommandValidationException.java
                  MissingDomainEventException.java
                  UnknownSchemaException.java
                  UnroutableCommandException.java
        resources/
          META-INF/
            spring/
              org.springframework.boot.autoconfigure.AutoConfiguration.imports
          akces-client.properties
      test/
        java/
          org/
            elasticsoftware/
              akces/
                client/
                  commands/
                    CreateAccountCommand.java
                    InvalidCommand.java
                    UnroutableCommand.java
                  events/
                    AccountCreatedEvent.java
                  AkcesClientTestConfiguration.java
                  AkcesClientTests.java
        resources/
          akces-client.properties
          logback-test.xml
    pom.xml
  query-support/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                query/
                  database/
                    beans/
                      DatabaseModelBeanFactoryPostProcessor.java
                      DatabaseModelEventHandlerFunctionAdapter.java
                    jdbc/
                      JdbcDatabaseModel.java
                    jpa/
                      JpaDatabaseModel.java
                      PartitionOffset.java
                      PartitionOffsetRepository.java
                    AkcesDatabaseModelAutoConfiguration.java
                    AkcesDatabaseModelController.java
                    AkcesDatabaseModelControllerState.java
                    DatabaseModelImplementationPresentCondition.java
                    DatabaseModelPartition.java
                    DatabaseModelPartitionState.java
                    DatabaseModelRuntime.java
                    DatabaseModelRuntimeFactory.java
                    KafkaDatabaseModelRuntime.java
                  models/
                    beans/
                      QueryModelBeanFactoryPostProcessor.java
                      QueryModelEventHandlerFunctionAdapter.java
                    AkcesQueryModelAutoConfiguration.java
                    AkcesQueryModelController.java
                    AkcesQueryModelControllerState.java
                    KafkaQueryModelRuntime.java
                    QueryModelExecutionCancelledException.java
                    QueryModelExecutionDisabledException.java
                    QueryModelExecutionException.java
                    QueryModelIdNotFoundException.java
                    QueryModelImplementationPresentCondition.java
                    QueryModelNotFoundException.java
                    QueryModelRuntime.java
                    QueryModelRuntimeFactory.java
                    QueryModels.java
                  QueryServiceApplication.java
        resources/
          META-INF/
            spring/
              org.springframework.boot.autoconfigure.AutoConfiguration.imports
          akces-databasemodel.properties
          akces-querymodel.properties
      test/
        java/
          org/
            elasticsoftware/
              akces/
                query/
                  database/
                    model/
                      DefaultJdbcModel.java
                    DatabaseModelRuntimeTests.java
                    DatabaseModelTestConfiguration.java
                  models/
                    account/
                      AccountQueryModel.java
                      AccountQueryModelState.java
                    wallet/
                      WalletQueryModel.java
                      WalletQueryModelState.java
                    QueryModelRuntimeTests.java
                    QueryModelTestConfiguration.java
        resources/
          db/
            changelog/
              liquibase.yaml
          logback-test.xml
    pom.xml
  runtime/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                aggregate/
                  AggregateRuntime.java
                  IndexParams.java
                beans/
                  AggregateBeanFactoryPostProcessor.java
                  CommandHandlerFunctionAdapter.java
                  DomainEventTypeValueCodeGeneratorDelegate.java
                  EventBridgeHandlerFunctionAdapter.java
                  EventHandlerFunctionAdapter.java
                  EventSourcingHandlerFunctionAdapter.java
                kafka/
                  AggregatePartition.java
                  AggregatePartitionCommandBus.java
                  AggregatePartitionState.java
                  AggregateRuntimeFactory.java
                  KafkaAggregateRuntime.java
                  PartitionUtils.java
                state/
                  AggregateStateRepository.java
                  AggregateStateRepositoryException.java
                  AggregateStateRepositoryFactory.java
                  InMemoryAggregateStateRepository.java
                  InMemoryAggregateStateRepositoryFactory.java
                  RocksDBAggregateStateRepository.java
                  RocksDBAggregateStateRepositoryFactory.java
                AggregateServiceApplication.java
                AkcesAggregateController.java
                AkcesControllerState.java
        resources/
          protobuf/
            AggregateStateRecord.proto
            CommandRecord.proto
            DomainEventRecord.proto
          akces-aggregateservice.properties
      test/
        java/
          org/
            elasticsoftware/
              akces/
                beans/
                  AotServicesTest.java
                  MinInsyncReplicasTest.java
              akcestest/
                aggregate/
                  account/
                    Account.java
                    AccountCreatedEvent.java
                    AccountState.java
                    CreateAccountCommand.java
                  orders/
                    BuyOrderCreatedEvent.java
                    BuyOrderPlacedEvent.java
                    BuyOrderProcess.java
                    BuyOrderRejectedEvent.java
                    FxMarket.java
                    OrderProcess.java
                    OrderProcessManager.java
                    OrderProcessManagerState.java
                    PlaceBuyOrderCommand.java
                    UserOrderProcessesCreatedEvent.java
                  wallet/
                    AmountReservedEvent.java
                    BalanceAlreadyExistsErrorEvent.java
                    BalanceCreatedEvent.java
                    CreateBalanceCommand.java
                    CreateWalletCommand.java
                    CreditWalletCommand.java
                    ExternalAccountCreatedEvent.java
                    InsufficientFundsErrorEvent.java
                    InvalidAccountCreatedEvent.java
                    InvalidAmountErrorEvent.java
                    InvalidCurrencyErrorEvent.java
                    ReserveAmountCommand.java
                    Wallet.java
                    WalletCreatedEvent.java
                    WalletCreditedEvent.java
                    WalletState.java
                control/
                  AkcesAggregateControllerTests.java
                old/
                  BalanceCreatedEvent.java
                  BuyOrderPlacedEvent.java
                  CreateWalletCommand.java
                  WalletCreditedEvent.java
                protocol/
                  ProtocolTests.java
                schemas/
                  AccountCreatedEvent.java
                  AccountCreatedEventV2.java
                  AccountCreatedEventV3.java
                  AccountTypeV1.java
                  AccountTypeV2.java
                  CreditWalletCommand.java
                  JsonSchemaTests.java
                  NotCompatibleAccountCreatedEventV4.java
                state/
                  RocksDBAggregateStateRepositoryTests.java
                util/
                  HostUtilsTests.java
                AggregateServiceApplicationTests.java
                RuntimeConfiguration.java
                RuntimeTests.java
                TestUtils.java
                WalletConfiguration.java
                WalletTests.java
        resources/
          akces-client.properties
          logback-test.xml
          test-application.yaml
    pom.xml
  shared/
    src/
      main/
        java/
          org/
            elasticsoftware/
              akces/
                control/
                  AggregateServiceCommandType.java
                  AggregateServiceDomainEventType.java
                  AggregateServiceRecord.java
                  AkcesControlRecord.java
                  AkcesRegistry.java
                errors/
                  AggregateAlreadyExistsErrorEvent.java
                  CommandExecutionErrorEvent.java
                gdpr/
                  jackson/
                    AkcesGDPRModule.java
                    PIIDataDeserializerModifier.java
                    PIIDataJsonDeserializer.java
                    PIIDataJsonSerializer.java
                    PIIDataSerializerModifier.java
                  EncryptingGDPRContext.java
                  GDPRAnnotationUtils.java
                  GDPRContext.java
                  GDPRContextHolder.java
                  GDPRContextRepository.java
                  GDPRContextRepositoryException.java
                  GDPRContextRepositoryFactory.java
                  GDPRKeyUtils.java
                  InMemoryGDPRContextRepository.java
                  InMemoryGDPRContextRepositoryFactory.java
                  NoopGDPRContext.java
                  RocksDBGDPRContextRepository.java
                  RocksDBGDPRContextRepositoryFactory.java
                kafka/
                  CustomKafkaConsumerFactory.java
                  CustomKafkaProducerFactory.java
                  RecordAndMetadata.java
                protocol/
                  AggregateStateRecord.java
                  CommandRecord.java
                  CommandResponseRecord.java
                  DomainEventRecord.java
                  GDPRKeyRecord.java
                  PayloadEncoding.java
                  ProtocolRecord.java
                schemas/
                  IncompatibleSchemaException.java
                  InvalidSchemaVersionException.java
                  KafkaSchemaRegistry.java
                  PreviousSchemaVersionMissingException.java
                  SchemaException.java
                  SchemaNotBackwardsCompatibleException.java
                  SchemaNotFoundException.java
                  SchemaVersionNotFoundException.java
                serialization/
                  AkcesControlRecordSerde.java
                  BigDecimalSerializer.java
                  ProtocolRecordSerde.java
                util/
                  EnvironmentPropertiesPrinter.java
                  HostUtils.java
                  KafkaSender.java
                  KafkaUtils.java
      test/
        java/
          org/
            elasticsoftware/
              akces/
                gdpr/
                  jackson/
                    AkcesGDPRModuleTests.java
                  GDPRContextTests.java
                  RocksDBGDPRContextRepositoryTests.java
    pom.xml
  pom.xml
FRAMEWORK_OVERVIEW.md
README.md
RELEASE.md

================================================================
Files
================================================================

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/Aggregate.java
================
public interface Aggregate<S extends AggregateState> {
default String getName() {
return getClass().getSimpleName();
⋮----
Class<S> getStateClass();
⋮----
default CommandBus getCommandBus() {
return CommandBusHolder.getCommandBus(getClass());

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/AggregateState.java
================
public interface AggregateState {
⋮----
String getAggregateId();
⋮----
default String getIndexKey() {
return getAggregateId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/AggregateStateType.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/CommandHandlerFunction.java
================
public interface CommandHandlerFunction<S extends AggregateState, C extends Command, E extends DomainEvent> {
⋮----
Stream<E> apply(@NotNull C command, S state);
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override isCreate()");
⋮----
default CommandType<C> getCommandType() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getCommandType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getAggregate()");
⋮----
default List<DomainEventType<E>> getProducedDomainEventTypes() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getProducedDomainEventTypes()");
⋮----
default List<DomainEventType<E>> getErrorEventTypes() {
throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getErrorEventTypes()");
⋮----
default CommandBus getCommandBus() {
return CommandBusHolder.getCommandBus(getAggregate().getClass());

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/CommandType.java
================
) implements SchemaType {
⋮----
public String getSchemaPrefix() {
⋮----
public boolean relaxExternalValidation() {

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/DomainEventType.java
================
) implements SchemaType {
⋮----
public String getSchemaPrefix() {
⋮----
public boolean relaxExternalValidation() {

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/EventBridgeHandlerFunction.java
================
public interface EventBridgeHandlerFunction<S extends AggregateState,E extends DomainEvent> {
void apply(@NotNull E event, @NotNull CommandBus commandBus);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing EventBridgeHandlerFunction directly, you must override getEventType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing EventBridgeHandlerFunction directly, you must override getAggregate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/EventHandlerFunction.java
================
public interface EventHandlerFunction<S extends AggregateState, InputEvent extends DomainEvent, E extends DomainEvent> {
⋮----
Stream<E> apply(@NotNull InputEvent event, S state);
⋮----
default DomainEventType<InputEvent> getEventType() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getEventType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getAggregate()");
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override isCreate()");
⋮----
default List<DomainEventType<E>> getProducedDomainEventTypes() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getProducedDomainEventTypes()");
⋮----
default List<DomainEventType<E>> getErrorEventTypes() {
throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getErrorEventTypes()");
⋮----
default CommandBus getCommandBus() {
return CommandBusHolder.getCommandBus(getAggregate().getClass());

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/EventSourcingHandlerFunction.java
================
public interface EventSourcingHandlerFunction<S extends AggregateState, E extends DomainEvent> {
⋮----
S apply(@NotNull E event, S state);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override getEventType()");
⋮----
default Aggregate<S> getAggregate() {
throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override getAggregate()");
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override isCreate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/aggregate/SchemaType.java
================
public interface SchemaType {
String getSchemaPrefix();
⋮----
default String getSchemaName() {
return getSchemaPrefix() + typeName();
⋮----
boolean relaxExternalValidation();
⋮----
String typeName();
⋮----
int version();
⋮----
Class<?> typeClass();
⋮----
boolean external();

================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/AggregateIdentifier.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/AggregateInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/AggregateStateInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/CommandHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/CommandInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/DatabaseModelEventHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/DatabaseModelInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/DomainEventInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/EventBridgeHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/EventHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/EventSourcingHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/PIIData.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelEventHandler.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelStateInfo.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/commands/Command.java
================
public interface Command {
⋮----
String getAggregateId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/commands/CommandBus.java
================
public interface CommandBus {
void send(Command command);

================
File: main/api/src/main/java/org/elasticsoftware/akces/commands/CommandBusHolder.java
================
public class CommandBusHolder {
⋮----
public static CommandBus getCommandBus(Class<? extends Aggregate> aggregateClass) {
return commandBusThreadLocal.get();

================
File: main/api/src/main/java/org/elasticsoftware/akces/events/DomainEvent.java
================
public interface DomainEvent {
⋮----
String getAggregateId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/events/ErrorEvent.java
================
public interface ErrorEvent extends DomainEvent {

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/AkcesProcess.java
================
public interface AkcesProcess {
⋮----
String getProcessId();

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/ProcessManager.java
================
public interface ProcessManager<S extends AggregateState, P extends AkcesProcess> extends Aggregate<S> {

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/ProcessManagerState.java
================
public interface ProcessManagerState<P extends AkcesProcess> extends AggregateState {
P getAkcesProcess(String processId) throws UnknownAkcesProcessException;
⋮----
boolean hasAkcesProcess(String processId);

================
File: main/api/src/main/java/org/elasticsoftware/akces/processmanager/UnknownAkcesProcessException.java
================
public class UnknownAkcesProcessException extends AkcesException {
⋮----
public String getProcessId() {

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/DatabaseModel.java
================
public interface DatabaseModel {
Map<String,Long> getOffsets(Set<String> partitionIds);
⋮----
Object startTransaction();
⋮----
void commitTransaction(Object transactionMarker, Map<String,Long> offsets);

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/DatabaseModelEventHandlerFunction.java
================
public interface DatabaseModelEventHandlerFunction<E extends DomainEvent> {
void accept(@NotNull E event);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getEventType()");
⋮----
default DatabaseModel getDatabaseModel() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getDatabaseModel()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModel.java
================
public interface QueryModel<S extends QueryModelState> {
default String getName() {
return getClass().getSimpleName();
⋮----
Class<S> getStateClass();
⋮----
String getIndexName();

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModelEventHandlerFunction.java
================
public interface QueryModelEventHandlerFunction<S extends QueryModelState, E extends DomainEvent> {
⋮----
S apply(@NotNull E event, S state);
⋮----
default DomainEventType<E> getEventType() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getEventType()");
⋮----
default QueryModel<S> getQueryModel() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getQueryModel()");
⋮----
default boolean isCreate() {
throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override isCreate()");

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModelState.java
================
public interface QueryModelState {
⋮----
String getIndexKey();

================
File: main/api/src/main/java/org/elasticsoftware/akces/query/QueryModelStateType.java
================


================
File: main/api/src/main/java/org/elasticsoftware/akces/AkcesException.java
================
public abstract class AkcesException extends RuntimeException {
⋮----
public String getAggregateName() {
⋮----
public String getAggregateId() {

================
File: main/api/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.8.13-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-api</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: API</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testng</groupId>
            <artifactId>testng</artifactId>
            <scope>test</scope>
        </dependency>







    </dependencies>

    <build>
        <plugins>
        </plugins>
    </build>
</project>

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClient.java
================
public interface AkcesClient {
⋮----
default CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nonnull Command command) {
return send(tenantId, null, command);
⋮----
default CompletionStage<List<DomainEvent>> send(@Nonnull Command command) {
return send(command, null);
⋮----
default CompletionStage<List<DomainEvent>> send(@Nonnull Command command, @Nullable String correlationId) {
return send(DEFAULT_TENANT_ID, correlationId, command);
⋮----
CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command);
⋮----
default void sendAndForget(@Nonnull Command command) {
sendAndForget(DEFAULT_TENANT_ID, null, command);
⋮----
default void sendAndForget(@Nonnull Command command, @Nullable String correlationId) {
sendAndForget(DEFAULT_TENANT_ID, correlationId, command);
⋮----
default void sendAndForget(@Nonnull String tenantId, @Nonnull Command command) {
sendAndForget(tenantId, null, command);
⋮----
void sendAndForget(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientAutoConfiguration.java
================
public class AkcesClientAutoConfiguration {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public AkcesControlRecordSerde controlSerde(ObjectMapper objectMapper) {
return new AkcesControlRecordSerde(objectMapper);
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public KafkaAdmin createKafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
⋮----
public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesClientSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ProducerFactory<String, ProtocolRecord> producerFactory(KafkaProperties properties) {
return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), serde.serializer());
⋮----
public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
⋮----
public ConsumerFactory<String, ProtocolRecord> commandResponseConsumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public ClassPathScanningCandidateComponentProvider domainEventScanner(Environment environment) {
ClassPathScanningCandidateComponentProvider provider = new ClassPathScanningCandidateComponentProvider(false);
provider.addIncludeFilter(new AnnotationTypeFilter(DomainEventInfo.class));
provider.setEnvironment(environment);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();
⋮----
public AkcesClientController akcesClient(@Qualifier("akcesClientProducerFactory") ProducerFactory<String, ProtocolRecord> producerFactory,
⋮----
return new AkcesClientController(producerFactory,

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientCommandException.java
================
public abstract class AkcesClientCommandException extends RuntimeException {
⋮----
this.commandType = commandInfo != null ? commandInfo.type() : null;
this.commandVersion = commandInfo != null ? commandInfo.version() : null;
⋮----
public Class<? extends Command> getCommandClass() {
⋮----
public String getCommandType() {
⋮----
public Integer getCommandVersion() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientController.java
================
public class AkcesClientController extends Thread implements AutoCloseable, AkcesClient, ApplicationContextAware {
private static final Logger logger = LoggerFactory.getLogger(AkcesClientController.class);
private static final TopicPartition AKCES_CONTROL_PARTITION = new TopicPartition("Akces-Control", 0);
⋮----
private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
loadSupportedDomainEvents(basePackage);
⋮----
loadSupportedDomainEvents("org.elasticsoftware.akces.errors");
⋮----
public TopicPartition getCommandResponsePartition() {
⋮----
public void run() {
try (final Consumer<String, AkcesControlRecord> controlConsumer = controlRecordConsumerFactory.createConsumer(
HostUtils.getHostName() + "-AkcesClientController-Control",
⋮----
final Consumer<String, ProtocolRecord> commandResponseConsumer = commandResponseConsumerFactory.createConsumer(
HostUtils.getHostName() + "-AkcesClientController-CommandResponses",
⋮----
final Producer<String, ProtocolRecord> producer = producerFactory.createProducer(HostUtils.getHostName() + "-AkcesClientController")) {
⋮----
partitions = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control").partitions().size();
⋮----
controlConsumer.assign(singletonList(AKCES_CONTROL_PARTITION));
⋮----
controlConsumer.seekToBeginning(singletonList(AKCES_CONTROL_PARTITION));
⋮----
int commandResponsePartitions = kafkaAdmin.describeTopics("Akces-CommandResponses").get("Akces-CommandResponses").partitions().size();
commandResponsePartition = new TopicPartition("Akces-CommandResponses",
resolveCommandResponsePartition(HostUtils.getHostName(), commandResponsePartitions));
commandResponseConsumer.assign(singletonList(commandResponsePartition));
⋮----
commandResponseConsumer.seekToEnd(singletonList(commandResponsePartition));
⋮----
process(controlConsumer, commandResponseConsumer, producer);
⋮----
commandQueue.drainTo(pendingRequests);
⋮----
pendingRequest.completableFuture().completeExceptionally(new CommandRefusedException(pendingRequest.command().getClass(), SHUTTING_DOWN));
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
private void loadSupportedDomainEvents(String basePackage) {
for (BeanDefinition beanDefinition : domainEventScanner.findCandidateComponents(basePackage)) {
⋮----
Class<? extends DomainEvent> domainEventClass = (Class<? extends DomainEvent>) Class.forName(beanDefinition.getBeanClassName());
DomainEventInfo domainEventInfo = domainEventClass.getAnnotation(DomainEventInfo.class);
TreeMap<Integer, DomainEventType<? extends DomainEvent>> versionMap = domainEventClasses.computeIfAbsent(domainEventInfo.type(), k -> new TreeMap<>());
versionMap.put(domainEventInfo.version(), new DomainEventType<>(domainEventInfo.type(), domainEventInfo.version(), domainEventClass, false, true, ErrorEvent.class.isAssignableFrom(domainEventClass), hasPIIDataAnnotation(domainEventClass)));
⋮----
private void process(Consumer<String, AkcesControlRecord> controlConsumer,
⋮----
processControlRecords(controlConsumer.poll(Duration.ofMillis(10)));
⋮----
processCommands(producer);
⋮----
processCommandResponses(commandResponseConsumer.poll(Duration.ofMillis(10)));
⋮----
logger.error("Unrecoverable exception in AkcesController", e);
⋮----
logger.error("Exception while loading Command (JSON)Schemas from SchemaRegistry", e);
⋮----
Map<TopicPartition, Long> endOffsets = controlConsumer.endOffsets(singletonList(AKCES_CONTROL_PARTITION));
ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(10));
processControlRecords(consumerRecords);
⋮----
if (consumerRecords.isEmpty() && endOffsets.getOrDefault(AKCES_CONTROL_PARTITION, 0L) <= controlConsumer.position(AKCES_CONTROL_PARTITION)) {
⋮----
private void processControlRecords(ConsumerRecords<String, AkcesControlRecord> consumerRecords) throws RestClientException, IOException {
if (!consumerRecords.isEmpty()) {
⋮----
AkcesControlRecord controlRecord = record.value();
⋮----
if (!aggregateServices.containsKey(record.key())) {
logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
⋮----
aggregateServices.put(record.key(), aggregateServiceRecord);
⋮----
logger.warn("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
⋮----
private void processCommands(Producer<String, ProtocolRecord> producer) {
⋮----
while(!commandQueue.isEmpty()) {
⋮----
CommandRequest headRequest = commandQueue.peek();
⋮----
registerCommand(
headRequest.commandType(),
headRequest.commandVersion(),
headRequest.command().getClass());
⋮----
CommandRequest commandRequest = commandQueue.poll();
⋮----
commandRequest.commandType(),
commandRequest.commandVersion(),
commandRequest.command().getClass());
⋮----
String topic = resolveTopic(
⋮----
commandRequest.command());
⋮----
CommandRecord commandRecord = new CommandRecord(
commandRequest.tenantId(),
⋮----
serialize(commandRequest.command()),
⋮----
commandRequest.command().getAggregateId(),
commandRequest.correlationId() != null ? commandRequest.correlationId() : UUID.randomUUID().toString(), commandResponsePartition.toString());
⋮----
resolvePartition(commandRecord.aggregateId()),
commandRecord.aggregateId(),
⋮----
commandRecords.put(producerRecord, commandRequest);
⋮----
if (commandRequest.completeAfterValidation()) {
commandRequest.completableFuture().complete(Collections.emptyList());
⋮----
commandRequest.completableFuture().completeExceptionally(e);
⋮----
if(!commandRecords.isEmpty()) {
⋮----
producer.beginTransaction();
for (Map.Entry<ProducerRecord<String, ProtocolRecord>, CommandRequest> entry : commandRecords.entrySet()) {
final CompletableFuture<List<DomainEvent>> completableFuture = entry.getValue().completableFuture();
final CommandRecord commandRecord = (CommandRecord) entry.getKey().value();
final Class<? extends Command> commandClass = entry.getValue().command().getClass();
if (!completableFuture.isDone()) {
KafkaSender.send(producer, entry.getKey(), (metadata, exception) -> {
⋮----
completableFuture.completeExceptionally(new CommandSendingFailedException(commandClass, exception));
⋮----
pendingCommandResponseMap.put(commandRecord.id(), new PendingCommandResponse(commandRecord, completableFuture));
⋮----
KafkaSender.send(producer, entry.getKey());
⋮----
producer.commitTransaction();
⋮----
private void processCommandResponses(ConsumerRecords<String, ProtocolRecord> consumerRecords) {
⋮----
ProtocolRecord protocolRecord = record.value();
⋮----
PendingCommandResponse pendingCommandResponse = pendingCommandResponseMap.remove(commandResponseRecord.commandId());
⋮----
logger.trace("Received CommandResponseRecord for unknown commandId: {}", commandResponseRecord.commandId());
⋮----
for (DomainEventRecord domainEventRecord : commandResponseRecord.events()) {
domainEvents.add(deserialize(domainEventRecord, commandResponseRecord.encryptionKey()));
⋮----
pendingCommandResponse.completableFuture().complete(domainEvents);
⋮----
pendingCommandResponse.completableFuture().completeExceptionally(e);
⋮----
logger.warn("Received unknown ProtocolRecord type: {}", protocolRecord.getClass().getSimpleName());
⋮----
public CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command) {
⋮----
checkRunning(command);
CommandInfo commandInfo = command.getClass().getAnnotation(CommandInfo.class);
⋮----
throw new IllegalArgumentException("Command class " + command.getClass().getName() + " is not annotated with @CommandInfo");
⋮----
commandQueue.add(new CommandRequest(
⋮----
commandInfo.type(),
commandInfo.version(),
⋮----
public void sendAndForget(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command) {
⋮----
completableFuture.get();
⋮----
if(e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public void close() throws Exception {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesClientController has been shutdown");
⋮----
logger.warn("AkcesClientController did not shutdown within 10 seconds");
⋮----
private AggregateServiceCommandType resolveCommandType(String type, int version) {
return aggregateServices.values().stream()
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), type, version))
.findFirst().flatMap(commandServiceRecord -> commandServiceRecord.supportedCommands().stream()
.filter(commandType -> commandType.typeName().equals(type) && commandType.version() == version)
.findFirst()).orElse(null);
⋮----
private AggregateServiceRecord resolveAggregateService(AggregateServiceCommandType commandType) {
⋮----
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType.typeName(), commandType.version()))
.findFirst().orElse(null);
⋮----
private String resolveTopic(String commandType, int version, Command command) {
List<AggregateServiceRecord> services = aggregateServices.values().stream()
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType, version))
.toList();
if (services.size() == 1) {
return services.getFirst().commandTopic();
} else if (services.isEmpty()) {
throw new UnroutableCommandException(command.getClass());
⋮----
private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, String commandType, int version) {
⋮----
if (supportedCommand.typeName().equals(commandType) &&
supportedCommand.version() == version) {
⋮----
public Integer resolvePartition(@Nonnull String aggregateId) {
return Math.abs(hashFunction.hashString(aggregateId, UTF_8).asInt()) % partitions;
⋮----
private Integer resolveCommandResponsePartition(String hostname, int partitions) {
return Math.abs(hashFunction.hashString(hostname, UTF_8).asInt()) % partitions;
⋮----
private void registerCommand(String type, int version, Class<? extends Command> commandClass) {
⋮----
if (!commandTypes.containsKey(commandClass)) {
⋮----
AggregateServiceCommandType commandType = resolveCommandType(type, version);
⋮----
ParsedSchema schema = schemaRegistry.validate(commandType.toLocalCommandType(commandClass));
commandSchemas.computeIfAbsent(
commandType.typeName(),
k -> new ConcurrentHashMap<>()).put(commandType.version(), schema);
logger.trace("Stored schema: {} v{}", commandType.schemaName(), commandType.version());
⋮----
commandSchemasLookup.put(commandClass, schema);
⋮----
commandTypes.put(commandClass, commandType);
⋮----
for (AggregateServiceDomainEventType domainEventType : resolveAggregateService(commandType).producedEvents()) {
processDomainEvent(commandClass, domainEventType);
⋮----
throw new UnroutableCommandException(commandClass);
⋮----
private void processDomainEvent(Class<? extends Command> commandClass, AggregateServiceDomainEventType aggregateServiceDomainEventType) throws SchemaException {
⋮----
domainEventClasses.get(aggregateServiceDomainEventType.typeName()).floorEntry(aggregateServiceDomainEventType.version()).getValue();
⋮----
domainEventSchemas.computeIfAbsent(
domainEventType.typeName(),
k -> new ConcurrentHashMap<>()).put(
domainEventType.version(),
schemaRegistry.validate(domainEventType));
logger.trace("Stored schema for: {} v{}", domainEventType.getSchemaName(), domainEventType.version());
⋮----
throw new MissingDomainEventException(
⋮----
aggregateServiceDomainEventType.typeName(),
aggregateServiceDomainEventType.version());
⋮----
private byte[] serialize(Command command) {
⋮----
ParsedSchema schema = commandSchemasLookup.get(command.getClass());
⋮----
JsonNode jsonNode = objectMapper.valueToTree(command);
jsonSchema.validate(jsonNode);
return objectMapper.writeValueAsBytes(jsonNode);
⋮----
return objectMapper.writeValueAsBytes(command);
⋮----
throw new CommandSerializationException(command.getClass(), e);
⋮----
throw new CommandValidationException(command.getClass(), e);
⋮----
private DomainEvent deserialize(DomainEventRecord der, @Nullable byte[] encryptionKey) throws IOException {
⋮----
setCurrentGDPRContext(encryptionKey != null ? new EncryptingGDPRContext(der.aggregateId(), encryptionKey, GDPRKeyUtils.isUUID(der.aggregateId())) : null);
⋮----
DomainEventType<? extends DomainEvent> domainEventType = domainEventClasses.get(der.name()).floorEntry(der.version()).getValue();
⋮----
return objectMapper.readValue(der.payload(), domainEventType.typeClass());
⋮----
resetCurrentGDPRContext();
⋮----
private void checkRunning(Command command) {
⋮----
throw new CommandRefusedException(command.getClass(), processState);
⋮----
public boolean isRunning() {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/AkcesClientControllerState.java
================


================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandRefusedException.java
================
public class CommandRefusedException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Command Refused because AkcesClient is not in RUNNING state");
⋮----
public AkcesClientControllerState getState() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandSendingFailedException.java
================
public class CommandSendingFailedException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Sending", cause);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandSerializationException.java
================
public class CommandSerializationException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Serializing", cause);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandServiceApplication.java
================
public class CommandServiceApplication {
public static void main(String[] args) {
SpringApplication application = new SpringApplication(CommandServiceApplication.class);
⋮----
application.setSources(Set.of(args));
⋮----
application.run();

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/CommandValidationException.java
================
public class CommandValidationException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Validating", cause);

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/MissingDomainEventException.java
================
public class MissingDomainEventException extends AkcesClientCommandException {
⋮----
commandClass.getAnnotation(CommandInfo.class),
⋮----
public String getSchemaName() {
⋮----
public int getVersion() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/UnknownSchemaException.java
================
public class UnknownSchemaException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Unknown Schema " + schemaIdentifier);
⋮----
public String getSchemaIdentifier() {

================
File: main/client/src/main/java/org/elasticsoftware/akces/client/UnroutableCommandException.java
================
public class UnroutableCommandException extends AkcesClientCommandException {
⋮----
super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Unable to Route Command, no AggregateService found that handles the Command");

================
File: main/client/src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#

org.elasticsoftware.akces.client.AkcesClientAutoConfiguration

================
File: main/client/src/main/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/commands/CreateAccountCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/commands/InvalidCommand.java
================
public record InvalidCommand(String id) implements Command {
⋮----
public @NotNull String getAggregateId() {
return id();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/commands/UnroutableCommand.java
================
public record UnroutableCommand(String id) implements Command {
⋮----
public @NotNull String getAggregateId() {
return id();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/events/AccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/AkcesClientTestConfiguration.java
================
public class AkcesClientTestConfiguration {

================
File: main/client/src/test/java/org/elasticsoftware/akces/client/AkcesClientTests.java
================
public class AkcesClientTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static <C extends Command> void prepareCommandSchemas(String url, List<Class<C>> commandClasses) {
SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
CommandInfo info = commandClass.getAnnotation(CommandInfo.class);
src.register("commands." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(commandClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<D>> domainEventClasses) {
⋮----
DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
⋮----
public static void prepareExternalServices(String bootstrapServers) {
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(new ObjectMapper());
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
⋮----
List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
List.of());
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", aggregateServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
public void testUnroutableCommandWithSendAndForget() {
Assertions.assertNotNull(akcesClient);
⋮----
while (!akcesClient.isRunning()) {
Thread.onSpinWait();
⋮----
Assertions.assertThrows(UnroutableCommandException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new UnroutableCommand(UUID.randomUUID().toString())));
⋮----
public void testUnroutableCommandWithSend() {
⋮----
CompletionStage<List<DomainEvent>> result =  akcesClient.send("TEST_TENANT", new UnroutableCommand(UUID.randomUUID().toString()));
⋮----
ExecutionException executionException = Assertions.assertThrows(ExecutionException.class, () -> result.toCompletableFuture().get());
Assertions.assertInstanceOf(UnroutableCommandException.class, executionException.getCause());
⋮----
public void testInvalidCommandWithSendAndForget() {
⋮----
Assertions.assertThrows(IllegalArgumentException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new InvalidCommand(UUID.randomUUID().toString())));
⋮----
public void testInvalidCommandWithSend() {
⋮----
Assertions.assertThrows(IllegalArgumentException.class, () -> akcesClient.send("TEST_TENANT", new InvalidCommand(UUID.randomUUID().toString())));
⋮----
public void testValidationErrorWithSendAndForget() {
⋮----
Assertions.assertThrows(CommandValidationException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new CreateAccountCommand(UUID.randomUUID().toString(), "NL", "Aike", "Christianen", null)));
⋮----
public void testValidationErrorWithSend() {
⋮----
CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT", new CreateAccountCommand(UUID.randomUUID().toString(), "NL", "Aike", "Christianen", null));
⋮----
Assertions.assertInstanceOf(CommandValidationException.class, executionException.getCause());
⋮----
public void testSendCommand() throws InterruptedException, JsonProcessingException {
⋮----
CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT",
new CreateAccountCommand(userId,
⋮----
Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
⋮----
TopicPartition commandResponsesPartition = akcesClient.getCommandResponsePartition();
TopicPartition commandPartition = new TopicPartition("Account-Commands", akcesClient.resolvePartition(userId));
testConsumer.assign(List.of(commandPartition));
testConsumer.seekToBeginning(List.of(commandPartition));
ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
⋮----
while (allRecords.isEmpty()) {
records.forEach(record -> allRecords.add(record.value()));
⋮----
records = testConsumer.poll(Duration.ofMillis(250));
⋮----
Assertions.assertEquals(1, allRecords.size());
CommandRecord cr = (CommandRecord) allRecords.getFirst();
⋮----
testProducer.beginTransaction();
List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", "Aike", "Christianen", "aike.christianen@gmail.com")), PayloadEncoding.JSON, userId, null, 0));
CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, null);
⋮----
testProducer.send(new ProducerRecord<>(commandResponsesPartition.topic(), commandResponsesPartition.partition(), crr.commandId(), crr));
testProducer.commitTransaction();
⋮----
CountDownLatch waitLatch = new CountDownLatch(1);
result.whenComplete((s, throwable) -> waitLatch.countDown());
Assertions.assertTrue(waitLatch.await(10, TimeUnit.SECONDS));
Assertions.assertTrue(result.toCompletableFuture().isDone());
Assertions.assertFalse(result.toCompletableFuture().isCompletedExceptionally());
Assertions.assertNotNull(result.toCompletableFuture().getNow(null));
Assertions.assertEquals(1, result.toCompletableFuture().getNow(null).size());
AccountCreatedEvent ace = (AccountCreatedEvent) result.toCompletableFuture().getNow(null).getFirst();
Assertions.assertEquals(userId, ace.userId());
Assertions.assertEquals("NL", ace.country());
Assertions.assertEquals("Aike", ace.firstName());
Assertions.assertEquals("Christianen", ace.lastName());
Assertions.assertEquals("aike.christianen@gmail.com", ace.email());
⋮----
public void testSendCommandWithCorrelationId() throws InterruptedException, JsonProcessingException {
⋮----
String correlationId = UUID.randomUUID().toString();
⋮----
new CreateAccountCommand(
⋮----
Assertions.assertEquals(correlationId, cr.correlationId());
⋮----
public void testGDPRDecryption() throws InterruptedException, JsonProcessingException {
⋮----
SecretKeySpec secretKeySpec = GDPRKeyUtils.createKey();
EncryptingGDPRContext gdprContext = new EncryptingGDPRContext(userId, secretKeySpec.getEncoded(), GDPRKeyUtils.isUUID(userId));
String encryptedFirstName = gdprContext.encrypt("Aike");
String encryptedLastName = gdprContext.encrypt("Christianen");
String encryptedEmail = gdprContext.encrypt("aike.christianen@gmail.com");
⋮----
List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", encryptedFirstName, encryptedLastName, encryptedEmail)), PayloadEncoding.JSON, userId, null, 0));
CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, secretKeySpec.getEncoded());
⋮----
public static class ContextInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
prepareCommandSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), List.of(CreateAccountCommand.class));
prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), List.of(AccountCreatedEvent.class));
prepareExternalServices(kafka.getBootstrapServers());
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)

================
File: main/client/src/test/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
akces.client.domainEventsPackage=org.elasticsoftware.akces.client.events
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: main/client/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.client" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: main/client/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.8.13-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Client</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-client</artifactId>
    <packaging>jar</packaging>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
    </dependencies>

</project>

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/beans/DatabaseModelBeanFactoryPostProcessor.java
================
public class DatabaseModelBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
⋮----
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
⋮----
Arrays.asList(beanFactory.getBeanNamesForAnnotation(DatabaseModelInfo.class)).forEach(beanName -> {
BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
⋮----
Class<?> queryModelClass = Class.forName(bd.getBeanClassName());
List<Method> queryModelEventHandlers = Arrays.stream(queryModelClass.getMethods())
.filter(method -> method.isAnnotationPresent(DatabaseModelEventHandler.class))
.toList();
queryModelEventHandlers.forEach(eventHandlerMethod -> processDatabaseModelEventHandler(beanName, eventHandlerMethod, bdr));
⋮----
throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
⋮----
bdr.registerBeanDefinition(beanName + "DatabaseModelRuntime",
BeanDefinitionBuilder.genericBeanDefinition(DatabaseModelRuntimeFactory.class)
.addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
.addConstructorArgReference("akcesDatabaseModelSchemaRegistry")
.addConstructorArgReference(beanName)
.getBeanDefinition());
bdr.registerBeanDefinition(beanName + "AkcesDatabaseModelController",
BeanDefinitionBuilder.genericBeanDefinition(AkcesDatabaseModelController.class)
.addConstructorArgReference("akcesDatabaseModelConsumerFactory")
.addConstructorArgReference("akcesDatabaseModelControlConsumerFactory")
.addConstructorArgReference("akcesDatabaseModelGDPRContextRepositoryFactory")
.addConstructorArgReference(beanName + "DatabaseModelRuntime")
.setInitMethodName("start")
.setDestroyMethodName("close")
⋮----
throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
⋮----
private void processDatabaseModelEventHandler(String aggregateBeanName, Method databaseModelEventHandlerMethod, BeanDefinitionRegistry bdr) {
if (databaseModelEventHandlerMethod.getParameterCount() == 1 &&
DomainEvent.class.isAssignableFrom(databaseModelEventHandlerMethod.getParameterTypes()[0]) &&
void.class.equals(databaseModelEventHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = databaseModelEventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_dmeh_" + databaseModelEventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
bdr.registerBeanDefinition(beanName,
BeanDefinitionBuilder.genericBeanDefinition(DatabaseModelEventHandlerFunctionAdapter.class)
.addConstructorArgReference(aggregateBeanName)
.addConstructorArgValue(databaseModelEventHandlerMethod.getName())
.addConstructorArgValue(databaseModelEventHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(eventInfo.type())
.addConstructorArgValue(eventInfo.version())
.setInitMethodName("init")
⋮----
throw new ApplicationContextException("Invalid DatabaseModelEventHandler method signature: " + databaseModelEventHandlerMethod);
⋮----
public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
⋮----
public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/beans/DatabaseModelEventHandlerFunctionAdapter.java
================
public class DatabaseModelEventHandlerFunctionAdapter<E extends DomainEvent>
⋮----
GDPRAnnotationUtils.hasPIIDataAnnotation(domainEventClass));
⋮----
public void init() {
⋮----
adapterMethod = databaseModel.getClass().getMethod(adapterMethodName, domainEventClass);
⋮----
throw new RuntimeException(e);
⋮----
public void accept(@NotNull E event) {
⋮----
adapterMethod.invoke(databaseModel, event);
⋮----
if (e.getCause() != null) {
if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public DomainEventType<E> getEventType() {
⋮----
public DatabaseModel getDatabaseModel() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jdbc/JdbcDatabaseModel.java
================
public class JdbcDatabaseModel implements DatabaseModel {
⋮----
private final DefaultTransactionDefinition transactionDefinition = new DefaultTransactionDefinition(TransactionDefinition.PROPAGATION_REQUIRED);
⋮----
this.transactionDefinition.setIsolationLevel(TransactionDefinition.ISOLATION_READ_COMMITTED);
this.transactionTemplate = new TransactionTemplate(transactionManager, transactionDefinition);
⋮----
public Map<String, Long> getOffsets(Set<String> partitionIds) {
return transactionTemplate.execute(status -> {
⋮----
.formatted(String.join(",", Collections.nCopies(partitionIds.size(), "?")));
⋮----
return jdbcTemplate.query(
⋮----
ps.setString(idx++, partitionId);
⋮----
(rs, rowNum) -> Map.entry(
rs.getString("partition_id"),
rs.getLong("record_offset")
⋮----
).stream().collect(Collectors.toMap(
⋮----
public Object startTransaction() {
return transactionManager.getTransaction(transactionDefinition);
⋮----
public void commitTransaction(Object transactionMarker, Map<String, Long> offsets) {
⋮----
throw new IllegalArgumentException("Invalid transaction marker");
⋮----
detectDatabaseType();
⋮----
String sql = getUpsertSql("partition_offsets", "partition_id", "record_offset");
⋮----
jdbcTemplate.batchUpdate(
⋮----
offsets.entrySet().stream()
.map(entry -> new Object[]{entry.getKey(), entry.getValue()})
.collect(Collectors.toList())
⋮----
transactionManager.commit(status);
⋮----
transactionManager.rollback(status);
throw new RuntimeException("Failed to commit offsets", e);
⋮----
private void detectDatabaseType() throws SQLException {
if(databaseType == null && jdbcTemplate.getDataSource() != null) {
try (Connection connection = jdbcTemplate.getDataSource().getConnection()) {
DatabaseMetaData metadata = connection.getMetaData();
databaseType = metadata.getDatabaseProductName().toLowerCase();
⋮----
private String getUpsertSql(String tableName, String keyColumn, String valueColumn) {
⋮----
""".formatted(tableName, keyColumn, valueColumn, keyColumn, valueColumn, valueColumn);
⋮----
""".formatted(tableName, keyColumn, valueColumn, valueColumn, valueColumn);
⋮----
""".formatted(tableName, keyColumn, valueColumn,
⋮----
default -> throw new UnsupportedOperationException("Unsupported database: " + databaseType);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jpa/JpaDatabaseModel.java
================
public class JpaDatabaseModel implements DatabaseModel {
⋮----
private final DefaultTransactionDefinition transactionDefinition = new DefaultTransactionDefinition(PROPAGATION_REQUIRED);
⋮----
transactionDefinition.setIsolationLevel(ISOLATION_READ_COMMITTED);
⋮----
public Map<String, Long> getOffsets(Set<String> partitionIds) {
return repository.findByPartitionIdIn(partitionIds).stream()
.collect(Collectors.toMap(
⋮----
public Object startTransaction() {
return transactionManager.getTransaction(transactionDefinition);
⋮----
public void commitTransaction(Object transactionMarker, Map<String, Long> offsets) {
⋮----
throw new IllegalArgumentException("Invalid transaction marker");
⋮----
repository.saveAll(
offsets.entrySet().stream()
.map(e -> new PartitionOffset(e.getKey(), e.getValue()))
.collect(Collectors.toList())
⋮----
transactionManager.commit(status);
⋮----
transactionManager.rollback(status);
throw new RuntimeException("Failed to commit offsets", e);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jpa/PartitionOffset.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/jpa/PartitionOffsetRepository.java
================
public interface PartitionOffsetRepository extends JpaRepository<PartitionOffset, String> {
List<PartitionOffset> findByPartitionIdIn(Collection<String> partitionIds);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/AkcesDatabaseModelAutoConfiguration.java
================
public class AkcesDatabaseModelAutoConfiguration {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public static DatabaseModelBeanFactoryPostProcessor databaseModelBeanFactoryPostProcessor() {
return new DatabaseModelBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesDatabaseModelSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public AkcesControlRecordSerde akcesControlRecordSerde(ObjectMapper objectMapper) {
return new AkcesControlRecordSerde(objectMapper);
⋮----
public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
⋮----
public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/AkcesDatabaseModelController.java
================
public class AkcesDatabaseModelController extends Thread implements AutoCloseable, ConsumerRebalanceListener, ApplicationContextAware, AkcesRegistry {
private static final Logger logger = LoggerFactory.getLogger(AkcesDatabaseModelController.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
super(databaseModelRuntime.getName() + "-AkcesDatabaseModelController");
⋮----
this.executorService = Executors.newCachedThreadPool(new CustomizableThreadFactory(databaseModelRuntime.getName() + "DatabaseModelPartitionThread-"));
⋮----
public void run() {
⋮----
controlRecordConsumerFactory.createConsumer(
databaseModelRuntime.getName() + "DatabaseModel-Akces-Control",
databaseModelRuntime.getName() + "-" + HostUtils.getHostName() + "DatabaseModel-Akces-Control",
⋮----
controlConsumer.subscribe(List.of("Akces-Control"), this);
⋮----
process();
⋮----
logger.info("Closing {} DatabaseModelPartitions", databaseModelPartitions.size());
databaseModelPartitions.values().forEach(databaseModelPartition -> {
⋮----
databaseModelPartition.close();
⋮----
logger.error("Error closing DatabaseModelPartition " + databaseModelPartition.getId(), e);
⋮----
controlConsumer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing controlConsumer", e);
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
logger.error("Error in AkcesDatabaseModelController", e);
⋮----
private void process() {
⋮----
processControlRecords();
⋮----
logger.error("Unrecoverable exception in AkcesDatabaseModelController", e);
⋮----
databaseModelRuntime.validateDomainEventSchemas();
⋮----
if (!partitionsToAssign.isEmpty()) {
⋮----
controlConsumer.seekToBeginning(partitionsToAssign);
⋮----
Map<TopicPartition, Long> initializedEndOffsets = controlConsumer.endOffsets(partitionsToAssign);
⋮----
ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
while (!initializedEndOffsets.isEmpty()) {
consumerRecords.forEach(record -> {
AkcesControlRecord controlRecord = record.value();
⋮----
if (aggregateServices.putIfAbsent(record.key(), aggregateServiceRecord) == null) {
logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
⋮----
logger.info("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
⋮----
if (consumerRecords.isEmpty()) {
initializedEndOffsets.entrySet().removeIf(entry -> entry.getValue() <= controlConsumer.position(entry.getKey()));
⋮----
consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
⋮----
DatabaseModelPartition databaseModelPartition = databaseModelPartitions.remove(topicPartition.partition());
⋮----
logger.info("Stopping DatabaseModelPartition {}", databaseModelPartition.getId());
⋮----
logger.error("Error closing DatabaseModelPartition", e);
⋮----
partitionsToRevoke.clear();
⋮----
DatabaseModelPartition aggregatePartition = new DatabaseModelPartition(
⋮----
topicPartition.partition(),
new TopicPartition("Akces-GDPRKeys", topicPartition.partition()),
databaseModelRuntime.getDomainEventTypes(),
⋮----
databaseModelPartitions.put(aggregatePartition.getId(), aggregatePartition);
logger.info("Starting DatabaseModelPartition {}", aggregatePartition.getId());
executorService.submit(aggregatePartition);
⋮----
partitionsToAssign.clear();
⋮----
private void processControlRecords() {
⋮----
if (!consumerRecords.isEmpty()) {
⋮----
if (!aggregateServices.containsKey(record.key())) {
⋮----
aggregateServices.put(record.key(), aggregateServiceRecord);
⋮----
public void close() throws Exception {
logger.info("Shutting down AkcesDatabaseModelController");
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesDatabaseModelController has been shutdown");
⋮----
logger.warn("AkcesDatabaseModelController did not shutdown within 10 seconds");
⋮----
public void onPartitionsRevoked(Collection<TopicPartition> topicPartitions) {
⋮----
if (!topicPartitions.isEmpty()) {
⋮----
partitionsToRevoke.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, revoking partitions: {}",
topicPartitions.stream().map(TopicPartition::partition).toList());
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, revoking partitions: {}",
⋮----
public void onPartitionsAssigned(Collection<TopicPartition> topicPartitions) {
⋮----
partitionsToAssign.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, assigning partitions : {}",
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, assigning partitions : {}",
⋮----
private boolean producesDomainEvent(List<AggregateServiceDomainEventType> producedEvents, DomainEventType<?> externalDomainEventType) {
⋮----
if (producedEvent.typeName().equals(externalDomainEventType.typeName()) &&
producedEvent.version() == externalDomainEventType.version()) {
⋮----
public String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType) {
List<AggregateServiceRecord> services = aggregateServices.values().stream()
.filter(commandServiceRecord -> producesDomainEvent(commandServiceRecord.producedEvents(), externalDomainEventType))
.toList();
if (services.size() == 1) {
return services.getFirst().domainEventTopic();
⋮----
throw new IllegalStateException("Cannot determine which service produces DomainEvent " + externalDomainEventType.typeName() + " v" + externalDomainEventType.version());
⋮----
public CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass) {
throw new UnsupportedOperationException();
⋮----
public String resolveTopic(@Nonnull Class<? extends Command> commandClass) {
⋮----
public String resolveTopic(@Nonnull CommandType<?> commandType) {
⋮----
public Integer resolvePartition(@Nonnull String aggregateId) {
⋮----
public boolean isRunning() {
return processState == RUNNING && databaseModelPartitions.values().stream().allMatch(DatabaseModelPartition::isProcessing);
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/AkcesDatabaseModelControllerState.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelImplementationPresentCondition.java
================
public class DatabaseModelImplementationPresentCondition extends SpringBootCondition {
⋮----
public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) {
boolean match = Optional.ofNullable(context.getBeanFactory())
.map(beanFactory -> beanFactory.getBeanNamesForAnnotation(DatabaseModelInfo.class))
.filter(beanNames -> beanNames.length > 0).isPresent();
return match ? ConditionOutcome.match() : ConditionOutcome.noMatch("No DatabaseModel beans found");

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelPartition.java
================
public class DatabaseModelPartition implements Runnable, AutoCloseable {
private static final Logger logger = LoggerFactory.getLogger(DatabaseModelPartition.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
⋮----
this.gdprContextRepository = gdprContextRepositoryFactory.create(runtime.getName(), id);
⋮----
public Integer getId() {
⋮----
public void run() {
⋮----
logger.info("Starting DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
this.consumer = consumerFactory.createConsumer(
runtime.getName() + "DatabaseModel-partition-" + id,
runtime.getName() + "DatabaseModel-partition-" + id + "-" + HostUtils.getHostName(),
⋮----
Set<String> distinctTopics = externalDomainEventTypes.stream()
.map(akcesRegistry::resolveTopic)
.collect(Collectors.toSet());
externalEventPartitions.addAll(distinctTopics.stream()
.map(topic -> new TopicPartition(topic, id))
.collect(Collectors.toSet()));
⋮----
consumer.assign(Stream.concat(
runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty(),
externalEventPartitions.stream()).toList());
logger.info("Assigned partitions {} for DatabaseModelPartition {} of {}DatabaseModel", consumer.assignment(), id, runtime.getName());
⋮----
process();
⋮----
logger.info("Shutting down DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
⋮----
logger.error("Unexpected error in DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName(), t);
⋮----
consumer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing consumer/producer", e);
⋮----
gdprContextRepository.close();
⋮----
logger.error("Error closing gdpr context repository", e);
⋮----
logger.info("Finished Shutting down DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
shutdownLatch.countDown();
⋮----
public void close() {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("DatabaseModelPartition={} has been shutdown", id);
⋮----
logger.warn("DatabaseModelPartition={} did not shutdown within 10 seconds", id);
⋮----
private void setupGDPRContext(String tenantId, String aggregateId) {
⋮----
GDPRContextHolder.setCurrentGDPRContext(gdprContextRepository.get(aggregateId));
⋮----
private void tearDownGDPRContext() {
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
private void process() {
⋮----
ConsumerRecords<String, ProtocolRecord> allRecords = consumer.poll(Duration.ofMillis(10));
if (!allRecords.isEmpty()) {
processRecords(allRecords);
⋮----
ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = consumer.poll(Duration.ofMillis(10));
gdprContextRepository.process(gdprKeyRecords.records(gdprKeyPartition));
⋮----
if (gdprKeyRecords.isEmpty() && initializedEndOffsets.getOrDefault(gdprKeyPartition, 0L) <= consumer.position(gdprKeyPartition)) {
⋮----
consumer.resume(externalEventPartitions);
⋮----
logger.info(
⋮----
runtime.getName(),
runtime.shouldHandlePIIData() ? "Handle PII Data" : "Not Handle PII Data");
⋮----
runtime.initializeOffsets(externalEventPartitions).forEach((topicPartition, offset)
-> consumer.seek(topicPartition, offset + 1));
⋮----
if(runtime.shouldHandlePIIData()) {
⋮----
long gdprKeyRepositoryOffset = gdprContextRepository.getOffset();
⋮----
runtime.getName());
consumer.seek(gdprKeyPartition, gdprContextRepository.getOffset() + 1);
⋮----
consumer.seekToBeginning(singletonList(gdprKeyPartition));
⋮----
initializedEndOffsets = consumer.endOffsets(List.of(gdprKeyPartition));
logger.info("Loading GDPR Keys for DatabaseModelPartition {} of {}DatabaseModel", id, runtime.getName());
⋮----
consumer.pause(externalEventPartitions);
⋮----
logger.error("Fatal error during " + processState + " phase, shutting down DatabaseModelPartition " + id + " of " + runtime.getName() + "DatabaseModel", e);
⋮----
private void processRecords(ConsumerRecords<String, ProtocolRecord> allRecords) {
if (logger.isTraceEnabled()) {
logger.trace("Processing {} records in a single batch", allRecords.count());
logger.trace("Processing {} gdpr key records", allRecords.records(gdprKeyPartition).size());
if (!externalEventPartitions.isEmpty()) {
logger.trace("Processing {} external event records", externalEventPartitions.stream()
.map(externalEventPartition -> allRecords.records(externalEventPartition).size())
.mapToInt(Integer::intValue).sum());
⋮----
List<ConsumerRecord<String, ProtocolRecord>> gdprKeyRecords = allRecords.records(gdprKeyPartition);
if (!gdprKeyRecords.isEmpty()) {
gdprContextRepository.process(gdprKeyRecords);
⋮----
runtime.apply(allRecords.partitions().stream()
.filter(topicPartition -> !topicPartition.equals(gdprKeyPartition))
.collect(Collectors.toMap(
⋮----
logger.error("IOException while processing events", e);
⋮----
public boolean isProcessing() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelPartitionState.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelRuntime.java
================
public interface DatabaseModelRuntime {
String getName();
⋮----
void apply(Map<TopicPartition, List<ConsumerRecord<String, ProtocolRecord>>> records,
⋮----
void validateDomainEventSchemas();
⋮----
boolean shouldHandlePIIData();
⋮----
Collection<DomainEventType<?>> getDomainEventTypes();
⋮----
Map<TopicPartition,Long> initializeOffsets(Collection<TopicPartition> topicPartitions);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/DatabaseModelRuntimeFactory.java
================
public class DatabaseModelRuntimeFactory implements FactoryBean<DatabaseModelRuntime>, ApplicationContextAware {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
public DatabaseModelRuntime getObject() throws Exception {
return createRuntime(databaseModel);
⋮----
public Class<?> getObjectType() {
⋮----
private DatabaseModelRuntime createRuntime(DatabaseModel databaseModel) {
⋮----
DatabaseModelInfo databaseModelInfo = databaseModel.getClass().getAnnotation(DatabaseModelInfo.class);
⋮----
throw new IllegalStateException("Class implementing DatabaseModel must be annotated with @DatabaseModelInfo");
⋮----
.setDatabaseModelInfo(databaseModelInfo)
.setDatabaseModel(databaseModel)
.setObjectMapper(objectMapper)
.setDatabaseModel(databaseModel);
⋮----
applicationContext.getBeansOfType(DatabaseModelEventHandlerFunction.class).values().stream()
.filter(adapter -> adapter.getDatabaseModel().equals(databaseModel))
.forEach(adapter -> {
DomainEventType<?> type = adapter.getEventType();
runtimeBuilder.addDatabaseModelEventHandler(type, adapter);
runtimeBuilder.addDomainEvent(type);
⋮----
return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/database/KafkaDatabaseModelRuntime.java
================
public class KafkaDatabaseModelRuntime implements DatabaseModelRuntime {
private static final Logger logger = LoggerFactory.getLogger(KafkaDatabaseModelRuntime.class);
⋮----
public String getName() {
⋮----
public Map<TopicPartition, Long> initializeOffsets(Collection<TopicPartition> topicPartitions) {
Set<String> partitionIds = topicPartitions.stream()
.map(TopicPartition::toString)
.collect(Collectors.toSet());
⋮----
Map<String, Long> offsets = databaseModel.getOffsets(partitionIds);
⋮----
return topicPartitions.stream()
.collect(Collectors.toMap(
⋮----
tp -> offsets.getOrDefault(tp.toString(), 0L)
⋮----
public void apply(Map<TopicPartition, List<ConsumerRecord<String, ProtocolRecord>>> records,
⋮----
final Object transactionMarker = databaseModel.startTransaction();
⋮----
for (TopicPartition topicPartition : records.keySet()) {
for (DomainEventRecord eventRecord : records.get(topicPartition).stream().map(consumerRecord -> (DomainEventRecord) consumerRecord.value()).toList()) {
⋮----
DomainEventType<?> domainEventType = getDomainEventType(eventRecord);
⋮----
DatabaseModelEventHandlerFunction<DomainEvent> eventHandler = databaseModelEventHandlers.get(domainEventType);
⋮----
if (Boolean.TRUE.equals(domainEventType.piiData())) {
GDPRContextHolder.setCurrentGDPRContext(gdprContextSupplier.apply(eventRecord.aggregateId()));
⋮----
eventHandler.accept(materialize(domainEventType, eventRecord));
⋮----
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
databaseModel.commitTransaction(transactionMarker, records.entrySet().stream()
⋮----
entry -> entry.getKey().toString(),
entry -> entry.getValue().stream()
.mapToLong(ConsumerRecord::offset)
.max()
.orElse(0L)
⋮----
public void validateDomainEventSchemas() {
for (DomainEventType<?> domainEventType : domainEvents.values()) {
schemaRegistry.validate(domainEventType);
⋮----
public Collection<DomainEventType<?>> getDomainEventTypes() {
return domainEvents.values();
⋮----
public boolean shouldHandlePIIData() {
⋮----
private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
⋮----
private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {
⋮----
return domainEvents.entrySet().stream()
.filter(entry -> entry.getValue().external())
.filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
.filter(entry -> entry.getValue().version() <= eventRecord.version())
.max(Comparator.comparingInt(entry -> entry.getValue().version()))
.map(Map.Entry::getValue).orElse(null);
⋮----
public static class Builder {
⋮----
public Builder setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
⋮----
public Builder setObjectMapper(ObjectMapper objectMapper) {
⋮----
public Builder setDatabaseModelInfo(DatabaseModelInfo databaseModelInfo) {
⋮----
public Builder setDatabaseModel(DatabaseModel databaseModel) {
⋮----
public Builder addDomainEvent(DomainEventType<?> domainEvent) {
this.domainEvents.put(domainEvent.typeClass(), domainEvent);
⋮----
public Builder addDatabaseModelEventHandler(DomainEventType<?> eventType,
⋮----
this.databaseModelEventHandlers.put(eventType, databaseModelEventHandler);
⋮----
public KafkaDatabaseModelRuntime build() {
final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
return new KafkaDatabaseModelRuntime(
⋮----
databaseModelInfo.value(),
databaseModelInfo.version(),
⋮----
public String toString() {
return "KafkaDatabaseModelRuntime{"+getName()+"}";

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/beans/QueryModelBeanFactoryPostProcessor.java
================
public class QueryModelBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
⋮----
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
⋮----
Arrays.asList(beanFactory.getBeanNamesForAnnotation(QueryModelInfo.class)).forEach(beanName -> {
BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
⋮----
Class<?> queryModelClass = Class.forName(bd.getBeanClassName());
List<Method> queryModelEventHandlers = Arrays.stream(queryModelClass.getMethods())
.filter(method -> method.isAnnotationPresent(QueryModelEventHandler.class))
.toList();
queryModelEventHandlers.forEach(eventHandlerMethod -> processQueryModelEventHandler(beanName, eventHandlerMethod, bdr));
⋮----
throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
⋮----
bdr.registerBeanDefinition(beanName + "QueryModelRuntime",
BeanDefinitionBuilder.genericBeanDefinition(QueryModelRuntimeFactory.class)
.addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
.addConstructorArgReference("akcesQueryModelSchemaRegistry")
.addConstructorArgReference(beanName)
.getBeanDefinition());
⋮----
throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
⋮----
private void processQueryModelEventHandler(String aggregateBeanName, Method queryModelEventHandlerMethod, BeanDefinitionRegistry bdr) {
QueryModelEventHandler queryModelEventHandler = queryModelEventHandlerMethod.getAnnotation(QueryModelEventHandler.class);
if (queryModelEventHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(queryModelEventHandlerMethod.getParameterTypes()[0]) &&
QueryModelState.class.isAssignableFrom(queryModelEventHandlerMethod.getParameterTypes()[1]) &&
QueryModelState.class.isAssignableFrom(queryModelEventHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = queryModelEventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_qmeh_" + queryModelEventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
bdr.registerBeanDefinition(beanName,
BeanDefinitionBuilder.genericBeanDefinition(QueryModelEventHandlerFunctionAdapter.class)
.addConstructorArgReference(aggregateBeanName)
.addConstructorArgValue(queryModelEventHandlerMethod.getName())
.addConstructorArgValue(queryModelEventHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(queryModelEventHandlerMethod.getParameterTypes()[1])
.addConstructorArgValue(queryModelEventHandler.create())
.addConstructorArgValue(eventInfo.type())
.addConstructorArgValue(eventInfo.version())
.setInitMethodName("init")
⋮----
throw new ApplicationContextException("Invalid QueryModelEventHandler method signature: " + queryModelEventHandlerMethod);
⋮----
public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
⋮----
public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/beans/QueryModelEventHandlerFunctionAdapter.java
================
public class QueryModelEventHandlerFunctionAdapter<S extends QueryModelState, E extends DomainEvent>
⋮----
hasPIIDataAnnotation(domainEventClass));
⋮----
public void init() {
⋮----
adapterMethod = queryModel.getClass().getMethod(adapterMethodName, domainEventClass, stateClass);
⋮----
throw new RuntimeException(e);
⋮----
public @NotNull S apply(@NotNull E event, S state) {
⋮----
return (S) adapterMethod.invoke(queryModel, event, state);
⋮----
if (e.getCause() != null) {
if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public DomainEventType<E> getEventType() {
⋮----
public QueryModel<S> getQueryModel() {
⋮----
public boolean isCreate() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelAutoConfiguration.java
================
public class AkcesQueryModelAutoConfiguration {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public static QueryModelBeanFactoryPostProcessor queryModelBeanFactoryPostProcessor() {
return new QueryModelBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesQueryModelSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
⋮----
public KafkaAdmin kafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
⋮----
public AkcesQueryModelController queryModelRuntimes(@Qualifier("akcesQueryModelKafkaAdmin") KafkaAdmin kafkaAdmin,
⋮----
return new AkcesQueryModelController(kafkaAdmin, consumerFactory, gdprContextRepositoryFactory);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelController.java
================
public class AkcesQueryModelController extends Thread implements AutoCloseable, ApplicationContextAware, QueryModels {
private static final Logger logger = LoggerFactory.getLogger(AkcesQueryModelController.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
⋮----
private final Cache<String, CachedQueryModelState<?>> queryModelStateCache = Caffeine.newBuilder()
.maximumSize(1000)
.build();
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
this.enabledRuntimes.putAll(
applicationContext.getBeansOfType(QueryModelRuntime.class).values().stream()
.collect(Collectors.toMap(runtime -> ((QueryModelRuntime<?>) runtime).getQueryModelClass(), runtime -> runtime)));
⋮----
private <S extends QueryModelState> QueryModelRuntime<S> getEnabledRuntime(Class<? extends QueryModel<S>> modelClass) {
return (QueryModelRuntime<S>) this.enabledRuntimes.get(modelClass);
⋮----
private <S extends QueryModelState> boolean isRuntimeDisabled(Class<? extends QueryModel<S>> modelClass) {
return this.disabledRuntimes.containsKey(modelClass);
⋮----
public <S extends QueryModelState> CompletionStage<S> getHydratedState(Class<? extends QueryModel<S>> modelClass, String id) {
QueryModelRuntime<S> runtime = getEnabledRuntime(modelClass);
⋮----
CachedQueryModelState<S> cachedQueryModelState = (CachedQueryModelState<S>) queryModelStateCache.getIfPresent(runtime.getName()+"-"+id);
S currentState = cachedQueryModelState != null ? cachedQueryModelState.state() : null;
Long currentOffset = cachedQueryModelState != null ? cachedQueryModelState.offset() : null;
⋮----
commandQueue.add(new HydrationRequest<>(runtime, completableFuture, id, currentState, currentOffset));
⋮----
} else if (isRuntimeDisabled(modelClass)) {
⋮----
return CompletableFuture.failedFuture(new QueryModelExecutionDisabledException(modelClass));
⋮----
return CompletableFuture.failedFuture(new QueryModelNotFoundException(modelClass));
⋮----
public void run() {
try (final Consumer<String, ProtocolRecord> indexConsumer = consumerFactory.createConsumer(
HostUtils.getHostName() + "-AkcesQueryModelController",
⋮----
process(indexConsumer);
⋮----
logger.info("AkcesQueryModelController is shutting down");
⋮----
commandQueue.drainTo(pendingRequests);
pendingRequests.forEach(request -> request.completableFuture.completeExceptionally(
new QueryModelExecutionCancelledException(request.runtime().getQueryModelClass())));
⋮----
Iterator<HydrationExecution<?>> iterator = hydrationExecutions.values().iterator();
while (iterator.hasNext()) {
HydrationExecution<?> execution = iterator.next();
execution.completableFuture.completeExceptionally(
new QueryModelExecutionCancelledException(execution.runtime().getQueryModelClass()));
iterator.remove();
⋮----
for (GDPRContextRepository gdprContextRepository : gdprContextRepositories.values()) {
⋮----
gdprContextRepository.close();
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
private void process(Consumer<String, ProtocolRecord> indexConsumer) {
⋮----
Map<TopicPartition, HydrationExecution<?>> newExecutions = processHydrationRequests(indexConsumer);
⋮----
hydrationExecutions.putAll(newExecutions);
indexConsumer.assign(Stream.concat(hydrationExecutions.keySet().stream(), gdprKeyPartitions.stream()).toList());
⋮----
if(!newExecutions.isEmpty()){
logger.info("Processing {} new HydrationExecutions", newExecutions.size());
⋮----
indexConsumer.endOffsets(newExecutions.keySet()).forEach((partition, endOffset) -> {
hydrationExecutions.computeIfPresent(partition, (topicPartition, hydrationExecution) -> hydrationExecution.withEndOffset(endOffset));
⋮----
newExecutions.forEach((partition, execution) -> {
if (execution.currentOffset() != null) {
indexConsumer.seek(partition, execution.currentOffset());
} else if(hydrationExecutions.get(partition).endOffset() > 0) {
⋮----
indexConsumer.seek(partition, 0);
⋮----
indexConsumer.seekToBeginning(List.of(partition));
⋮----
if(!hydrationExecutions.isEmpty()) {
logger.info("Processing HydrationExecutions {}", hydrationExecutions);
⋮----
ConsumerRecords<String, ProtocolRecord> consumerRecords = indexConsumer.poll(Duration.ofMillis(10));
if(!consumerRecords.isEmpty()) {
logger.info("Processing {}", consumerRecords.partitions());
⋮----
if(!gdprKeyPartitions.isEmpty()) {
List<TopicPartition> gdprKeyPartitions = consumerRecords.partitions().stream()
.filter(topicPartition -> topicPartition.topic().equals("Akces-GDPRKeys")).toList();
logger.info("Processing {} GDPRKeyPartitions", gdprKeyPartitions.size());
⋮----
gdprContextRepositories.get(gdprKeyPartition).process(consumerRecords.records(gdprKeyPartition));
⋮----
List<TopicPartition> indexPartitions = consumerRecords.partitions().stream()
.filter(partition -> !partition.topic().equals("Akces-GDPRKeys")).toList();
logger.info("Processing {} indexPartitions", indexPartitions.size());
⋮----
hydrationExecutions.computeIfPresent(partition,
⋮----
processHydrationExecution(
hydrationExecution.runtime().shouldHandlePIIData() ? getGDPRContextRepository(hydrationExecution.id()) : null,
⋮----
consumerRecords.records(partition)));
⋮----
Iterator<HydrationExecution<?>> itr = hydrationExecutions.values().iterator();
while (itr.hasNext()) {
HydrationExecution<?> execution = itr.next();
if (execution.endOffset() <= indexConsumer.position(execution.indexPartition())) {
logger.info(
⋮----
execution.runtime().getIndexName(),
execution.id(),
execution.runtime().getName(),
execution.indexPartition(),
execution.endOffset(),
indexConsumer.position(execution.indexPartition()));
⋮----
execution.complete();
itr.remove();
queryModelStateCache.put(
execution.runtime().getName()+"-"+execution.id(),
⋮----
execution.currentState(),
indexConsumer.position(execution.indexPartition())));
⋮----
logger.error("Unrecoverable exception in AkcesQueryModelController while {}", processState, e);
⋮----
ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = indexConsumer.poll(Duration.ofMillis(10));
⋮----
gdprContextRepositories.get(gdprKeyPartition).process(gdprKeyRecords.records(gdprKeyPartition));
⋮----
initializedEndOffsets.computeIfPresent(gdprKeyPartition, (partition, endOffset) -> {
if (endOffset <= indexConsumer.position(gdprKeyPartition)) {
⋮----
if (gdprKeyRecords.isEmpty() && initializedEndOffsets.isEmpty()) {
⋮----
Iterator<QueryModelRuntime> iterator = enabledRuntimes.values().iterator();
⋮----
QueryModelRuntime queryModelRuntime = iterator.next();
⋮----
queryModelRuntime.validateDomainEventSchemas();
logger.info("Enabling {} QueryModelRuntime", queryModelRuntime.getName());
⋮----
logger.error(
⋮----
queryModelRuntime.getName(),
⋮----
disabledRuntimes.put(queryModelRuntime.getQueryModelClass(), queryModelRuntime);
⋮----
if(enabledRuntimes.isEmpty() && !disabledRuntimes.isEmpty()) {
logger.error("No QueryModelRuntimes enabled. This is an error. Shutting down");
⋮----
} else if (enabledRuntimes.values().stream().anyMatch(QueryModelRuntime::shouldHandlePIIData)) {
⋮----
logger.info("Loading GDPR keys");
⋮----
TopicDescription controlTopicDescription = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control");
totalPartitions = controlTopicDescription.partitions().size();
⋮----
gdprKeyPartitions.add(new TopicPartition("Akces-GDPRKeys", i));
⋮----
gdprKeyPartitions.forEach(partition -> {
gdprContextRepositories.put(partition, gdprContextRepositoryFactory.create("AkcesQueryModelController",partition.partition()));
⋮----
indexConsumer.assign(gdprKeyPartitions);
⋮----
indexConsumer.seek(partition, gdprContextRepositories.get(partition).getOffset() + 1);
⋮----
initializedEndOffsets = indexConsumer.endOffsets(gdprKeyPartitions);
⋮----
private GDPRContextRepository getGDPRContextRepository(String id) {
Integer partition = Math.abs(hashFunction.hashString(id, UTF_8).asInt()) % totalPartitions;
return gdprContextRepositories.get(new TopicPartition("Akces-GDPRKeys", partition));
⋮----
private Map<TopicPartition, HydrationExecution<?>> processHydrationRequests(Consumer<String, ProtocolRecord> indexConsumer) {
⋮----
HydrationRequest request = commandQueue.poll(100, TimeUnit.MILLISECONDS);
⋮----
logger.info("Processing HydrationRequest on index {} with id {} and runtime {}", request.runtime().getIndexName(), request.id(), request.runtime().getName());
⋮----
QueryModelRuntime runtime = request.runtime();
String topicName = getIndexTopicName(runtime.getIndexName(), request.id());
if (!indexConsumer.partitionsFor(topicName).isEmpty()) {
TopicPartition indexPartition = new TopicPartition(topicName, 0);
newExecutions.put(indexPartition, new HydrationExecution<>(runtime, request.completableFuture(), request.id(), request.currentState(), request.currentOffset(), indexPartition, null));
⋮----
logger.warn("KafkaTopic {} not found for HydrationRequest on index {} with id {}", topicName, request.runtime().getIndexName(), request.id());
request.completableFuture().completeExceptionally(new QueryModelIdNotFoundException(request.runtime().getQueryModelClass(), request.id()));
⋮----
request = commandQueue.poll();
⋮----
logger.warn("Interrupted while processing HydrationRequests", e);
⋮----
Thread.currentThread().interrupt();
⋮----
private <S extends QueryModelState> HydrationExecution<S> processHydrationExecution(@Nullable GDPRContextRepository gdprContextRepository,
⋮----
GDPRContext gdprContext = gdprContextRepository.get(execution.id());
logger.info("Setting GDPRContext {} for aggregateId {}", gdprContext.getClass().getSimpleName(), execution.id());
GDPRContextHolder.setCurrentGDPRContext(gdprContext);
⋮----
records.size(),
⋮----
execution.runtime().getName());
return execution.withCurrentState(execution.runtime().apply(
records.stream().map(record -> (DomainEventRecord) record.value())
.toList(), execution.currentState()));
⋮----
logger.error("Exception while processing HydrationExecution", e);
⋮----
new QueryModelExecutionException(
⋮----
execution.runtime().getQueryModelClass(),
⋮----
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
public void close() throws Exception {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesQueryModelController has been shutdown");
⋮----
logger.warn("AkcesQueryModelController did not shutdown within 10 seconds");
⋮----
public boolean isRunning() {
⋮----
HydrationExecution<S> withEndOffset(Long endOffset) {
⋮----
HydrationExecution<S> withCurrentState(S currentState) {
⋮----
void complete() {
⋮----
completableFuture.complete(currentState);
⋮----
completableFuture.completeExceptionally(new QueryModelIdNotFoundException(runtime.getQueryModelClass(), id));

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelControllerState.java
================


================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/KafkaQueryModelRuntime.java
================
public class KafkaQueryModelRuntime<S extends QueryModelState> implements QueryModelRuntime<S> {
private static final Logger logger = LoggerFactory.getLogger(KafkaQueryModelRuntime.class);
⋮----
public String getName() {
return type.typeName();
⋮----
public String getIndexName() {
return type.indexName();
⋮----
public Class<? extends QueryModel<S>> getQueryModelClass() {
⋮----
public S apply(List<DomainEventRecord> eventRecords, S currentState) throws IOException {
⋮----
DomainEventType<?> domainEventType = getDomainEventType(eventRecord);
⋮----
if (state == null && createStateHandler != null && createStateHandler.getEventType().equals(domainEventType)) {
state = createStateHandler.apply(materialize(domainEventType, eventRecord), null);
⋮----
QueryModelEventHandlerFunction<S, DomainEvent> eventHandler = queryModelEventHandlers.get(domainEventType);
⋮----
state = eventHandler.apply(materialize(domainEventType, eventRecord), state);
⋮----
public void validateDomainEventSchemas() {
for (DomainEventType<?> domainEventType : domainEvents.values()) {
schemaRegistry.validate(domainEventType);
⋮----
public boolean shouldHandlePIIData() {
⋮----
private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
⋮----
private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {
⋮----
return domainEvents.entrySet().stream()
.filter(entry -> entry.getValue().external())
.filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
.filter(entry -> entry.getValue().version() <= eventRecord.version())
.max(Comparator.comparingInt(entry -> entry.getValue().version()))
.map(Map.Entry::getValue).orElse(null);
⋮----
public static class Builder<S extends QueryModelState> {
⋮----
public Builder<S> setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
⋮----
public Builder<S> setObjectMapper(ObjectMapper objectMapper) {
⋮----
public Builder<S> setStateType(QueryModelStateType<S> stateType) {
⋮----
public Builder<S> setQueryModelClass(Class<? extends QueryModel<S>> queryModelClass) {
⋮----
public Builder<S> setCreateHandler(QueryModelEventHandlerFunction<S, DomainEvent> createStateHandler) {
⋮----
public Builder<S> addDomainEvent(DomainEventType<?> domainEvent) {
this.domainEvents.put(domainEvent.typeClass(), domainEvent);
⋮----
public Builder<S> addQueryModelEventHandler(DomainEventType<?> eventType,
⋮----
this.queryModelEventHandlers.put(eventType, eventSourcingHandler);
⋮----
public KafkaQueryModelRuntime<S> build() {
final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
⋮----
public String toString() {
return "KafkaQueryModelRuntime{"+getName()+"}";

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionCancelledException.java
================
public class QueryModelExecutionCancelledException extends QueryModelExecutionException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionDisabledException.java
================
public class QueryModelExecutionDisabledException extends QueryModelExecutionException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionException.java
================
public class QueryModelExecutionException extends RuntimeException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelIdNotFoundException.java
================
public class QueryModelIdNotFoundException extends QueryModelExecutionException {
⋮----
super("Id "+modelId+ "doesn't exist for QueryModel: "+modelClass.getSimpleName(), modelClass);
⋮----
public String getModelId() {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelImplementationPresentCondition.java
================
public class QueryModelImplementationPresentCondition extends SpringBootCondition {
⋮----
public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) {
boolean match = Optional.ofNullable(context.getBeanFactory())
.map(beanFactory -> beanFactory.getBeanNamesForAnnotation(QueryModelInfo.class))
.filter(beanNames -> beanNames.length > 0).isPresent();
return match ? ConditionOutcome.match() : ConditionOutcome.noMatch("No QueryModel beans found");

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelNotFoundException.java
================
public class QueryModelNotFoundException extends QueryModelExecutionException {

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelRuntime.java
================
public interface QueryModelRuntime<S extends QueryModelState> {
String getName();
⋮----
String getIndexName();
⋮----
Class<? extends QueryModel<S>> getQueryModelClass();
⋮----
S apply(List<DomainEventRecord> eventRecords, S currentState) throws IOException;
⋮----
void validateDomainEventSchemas();
⋮----
boolean shouldHandlePIIData();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelRuntimeFactory.java
================
public class QueryModelRuntimeFactory<S extends QueryModelState> implements FactoryBean<QueryModelRuntime<S>>, ApplicationContextAware {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
public QueryModelRuntime<S> getObject() throws Exception {
return createRuntime(queryModel);
⋮----
public Class<?> getObjectType() {
⋮----
private QueryModelRuntime<S> createRuntime(QueryModel<S> queryModel) {
⋮----
QueryModelInfo queryModelInfo = queryModel.getClass().getAnnotation(QueryModelInfo.class);
⋮----
runtimeBuilder.setStateType(new QueryModelStateType<S>(
queryModelInfo.value(),
queryModelInfo.version(),
queryModel.getStateClass(),
queryModelInfo.indexName()
⋮----
throw new IllegalStateException("Class implementing Aggregate must be annotated with @QueryModelInfo");
⋮----
runtimeBuilder.setQueryModelClass((Class<? extends QueryModel<S>>) queryModel.getClass());
runtimeBuilder.setObjectMapper(objectMapper);
⋮----
applicationContext.getBeansOfType(QueryModelEventHandlerFunction.class).values().stream()
.filter(adapter -> adapter.getQueryModel().equals(queryModel))
.forEach(adapter -> {
DomainEventType<?> type = adapter.getEventType();
if (adapter.isCreate()) {
runtimeBuilder.setCreateHandler(adapter);
runtimeBuilder.addDomainEvent(type);
⋮----
runtimeBuilder.addQueryModelEventHandler(type, adapter);
⋮----
return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModels.java
================
public interface QueryModels {
<S extends QueryModelState> CompletionStage<S> getHydratedState(Class<? extends QueryModel<S>> modelClass, String id);

================
File: main/query-support/src/main/java/org/elasticsoftware/akces/query/QueryServiceApplication.java
================
public class QueryServiceApplication {
public static void main(String[] args) {
SpringApplication application = new SpringApplication(QueryServiceApplication.class);
⋮----
application.setSources(Set.of(args));
⋮----
application.run();

================
File: main/query-support/src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#

org.elasticsoftware.akces.query.models.AkcesQueryModelAutoConfiguration
org.elasticsoftware.akces.query.database.AkcesDatabaseModelAutoConfiguration

================
File: main/query-support/src/main/resources/akces-databasemodel.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor

================
File: main/query-support/src/main/resources/akces-querymodel.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/database/model/DefaultJdbcModel.java
================
public class DefaultJdbcModel extends JdbcDatabaseModel {
⋮----
public void handle(AccountCreatedEvent event) {
jdbcTemplate.update("""
⋮----
event.userId(),
event.country(),
event.firstName(),
event.lastName(),
event.email()

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/database/DatabaseModelRuntimeTests.java
================
public class DatabaseModelRuntimeTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
.withDatabaseName("akces")
.withUsername("akces")
.withPassword("akces")
⋮----
.withNetworkAliases("postgresql");
⋮----
public static class ContextInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
List.of(
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
"spring.datasource.url=" + postgresql.getJdbcUrl(),
⋮----
public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<?>> domainEventClasses) {
SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static void cleanUp() throws IOException {
⋮----
if (Files.exists(Paths.get("/tmp/akces"))) {
⋮----
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
void testContextLoads() {
assertNotNull(adminClient);
assertNotNull(schemaRegistryClient);
assertNotNull(consumerFactory);
assertNotNull(objectMapper);
assertNotNull(walletController);
assertNotNull(accountController);
assertNotNull(orderProcessManagerController);
assertNotNull(akcesClientController);
assertNotNull(defaultModelController);
⋮----
while (!walletController.isRunning() ||
!accountController.isRunning() ||
!orderProcessManagerController.isRunning() ||
!akcesClientController.isRunning() ||
!defaultModelController.isRunning()) {
Thread.onSpinWait();
⋮----
public void testFindBeans() {
⋮----
assertNotNull(applicationContext);
assertEquals(1, applicationContext.getBeansOfType(DatabaseModelRuntime.class).size());
⋮----
public void testCreateAccount() throws Exception {
⋮----
CreateAccountCommand command = new CreateAccountCommand(
⋮----
akcesClientController.sendAndForget("Default", userId, command);
⋮----
Thread.sleep(1000);
⋮----
JdbcTemplate jdbcTemplate = applicationContext.getBean(JdbcTemplate.class);
Map<String, Object> result = jdbcTemplate.queryForMap(
⋮----
assertEquals(userId, result.get("user_id"));
assertEquals("NL", result.get("country"));
assertEquals("John", result.get("first_name"));
assertEquals("Doe", result.get("last_name"));
assertEquals("john.doe@example.com", result.get("email"));

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/database/DatabaseModelTestConfiguration.java
================
public class DatabaseModelTestConfiguration {
⋮----
public SpringLiquibase liquibase(DataSource dataSource) {
SpringLiquibase liquibase = new SpringLiquibase();
liquibase.setChangeLog("classpath:db/changelog/liquibase.yaml");
liquibase.setDataSource(dataSource);

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/account/AccountQueryModel.java
================
public class AccountQueryModel implements QueryModel<AccountQueryModelState> {
⋮----
public String getName() {
⋮----
public Class<AccountQueryModelState> getStateClass() {
⋮----
public String getIndexName() {
⋮----
public AccountQueryModelState create(AccountCreatedEvent event, AccountQueryModelState isNull) {
return new AccountQueryModelState(event.userId(), event.country(), event.firstName(), event.lastName(), event.email());

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/account/AccountQueryModelState.java
================
) implements QueryModelState {
⋮----
public String getIndexKey() {
return accountId();

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/wallet/WalletQueryModel.java
================
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
⋮----
public String getName() {
⋮----
public Class<WalletQueryModelState> getStateClass() {
⋮----
public String getIndexName() {
⋮----
public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
return new WalletQueryModelState(event.id(), List.of());
⋮----
public WalletQueryModelState createBalance(BalanceCreatedEvent event, WalletQueryModelState currentState) {
WalletQueryModelState.Balance balance = new WalletQueryModelState.Balance(event.currency(), BigDecimal.ZERO);
List<WalletQueryModelState.Balance> balances = new ArrayList<>(currentState.balances());
balances.add(balance);
return new WalletQueryModelState(currentState.walletId(), balances);
⋮----
public WalletQueryModelState creditWallet(WalletCreditedEvent event, WalletQueryModelState currentState) {
return new WalletQueryModelState(
currentState.walletId(),
currentState.balances().stream().map(balance -> {
if (balance.currency().equals(event.currency())) {
⋮----
balance.currency(),
balance.amount().add(event.amount()),
balance.reservedAmount()
⋮----
}).toList());

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/wallet/WalletQueryModelState.java
================
public record WalletQueryModelState(String walletId, List<Balance> balances) implements QueryModelState {
⋮----
public String getIndexKey() {
return walletId();
⋮----
public BigDecimal getAvailableAmount() {
return amount.subtract(reservedAmount);

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/QueryModelRuntimeTests.java
================
public class QueryModelRuntimeTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<?>> domainEventClasses) {
SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static void cleanUp() throws IOException {
⋮----
if (Files.exists(Paths.get("/tmp/akces"))) {
⋮----
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
void testContextLoads() {
assertNotNull(adminClient);
assertNotNull(schemaRegistryClient);
assertNotNull(consumerFactory);
assertNotNull(objectMapper);
assertNotNull(walletController);
assertNotNull(accountController);
assertNotNull(orderProcessManagerController);
assertNotNull(akcesClientController);
assertNotNull(akcesQueryModelController);
⋮----
while (!walletController.isRunning() ||
!accountController.isRunning() ||
!orderProcessManagerController.isRunning() ||
!akcesClientController.isRunning() ||
!akcesQueryModelController.isRunning()) {
Thread.onSpinWait();
⋮----
public void testFindBeans() {
⋮----
assertNotNull(applicationContext);
assertEquals(4, applicationContext.getBeansOfType(QueryModelEventHandlerFunction.class).size());
assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_create_WalletCreated_1"));
assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_createBalance_BalanceCreated_1"));
assertNotNull(applicationContext.getBean("AccountQueryModel_qmeh_create_AccountCreated_1"));
assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_creditWallet_WalletCredited_1"));
assertEquals(2, applicationContext.getBeansOfType(QueryModelRuntimeFactory.class).size());
assertNotNull(applicationContext.getBean("WalletQueryModelQueryModelRuntime"));
assertNotNull(applicationContext.getBean("AccountQueryModelQueryModelRuntime"));
assertEquals(2, applicationContext.getBeansOfType(QueryModelRuntime.class).size());
⋮----
public void testWithUnknownId() throws InterruptedException, TimeoutException {
⋮----
CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(
WalletQueryModel.class, "unknown-id").toCompletableFuture();
assertNotNull(stateFuture);
ExecutionException exception = assertThrows(ExecutionException.class, stateFuture::get);
assertInstanceOf(QueryModelIdNotFoundException.class, exception.getCause());
⋮----
public void testWithUnknownIdAgainAndInsureNotCreated() throws InterruptedException, TimeoutException {
⋮----
CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, "unknown-id")
.toCompletableFuture();
⋮----
public void testRegisteredSchemas() throws RestClientException, IOException {
⋮----
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas("domainevents.WalletCreated", false, false);
assertFalse(registeredSchemas.isEmpty());
assertEquals(1, schemaRegistryClient.getVersion("domainevents.WalletCreated", registeredSchemas.getFirst()));
⋮----
registeredSchemas = schemaRegistryClient.getSchemas("domainevents.AccountCreated", false, false);
⋮----
assertEquals(1, schemaRegistryClient.getVersion("domainevents.AccountCreated", registeredSchemas.getFirst()));
⋮----
public void testCreateAndQueryWalletQueryModel() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
CreateWalletCommand createWalletCommand = new CreateWalletCommand(userId, "BTC");
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createWalletCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));
Assertions.assertNotNull(result);
Assertions.assertEquals(2, result.size());
assertInstanceOf(WalletCreatedEvent.class, result.getFirst());
assertInstanceOf(BalanceCreatedEvent.class, result.getLast());
⋮----
CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
WalletQueryModelState state = assertDoesNotThrow(() -> stateFuture.get(10, TimeUnit.SECONDS));
assertNotNull(state);
⋮----
public void testCreateAndQueryWalletQueryModelWithMultipleConcurrentRequests() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
List<String> userIds = List.of(
⋮----
List<CompletableFuture<List<DomainEvent>>> futures = userIds.stream()
.map(userId -> akcesClientController.send("TEST_TENANT", new CreateWalletCommand(userId, "BTC"))
.toCompletableFuture())
.toList();
⋮----
CompletableFuture<Void> allOf = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
allOf.get(10, TimeUnit.SECONDS);
⋮----
List<DomainEvent> result = future.get();
assertNotNull(result);
assertEquals(2, result.size());
assertInstanceOf(WalletCreatedEvent.class, result.get(0));
assertInstanceOf(BalanceCreatedEvent.class, result.get(1));
⋮----
List<CompletableFuture<WalletQueryModelState>> stateFutures = userIds.stream()
.map(userId -> akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
CompletableFuture<Void> allOfStates = CompletableFuture.allOf(stateFutures.toArray(new CompletableFuture[0]));
allOfStates.get(10, TimeUnit.SECONDS);
⋮----
WalletQueryModelState state = stateFuture.get();
⋮----
public void testAccountQueryModel() {
⋮----
CreateAccountCommand createAccountCommand = new CreateAccountCommand(userId, "US", "John", "Doe", "john.doe@example.com");
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createAccountCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));
⋮----
Assertions.assertEquals(1, result.size());
assertInstanceOf(AccountCreatedEvent.class, result.getFirst());
⋮----
CompletableFuture<AccountQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
⋮----
AccountQueryModelState state = assertDoesNotThrow(() -> stateFuture.get(10, TimeUnit.SECONDS));
⋮----
assertThat(state.country()).isEqualTo("US");
assertThat(state.firstName()).isEqualTo("John");
assertThat(state.lastName()).isEqualTo("Doe");
assertThat(state.email()).isEqualTo("john.doe@example.com");
⋮----
public void testAccountQueryModelUsingCache() {
⋮----
CompletableFuture<AccountQueryModelState> stateFuture1 = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
⋮----
assertNotNull(stateFuture1);
AccountQueryModelState state1 = assertDoesNotThrow(() -> stateFuture1.get(10, TimeUnit.SECONDS));
assertNotNull(state1);
assertThat(state1.country()).isEqualTo("US");
assertThat(state1.firstName()).isEqualTo("John");
assertThat(state1.lastName()).isEqualTo("Doe");
assertThat(state1.email()).isEqualTo("john.doe@example.com");
⋮----
CompletableFuture<AccountQueryModelState> stateFuture2 = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
⋮----
assertNotNull(stateFuture2);
AccountQueryModelState state2 = assertDoesNotThrow(() -> stateFuture2.get(10, TimeUnit.SECONDS));
assertNotNull(state2);
assertThat(state2.country()).isEqualTo("US");
assertThat(state2.firstName()).isEqualTo("John");
assertThat(state2.lastName()).isEqualTo("Doe");
assertThat(state2.email()).isEqualTo("john.doe@example.com");
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture);
WalletQueryModelState walletState = assertDoesNotThrow(() -> walletStateFuture.get(10, TimeUnit.SECONDS));
assertNotNull(walletState);
assertEquals(1, walletState.balances().size());
assertEquals("EUR", walletState.balances().getFirst().currency());
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture2 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture2);
WalletQueryModelState walletState2 = assertDoesNotThrow(() -> walletStateFuture2.get(10, TimeUnit.SECONDS));
assertNotNull(walletState2);
assertEquals(1, walletState2.balances().size());
assertEquals("EUR", walletState2.balances().getFirst().currency());
⋮----
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreateBalanceCommand(userId, "BTC"))
.toCompletableFuture().get(10, TimeUnit.SECONDS));
⋮----
assertInstanceOf(BalanceCreatedEvent.class, result.getFirst());
assertEquals("BTC", ((BalanceCreatedEvent) result.getFirst()).currency());
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture3 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture3);
WalletQueryModelState walletState3 = assertDoesNotThrow(() -> walletStateFuture3.get(10, TimeUnit.SECONDS));
assertNotNull(walletState3);
assertEquals(2, walletState3.balances().size());
assertEquals("EUR", walletState3.balances().getFirst().currency());
assertEquals("BTC", walletState3.balances().getLast().currency());
⋮----
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreateBalanceCommand(userId, "ETH"))
⋮----
assertInstanceOf(BalanceCreatedEvent.class, result.get(0));
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture4 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture4);
WalletQueryModelState walletState4 = assertDoesNotThrow(() -> walletStateFuture4.get(10, TimeUnit.SECONDS));
assertNotNull(walletState4);
assertEquals(3, walletState4.balances().size());
assertEquals("EUR", walletState4.balances().get(0).currency());
assertEquals("BTC", walletState4.balances().get(1).currency());
assertEquals("ETH", walletState4.balances().get(2).currency());
⋮----
result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreditWalletCommand(userId, "EUR", new BigDecimal("1000.00")))
⋮----
assertInstanceOf(WalletCreditedEvent.class, result.get(0));
assertEquals("EUR", ((WalletCreditedEvent) result.get(0)).currency());
assertEquals(new BigDecimal("1000.00"), ((WalletCreditedEvent) result.get(0)).amount());
⋮----
CompletableFuture<WalletQueryModelState> walletStateFuture5 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
⋮----
assertNotNull(walletStateFuture5);
WalletQueryModelState walletState5 = assertDoesNotThrow(() -> walletStateFuture5.get(10, TimeUnit.SECONDS));
assertNotNull(walletState5);
assertEquals(3, walletState5.balances().size());
assertEquals("EUR", walletState5.balances().getFirst().currency());
assertEquals(new BigDecimal("1000.00"), walletState5.balances().getFirst().amount());
⋮----
public static class ContextInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
List.of(
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)
⋮----
public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
⋮----
controlProducer.commitTransaction();

================
File: main/query-support/src/test/java/org/elasticsoftware/akces/query/models/QueryModelTestConfiguration.java
================
public class QueryModelTestConfiguration {

================
File: main/query-support/src/test/resources/db/changelog/liquibase.yaml
================
databaseChangeLog:
  - changeSet:
      id: create-partition-offsets-table
      author: jwijgerd
      changes:
        - createTable:
            tableName: partition_offsets
            columns:
              - column:
                  name: partition_id
                  type: varchar(255)
                  constraints:
                    primaryKey: true
                    nullable: false
              - column:
                  name: record_offset
                  type: bigint
                  constraints:
                    nullable: false
      rollback:
        - dropTable:
            tableName: partition_offsets

  - changeSet:
      id: create-account-table
      author: jwijgerd
      changes:
        - createTable:
            tableName: account
            columns:
              - column:
                  name: user_id
                  type: varchar(255)
                  constraints:
                    primaryKey: true
                    nullable: false
              - column:
                  name: country
                  type: varchar(2)
                  constraints:
                    nullable: false
              - column:
                  name: first_name
                  type: varchar(255)
                  constraints:
                    nullable: false
              - column:
                  name: last_name
                  type: varchar(255)
                  constraints:
                    nullable: false
              - column:
                  name: email
                  type: varchar(255)
                  constraints:
                    nullable: false
                    unique: true
      rollback:
        - dropTable:
            tableName: account

================
File: main/query-support/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.query.models" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.query.database" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.gdpr" level="info" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: main/query-support/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.8.13-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Query Support</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-query-support</artifactId>
    <packaging>jar</packaging>

    <properties>

    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-jdbc</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.persistence</groupId>
            <artifactId>jakarta.persistence-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.data</groupId>
            <artifactId>spring-data-jpa</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>

        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>postgresql</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.postgresql</groupId>
            <artifactId>postgresql</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.liquibase</groupId>
            <artifactId>liquibase-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-jdbc</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-runtime</artifactId>
            <classifier>tests</classifier>
            <type>test-jar</type>
            <version>${project.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-client</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-runtime</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
    </dependencies>

</project>

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/aggregate/AggregateRuntime.java
================
public interface AggregateRuntime {
⋮----
String getName();
⋮----
Class<? extends Aggregate> getAggregateClass();
⋮----
void handleCommandRecord(CommandRecord commandRecord,
⋮----
void handleExternalDomainEventRecord(DomainEventRecord eventRecord,
⋮----
Collection<DomainEventType<?>> getAllDomainEventTypes();
⋮----
Collection<DomainEventType<?>> getProducedDomainEventTypes();
⋮----
Collection<DomainEventType<?>> getExternalDomainEventTypes();
⋮----
Collection<CommandType<?>> getAllCommandTypes();
⋮----
Collection<CommandType<?>> getLocalCommandTypes();
⋮----
Collection<CommandType<?>> getExternalCommandTypes();
⋮----
CommandType<?> getLocalCommandType(String type, int version);
⋮----
void registerAndValidate(DomainEventType<?> domainEventType, boolean forceRegisterOnIncompatible) throws SchemaException;
⋮----
default void registerAndValidate(DomainEventType<?> domainEventType) throws SchemaException {
registerAndValidate(domainEventType, false);
⋮----
void registerAndValidate(CommandType<?> commandType,  boolean forceRegisterOnIncompatible) throws SchemaException;
⋮----
default void registerAndValidate(CommandType<?> commandType) throws SchemaException {
registerAndValidate(commandType, false);
⋮----
Command materialize(CommandType<?> commandType, CommandRecord commandRecord) throws IOException;
⋮----
byte[] serialize(Command command) throws SerializationException;
⋮----
boolean shouldGenerateGDPRKey(CommandRecord commandRecord);
⋮----
boolean shouldGenerateGDPRKey(DomainEventRecord eventRecord);
⋮----
boolean requiresGDPRContext(DomainEventRecord eventRecord);
⋮----
boolean requiresGDPRContext(CommandRecord eventRecord);
⋮----
boolean shouldHandlePIIData();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/aggregate/IndexParams.java
================


================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/AggregateBeanFactoryPostProcessor.java
================
public class AggregateBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
private static final Logger logger = LoggerFactory.getLogger(AggregateBeanFactoryPostProcessor.class);
public static final List<DomainEventType<? extends DomainEvent>> COMMAND_HANDLER_CREATE_SYSTEM_ERRORS = List.of(
⋮----
public static final List<DomainEventType<? extends DomainEvent>> COMMAND_HANDLER_SYSTEM_ERRORS = List.of(
⋮----
public static final List<DomainEventType<? extends DomainEvent>> EVENT_HANDLER_CREATE_SYSTEM_ERRORS = List.of(
⋮----
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
⋮----
logger.info("Processing Aggregate beans");
Arrays.asList(beanFactory.getBeanNamesForAnnotation(AggregateInfo.class)).forEach(beanName -> {
logger.info("Processing Aggregate bean {}", beanName);
BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
⋮----
Class<?> aggregateClass = Class.forName(bd.getBeanClassName());
List<Method> commandHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(CommandHandler.class))
.toList();
List<Method> eventHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(EventHandler.class))
⋮----
List<Method> eventSourcingHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(EventSourcingHandler.class))
⋮----
List<Method> eventBridgeHandlers = Arrays.stream(aggregateClass.getMethods())
.filter(method -> method.isAnnotationPresent(EventBridgeHandler.class))
⋮----
commandHandlers.forEach(commandHandlerMethod ->
processCommandHandler(beanName, commandHandlerMethod, bdr));
eventHandlers.forEach(eventHandlerMethod ->
processEventHandler(beanName, eventHandlerMethod, bdr));
eventSourcingHandlers.forEach(eventSourcingHandlerMethod ->
processEventSourcingHandler(beanName, eventSourcingHandlerMethod, bdr));
eventBridgeHandlers.forEach(eventBridgeHandlerMethod ->
processEventBridgeHandler(beanName, eventBridgeHandlerMethod, bdr));
⋮----
throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
⋮----
bdr.registerBeanDefinition(beanName + "AggregateRuntimeFactory",
BeanDefinitionBuilder.genericBeanDefinition(AggregateRuntimeFactory.class)
.addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
.addConstructorArgReference("aggregateServiceSchemaRegistry")
.addConstructorArgReference(beanName)
.getBeanDefinition());
⋮----
if (beanFactory.containsBeanDefinition("aggregateServiceConsumerFactory") &&
beanFactory.containsBeanDefinition("aggregateServiceProducerFactory") &&
beanFactory.containsBeanDefinition("aggregateServiceControlProducerFactory") &&
beanFactory.containsBeanDefinition("aggregateServiceAggregateStateRepositoryFactory")) {
bdr.registerBeanDefinition(beanName + "AkcesController",
BeanDefinitionBuilder.genericBeanDefinition(AkcesAggregateController.class)
.addConstructorArgReference("aggregateServiceConsumerFactory")
.addConstructorArgReference("aggregateServiceProducerFactory")
.addConstructorArgReference("aggregateServiceControlConsumerFactory")
.addConstructorArgReference("aggregateServiceControlProducerFactory")
.addConstructorArgReference("aggregateServiceAggregateStateRepositoryFactory")
.addConstructorArgReference("aggregateServiceGDPRContextRepositoryFactory")
.addConstructorArgReference(beanName + "AggregateRuntimeFactory")
⋮----
.addConstructorArgReference("aggregateServiceKafkaAdmin")
.setInitMethodName("start")
.setDestroyMethodName("close")
⋮----
throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
⋮----
private void processEventSourcingHandler(String aggregateBeanName, Method eventSourcingHandlerMethod, BeanDefinitionRegistry bdr) {
EventSourcingHandler eventSourcingHandler = eventSourcingHandlerMethod.getAnnotation(EventSourcingHandler.class);
if (eventSourcingHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(eventSourcingHandlerMethod.getParameterTypes()[0]) &&
AggregateState.class.isAssignableFrom(eventSourcingHandlerMethod.getParameterTypes()[1]) &&
AggregateState.class.isAssignableFrom(eventSourcingHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = eventSourcingHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_esh_" + eventSourcingHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
bdr.registerBeanDefinition(beanName,
BeanDefinitionBuilder.genericBeanDefinition(EventSourcingHandlerFunctionAdapter.class)
.addConstructorArgReference(aggregateBeanName)
.addConstructorArgValue(eventSourcingHandlerMethod.getName())
.addConstructorArgValue(eventSourcingHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(eventSourcingHandlerMethod.getParameterTypes()[1])
.addConstructorArgValue(eventSourcingHandler.create())
.addConstructorArgValue(eventInfo.type())
.addConstructorArgValue(eventInfo.version())
.setInitMethodName("init")
⋮----
throw new ApplicationContextException("Invalid EventSourcingHandler method signature: " + eventSourcingHandlerMethod);
⋮----
private void processEventHandler(String aggregateBeanName, Method eventHandlerMethod, BeanDefinitionRegistry bdr) {
EventHandler eventHandler = eventHandlerMethod.getAnnotation(EventHandler.class);
if (eventHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(eventHandlerMethod.getParameterTypes()[0]) &&
AggregateState.class.isAssignableFrom(eventHandlerMethod.getParameterTypes()[1]) &&
Stream.class.isAssignableFrom(eventHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = eventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_eh_" + eventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
⋮----
BeanDefinitionBuilder.genericBeanDefinition(EventHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(eventHandlerMethod.getName())
.addConstructorArgValue(eventHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(eventHandlerMethod.getParameterTypes()[1])
.addConstructorArgValue(eventHandler.create())
.addConstructorArgValue(generateDomainEventTypes(eventHandler.produces(), eventHandler.create()))
.addConstructorArgValue(generateEventHandlerErrorEventTypes(eventHandler.errors(), eventHandler.create()))
⋮----
throw new ApplicationContextException("Invalid EventHandler method signature: " + eventHandlerMethod);
⋮----
private void processCommandHandler(String aggregateBeanName, Method commandHandlerMethod, BeanDefinitionRegistry bdr) {
⋮----
CommandHandler commandHandler = commandHandlerMethod.getAnnotation(CommandHandler.class);
if (commandHandlerMethod.getParameterCount() == 2 &&
Command.class.isAssignableFrom(commandHandlerMethod.getParameterTypes()[0]) &&
AggregateState.class.isAssignableFrom(commandHandlerMethod.getParameterTypes()[1]) &&
Stream.class.isAssignableFrom(commandHandlerMethod.getReturnType())) {
CommandInfo commandInfo = commandHandlerMethod.getParameterTypes()[0].getAnnotation(CommandInfo.class);
⋮----
String beanName = aggregateBeanName + "_ch_" + commandHandlerMethod.getName() + "_" + commandInfo.type() + "_" + commandInfo.version();
⋮----
BeanDefinitionBuilder.genericBeanDefinition(CommandHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(commandHandlerMethod.getName())
.addConstructorArgValue(commandHandlerMethod.getParameterTypes()[0])
.addConstructorArgValue(commandHandlerMethod.getParameterTypes()[1])
.addConstructorArgValue(commandHandler.create())
.addConstructorArgValue(generateDomainEventTypes(commandHandler.produces(), commandHandler.create()))
.addConstructorArgValue(generateCommandHandlerErrorEventTypes(commandHandler.errors(), commandHandler.create()))
.addConstructorArgValue(commandInfo.type())
.addConstructorArgValue(commandInfo.version())
.setInitMethodName("init").getBeanDefinition()
⋮----
throw new ApplicationContextException("Invalid CommandHandler method signature: " + commandHandlerMethod);
⋮----
private void processEventBridgeHandler(String aggregateBeanName, Method eventBridgeHandlerMethod, BeanDefinitionRegistry bdr) {
if (eventBridgeHandlerMethod.getParameterCount() == 2 &&
DomainEvent.class.isAssignableFrom(eventBridgeHandlerMethod.getParameterTypes()[0]) &&
eventBridgeHandlerMethod.getParameterTypes()[1].equals(CommandBus.class) &&
void.class.equals(eventBridgeHandlerMethod.getReturnType())) {
DomainEventInfo eventInfo = eventBridgeHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);
⋮----
String beanName = aggregateBeanName + "_ebh_" + eventBridgeHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
⋮----
BeanDefinitionBuilder.genericBeanDefinition(EventBridgeHandlerFunctionAdapter.class)
⋮----
.addConstructorArgValue(eventBridgeHandlerMethod.getName())
.addConstructorArgValue(eventBridgeHandlerMethod.getParameterTypes()[0])
⋮----
throw new ApplicationContextException("Invalid EventBridgeHandler method signature: " + eventBridgeHandlerMethod);
⋮----
private List<DomainEventType<?>> generateDomainEventTypes(Class<? extends DomainEvent>[] domainEventClasses,
⋮----
return Arrays.stream(domainEventClasses).map(eventClass -> {
DomainEventInfo eventInfo = eventClass.getAnnotation(DomainEventInfo.class);
return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, isCreate, false, false, hasPIIDataAnnotation(eventClass));
}).collect(Collectors.toList());
⋮----
private List<DomainEventType<?>> generateEventHandlerErrorEventTypes(Class<? extends DomainEvent>[] domainEventClasses, boolean isCreate) {
Stream<DomainEventType<? extends DomainEvent>> systemErrorEvents = (isCreate) ? EVENT_HANDLER_CREATE_SYSTEM_ERRORS.stream() : Stream.empty();
return Stream.concat(Arrays.stream(domainEventClasses).map(eventClass -> {
⋮----
return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, false, false, true, hasPIIDataAnnotation(eventClass));
}), systemErrorEvents).collect(Collectors.toList());
⋮----
private List<DomainEventType<?>> generateCommandHandlerErrorEventTypes(Class<? extends DomainEvent>[] domainEventClasses, boolean isCreate) {
Stream<DomainEventType<? extends DomainEvent>> systemErrorEvents = (isCreate) ? COMMAND_HANDLER_CREATE_SYSTEM_ERRORS.stream() : COMMAND_HANDLER_SYSTEM_ERRORS.stream();
⋮----
public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
logger.info("Processing Aggregate beans for AOT");
⋮----
public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/CommandHandlerFunctionAdapter.java
================
public class CommandHandlerFunctionAdapter<S extends AggregateState, C extends Command, E extends DomainEvent>
⋮----
hasPIIDataAnnotation(commandClass));
⋮----
public void init() {
⋮----
adapterMethod = aggregate.getClass().getMethod(adapterMethodName, commandClass, stateClass);
⋮----
throw new RuntimeException(e);
⋮----
public Stream<E> apply(C command, S state) {
⋮----
return (Stream<E>) adapterMethod.invoke(aggregate, command, state);
⋮----
if (e.getCause() != null) {
if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public boolean isCreate() {
⋮----
public CommandType<C> getCommandType() {
⋮----
public Aggregate<S> getAggregate() {
⋮----
public List<DomainEventType<E>> getProducedDomainEventTypes() {
⋮----
public List<DomainEventType<E>> getErrorEventTypes() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/DomainEventTypeValueCodeGeneratorDelegate.java
================
public class DomainEventTypeValueCodeGeneratorDelegate implements ValueCodeGenerator.Delegate {
⋮----
public CodeBlock generateCode(ValueCodeGenerator valueCodeGenerator, Object value) {
⋮----
return CodeBlock.builder()
.add("new $T($S, $L, $T.class, $L, $L, $L, $L)",
⋮----
.build();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/EventBridgeHandlerFunctionAdapter.java
================
public class EventBridgeHandlerFunctionAdapter<S extends AggregateState, E extends DomainEvent> implements EventBridgeHandlerFunction<S, E> {
⋮----
ErrorEvent.class.isAssignableFrom(inputEventClass),
hasPIIDataAnnotation(inputEventClass));
⋮----
public void init() {
⋮----
adapterMethod = aggregate.getClass().getMethod(adapterMethodName, inputEventClass, CommandBus.class);
⋮----
throw new RuntimeException(e);
⋮----
public void apply(@NotNull E event, CommandBus commandBus) {
⋮----
adapterMethod.invoke(aggregate, event, commandBus);
⋮----
if (e.getCause() != null) {
if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public DomainEventType<E> getEventType() {
⋮----
public Aggregate<S> getAggregate() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/EventHandlerFunctionAdapter.java
================
public class EventHandlerFunctionAdapter<S extends AggregateState, InputEvent extends DomainEvent, E extends DomainEvent> implements EventHandlerFunction<S, InputEvent, E> {
⋮----
ErrorEvent.class.isAssignableFrom(inputEventClass),
hasPIIDataAnnotation(inputEventClass));
⋮----
public void init() {
⋮----
adapterMethod = aggregate.getClass().getMethod(adapterMethodName, inputEventClass, stateClass);
⋮----
throw new RuntimeException(e);
⋮----
public Stream<E> apply(@NotNull InputEvent event, S state) {
⋮----
return (Stream<E>) adapterMethod.invoke(aggregate, event, state);
⋮----
if (e.getCause() != null) {
if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public DomainEventType<InputEvent> getEventType() {
⋮----
public Aggregate<S> getAggregate() {
⋮----
public boolean isCreate() {
⋮----
public List<DomainEventType<E>> getProducedDomainEventTypes() {
⋮----
public List<DomainEventType<E>> getErrorEventTypes() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/beans/EventSourcingHandlerFunctionAdapter.java
================
public class EventSourcingHandlerFunctionAdapter<S extends AggregateState, E extends DomainEvent> implements EventSourcingHandlerFunction<S, E> {
⋮----
hasPIIDataAnnotation(domainEventClass));
⋮----
public void init() {
⋮----
adapterMethod = aggregate.getClass().getMethod(adapterMethodName, domainEventClass, stateClass);
⋮----
throw new RuntimeException(e);
⋮----
public @NotNull S apply(@NotNull E event, S state) {
⋮----
return (S) adapterMethod.invoke(aggregate, event, state);
⋮----
if (e.getCause() != null) {
if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());
⋮----
public DomainEventType<E> getEventType() {
⋮----
public Aggregate<S> getAggregate() {
⋮----
public boolean isCreate() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartition.java
================
public class AggregatePartition implements Runnable, AutoCloseable, CommandBus {
private static final Logger logger = LoggerFactory.getLogger(AggregatePartition.class);
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
⋮----
this.stateRepository = stateRepositoryFactory.create(runtime, id);
⋮----
this.gdprContextRepository = gdprContextRepositoryFactory.create(runtime.getName(), id);
⋮----
public Integer getId() {
⋮----
public void run() {
⋮----
this.aggregatePartitionThread = Thread.currentThread();
⋮----
AggregatePartitionCommandBus.registerCommandBus(this);
logger.info("Starting AggregatePartition {} of {}Aggregate", id, runtime.getName());
this.consumer = consumerFactory.createConsumer(
runtime.getName() + "Aggregate-partition-" + id,
runtime.getName() + "Aggregate-partition-" + id + "-" + HostUtils.getHostName(),
⋮----
this.producer = producerFactory.createProducer(runtime.getName() + "Aggregate-partition-" + id + "-" + HostUtils.getHostName());
⋮----
externalDomainEventTypes.forEach(domainEventType -> {
String topic = ackesRegistry.resolveTopic(domainEventType);
externalEventPartitions.add(new TopicPartition(topic, id));
⋮----
consumer.assign(Stream.concat(Stream.concat(
Stream.of(commandPartition, domainEventPartition, statePartition), runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty()),
externalEventPartitions.stream())
.toList());
logger.info("Assigned partitions {} for AggregatePartition {} of {}Aggregate", consumer.assignment(), id, runtime.getName());
⋮----
process();
⋮----
logger.info("Shutting down AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
logger.error("Unexpected error in AggregatePartition {} of {}Aggregate", id, runtime.getName(), t);
⋮----
consumer.close(Duration.ofSeconds(5));
producer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing consumer/producer", e);
⋮----
stateRepository.close();
⋮----
logger.error("Error closing state repository", e);
⋮----
gdprContextRepository.close();
⋮----
logger.error("Error closing gdpr context repository", e);
⋮----
AggregatePartitionCommandBus.registerCommandBus(null);
⋮----
logger.info("Finished Shutting down AggregatePartition {} of {}Aggregate", id, runtime.getName());
shutdownLatch.countDown();
⋮----
public void close() throws InterruptedException {
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AggregatePartition={} has been shutdown", id);
⋮----
logger.warn("AggregatePartition={} did not shutdown within 10 seconds", id);
⋮----
public void send(Command command) {
⋮----
if (Thread.currentThread() != aggregatePartitionThread) {
throw new IllegalStateException("send() can only be called from the AggregatePartition thread");
⋮----
CommandType<?> commandType = ackesRegistry.resolveType(command.getClass());
⋮----
runtime.registerAndValidate(commandType);
⋮----
logger.error("Problem registering command {}", commandType.typeName(), e);
⋮----
throw new RuntimeException(e);
⋮----
String topic = ackesRegistry.resolveTopic(commandType);
⋮----
CommandRecord commandRecord = new CommandRecord(
⋮----
commandType.typeName(),
commandType.version(),
runtime.serialize(command),
⋮----
command.getAggregateId(),
⋮----
Integer partition = ackesRegistry.resolvePartition(command.getAggregateId());
KafkaSender.send(producer, new ProducerRecord<>(topic, partition, commandRecord.id(), commandRecord));
⋮----
private void send(ProtocolRecord protocolRecord) {
⋮----
logger.trace("Sending AggregateStateRecord with id {} to {}", asr.aggregateId(), statePartition);
⋮----
Future<RecordMetadata> result = KafkaSender.send(producer, new ProducerRecord<>(statePartition.topic(), statePartition.partition(), asr.aggregateId(), asr));
⋮----
stateRepository.prepare(asr, result);
⋮----
logger.trace("Sending DomainEventRecord {}:{} with id {} to {}", der.name(), der.version(), der.id(), domainEventPartition);
KafkaSender.send(producer, new ProducerRecord<>(domainEventPartition.topic(), domainEventPartition.partition(), der.id(), der));
⋮----
logger.trace("Sending GDPRKeyRecord with id {} to {}", gkr.aggregateId(), gdprKeyPartition);
Future<RecordMetadata> result = KafkaSender.send(producer, new ProducerRecord<>(gdprKeyPartition.topic(), gdprKeyPartition.partition(), gkr.aggregateId(), gkr));
gdprContextRepository.prepare(gkr, result);
⋮----
throw new IllegalArgumentException("""
⋮----
private void index(DomainEventRecord der, IndexParams params) {
// send to the index topic
String topicName = getIndexTopicName(params.indexName(), params.indexKey());
if (params.createIndex()) {
if (consumer.partitionsFor(topicName).isEmpty()) {
if (indexTopicCreator.apply(params.indexName(), params.indexKey())) {
logger.info("Creating DomainEventIndex topic {}", topicName);
⋮----
logger.trace("Indexing DomainEventRecord {}:{} with id {} to topic {}", der.name(), der.version(), der.id(), topicName + "-0");
⋮----
KafkaSender.send(producer, new ProducerRecord<>(topicName, 0, der.id(), der));
⋮----
private void setupGDPRContext(String tenantId, String aggregateId, boolean createIfMissing) {
⋮----
if (!gdprContextRepository.exists(aggregateId) && createIfMissing) {
logger.trace("Generating GDPR key for aggregate {}", aggregateId);
⋮----
GDPRKeyRecord gdprKeyRecord = new GDPRKeyRecord(
⋮----
GDPRKeyUtils.createKey().getEncoded());
⋮----
send(gdprKeyRecord);
⋮----
GDPRContextHolder.setCurrentGDPRContext(gdprContextRepository.get(aggregateId));
⋮----
private void tearDownGDPRContext() {
GDPRContextHolder.resetCurrentGDPRContext();
⋮----
private void handleCommand(CommandRecord commandRecord) {
⋮----
final List<DomainEventRecord> responseRecords = commandRecord.replyToTopicPartition() != null ? new ArrayList<>() : null;
⋮----
send(pr);
⋮----
responseRecords.add(der);
⋮----
if(runtime.requiresGDPRContext(commandRecord)) {
setupGDPRContext(commandRecord.tenantId(), commandRecord.aggregateId(), runtime.shouldGenerateGDPRKey(commandRecord));
⋮----
logger.trace("Handling CommandRecord with type {}", commandRecord.name());
runtime.handleCommandRecord(commandRecord, protocolRecordConsumer, this::index, () -> stateRepository.get(commandRecord.aggregateId()));
⋮----
CommandResponseRecord crr = new CommandResponseRecord(
commandRecord.tenantId(),
commandRecord.aggregateId(),
commandRecord.correlationId(),
commandRecord.id(),
⋮----
getCurrentGDPRContext() != null ? getCurrentGDPRContext().getEncryptionKey() : null);
TopicPartition replyToTopicPartition = PartitionUtils.parseReplyToTopicPartition(commandRecord.replyToTopicPartition());
logger.trace("Sending CommandResponseRecord with commandId {} to {}", crr.commandId(), replyToTopicPartition);
KafkaSender.send(producer, new ProducerRecord<>(replyToTopicPartition.topic(), replyToTopicPartition.partition(), crr.commandId(), crr));
⋮----
logger.error("Error handling command", e);
⋮----
tearDownGDPRContext();
⋮----
private void handleExternalEvent(DomainEventRecord eventRecord) {
⋮----
logger.trace("Handling DomainEventRecord with type {} as External Event", eventRecord.name());
⋮----
if(runtime.requiresGDPRContext(eventRecord)) {
setupGDPRContext(eventRecord.tenantId(), eventRecord.aggregateId(), runtime.shouldGenerateGDPRKey(eventRecord));
⋮----
runtime.handleExternalDomainEventRecord(eventRecord,
⋮----
() -> stateRepository.get(eventRecord.aggregateId()),
⋮----
logger.error("Error handling external event", e);
⋮----
private void process() {
⋮----
ConsumerRecords<String, ProtocolRecord> allRecords = consumer.poll(Duration.ofMillis(10));
if (!allRecords.isEmpty()) {
processRecords(allRecords);
⋮----
ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = consumer.poll(Duration.ofMillis(10));
gdprContextRepository.process(gdprKeyRecords.records(gdprKeyPartition));
⋮----
if (gdprKeyRecords.isEmpty() && initializedEndOffsets.getOrDefault(gdprKeyPartition, 0L) <= consumer.position(gdprKeyPartition)) {
⋮----
if (initializedEndOffsets.getOrDefault(statePartition, 0L) == 0L) {
⋮----
commitInitialOffsetsIfNecessary();
logger.info("No state found in Kafka for AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
consumer.resume(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());
⋮----
logger.info("Loading state for AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
consumer.resume(singletonList(statePartition));
⋮----
consumer.pause(singletonList(gdprKeyPartition));
⋮----
ConsumerRecords<String, ProtocolRecord> stateRecords = consumer.poll(Duration.ofMillis(10));
stateRepository.process(stateRecords.records(statePartition));
⋮----
if (stateRecords.isEmpty() && initializedEndOffsets.getOrDefault(statePartition, 0L) <= consumer.position(statePartition)) {
⋮----
consumer.resume(Stream.concat(
Stream.concat(Stream.of(commandPartition, domainEventPartition),runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty()),
externalEventPartitions.stream()).toList());
⋮----
logger.info(
⋮----
runtime.getName(),
runtime.shouldHandlePIIData() ? "Handle PII Data" : "Not Handle PII Data");
⋮----
long stateRepositoryOffset = stateRepository.getOffset();
⋮----
runtime.getName());
consumer.seek(statePartition, stateRepository.getOffset() + 1);
⋮----
consumer.seekToBeginning(singletonList(statePartition));
⋮----
if(runtime.shouldHandlePIIData()) {
⋮----
long gdprKeyRepositoryOffset = gdprContextRepository.getOffset();
⋮----
consumer.seek(gdprKeyPartition, gdprContextRepository.getOffset() + 1);
⋮----
consumer.seekToBeginning(singletonList(gdprKeyPartition));
⋮----
initializedEndOffsets = consumer.endOffsets(List.of(gdprKeyPartition, statePartition));
logger.info("Loading GDPR Keys for AggregatePartition {} of {}Aggregate", id, runtime.getName());
⋮----
consumer.pause(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());
⋮----
initializedEndOffsets = consumer.endOffsets(List.of(statePartition));
⋮----
logger.error("Fatal error during " + processState + " phase, shutting down AggregatePartition " + id + " of " + runtime.getName() + "Aggregate", e);
⋮----
private void commitInitialOffsetsIfNecessary() {
⋮----
String autoOffsetResetConfig = (String) Optional.ofNullable(consumerFactory.getConfigurationProperties()
.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG)).orElse("latest");
if ("latest".equals(autoOffsetResetConfig)) {
List<TopicPartition> topicPartitions = Stream.concat(Stream.of(commandPartition, domainEventPartition, statePartition), externalEventPartitions.stream()).toList();
final Map<TopicPartition, Long> beginningOffsets = consumer.beginningOffsets(topicPartitions);
final Map<TopicPartition, OffsetAndMetadata> committedOffsets = consumer.committed(new HashSet<>(topicPartitions));
⋮----
committedOffsets.forEach((topicPartition, offsetAndMetadata) -> {
⋮----
logger.info("TopicPartition[{}] has no committed offsets, will commit offset {} to avoid " +
"skipping records", topicPartition, beginningOffsets.getOrDefault(topicPartition, 0L));
uncommittedTopicPartitions.put(topicPartition, new OffsetAndMetadata(beginningOffsets.getOrDefault(topicPartition, 0L)));
⋮----
if (!uncommittedTopicPartitions.isEmpty()) {
producer.beginTransaction();
producer.sendOffsetsToTransaction(uncommittedTopicPartitions, consumer.groupMetadata());
producer.commitTransaction();
⋮----
private void processRecords(ConsumerRecords<String, ProtocolRecord> allRecords) {
⋮----
if (logger.isTraceEnabled()) {
logger.trace("Processing {} records in a single transaction", allRecords.count());
logger.trace("Processing {} gdpr key records", allRecords.records(gdprKeyPartition).size());
logger.trace("Processing {} command records", allRecords.records(commandPartition).size());
if (!externalEventPartitions.isEmpty()) {
logger.trace("Processing {} external event records", externalEventPartitions.stream()
.map(externalEventPartition -> allRecords.records(externalEventPartition).size())
.mapToInt(Integer::intValue).sum());
⋮----
logger.trace("Processing {} state records", allRecords.records(statePartition).size());
logger.trace("Processing {} internal event records", allRecords.records(domainEventPartition).size());
⋮----
List<ConsumerRecord<String, ProtocolRecord>> gdprKeyRecords = allRecords.records(gdprKeyPartition);
if (!gdprKeyRecords.isEmpty()) {
gdprContextRepository.process(gdprKeyRecords);
offsets.put(gdprKeyPartition, gdprKeyRecords.getLast().offset());
⋮----
.forEach(externalEventPartition -> allRecords.records(externalEventPartition)
.forEach(eventRecord -> {
handleExternalEvent((DomainEventRecord) eventRecord.value());
offsets.put(externalEventPartition, eventRecord.offset());
⋮----
allRecords.records(commandPartition)
.forEach(commandRecord -> {
handleCommand((CommandRecord) commandRecord.value());
offsets.put(commandPartition, commandRecord.offset());
⋮----
List<ConsumerRecord<String, ProtocolRecord>> stateRecords = allRecords.records(statePartition);
if (!stateRecords.isEmpty()) {
stateRepository.process(stateRecords);
offsets.put(statePartition, stateRecords.get(stateRecords.size() - 1).offset());
⋮----
allRecords.records(domainEventPartition)
.forEach(domainEventRecord -> offsets.put(domainEventPartition, domainEventRecord.offset()));
⋮----
producer.sendOffsetsToTransaction(offsets.entrySet().stream()
.collect(Collectors.toMap(Map.Entry::getKey, e -> new OffsetAndMetadata(e.getValue() + 1))),
consumer.groupMetadata());
⋮----
stateRepository.commit();
⋮----
gdprContextRepository.commit();
⋮----
producer.abortTransaction();
rollbackConsumer(allRecords);
stateRepository.rollback();
gdprContextRepository.rollback();
⋮----
private void rollbackConsumer(ConsumerRecords<String, ProtocolRecord> consumerRecords) {
consumerRecords.partitions().forEach(topicPartition -> {
⋮----
consumerRecords.records(topicPartition).stream().map(ConsumerRecord::offset).min(Long::compareTo)
.ifPresent(offset -> consumer.seek(topicPartition, offset));
⋮----
public boolean isProcessing() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartitionCommandBus.java
================
class AggregatePartitionCommandBus extends CommandBusHolder {
static void registerCommandBus(AggregatePartition aggregatePartition) {
commandBusThreadLocal.set(aggregatePartition);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartitionState.java
================


================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregateRuntimeFactory.java
================
public class AggregateRuntimeFactory<S extends AggregateState> implements FactoryBean<AggregateRuntime>, ApplicationContextAware {
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
⋮----
public AggregateRuntime getObject() throws Exception {
return createRuntime(aggregate);
⋮----
public Class<?> getObjectType() {
⋮----
private KafkaAggregateRuntime createRuntime(Aggregate<S> aggregate) {
⋮----
AggregateInfo aggregateInfo = aggregate.getClass().getAnnotation(AggregateInfo.class);
⋮----
runtimeBuilder.setStateType(new AggregateStateType<>(
aggregateInfo.value(),
aggregateInfo.version(),
aggregate.getStateClass(),
aggregateInfo.generateGDPRKeyOnCreate(),
aggregateInfo.indexed(),
aggregateInfo.indexName(),
hasPIIDataAnnotation(aggregate.getStateClass())
⋮----
throw new IllegalStateException("Class implementing Aggregate must be annotated with @AggregateInfo");
⋮----
.setAggregateClass(aggregate.getClass())
.setObjectMapper(objectMapper)
.setGenerateGDPRKeyOnCreate(aggregateInfo.generateGDPRKeyOnCreate());
⋮----
applicationContext.getBeansOfType(CommandHandlerFunction.class).values().stream()
⋮----
.filter(adapter -> adapter.getAggregate().equals(aggregate))
.forEach(adapter -> {
CommandType<?> type = adapter.getCommandType();
if (adapter.isCreate()) {
⋮----
.setCommandCreateHandler(adapter)
.addCommand(type);
⋮----
.addCommandHandler(type, adapter)
⋮----
for (Object producedDomainEventType : adapter.getProducedDomainEventTypes()) {
runtimeBuilder.addDomainEvent((DomainEventType<?>) producedDomainEventType);
⋮----
for (Object errorEventType : adapter.getErrorEventTypes()) {
runtimeBuilder.addDomainEvent((DomainEventType<?>) errorEventType);
⋮----
applicationContext.getBeansOfType(EventHandlerFunction.class).values().stream()
⋮----
DomainEventType<?> type = adapter.getEventType();
⋮----
.setEventCreateHandler(adapter)
.addDomainEvent(type);
⋮----
.addExternalEventHandler(type, adapter)
⋮----
applicationContext.getBeansOfType(EventSourcingHandlerFunction.class).values().stream()
⋮----
.setEventSourcingCreateHandler(adapter)
⋮----
.addEventSourcingHandler(type, adapter)
⋮----
applicationContext.getBeansOfType(EventBridgeHandlerFunction.class).values().stream()
⋮----
.addEventBridgeHandler(type, adapter)
⋮----
return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/KafkaAggregateRuntime.java
================
public class KafkaAggregateRuntime implements AggregateRuntime {
private static final Logger log = LoggerFactory.getLogger(KafkaAggregateRuntime.class);
⋮----
public String getName() {
return type.typeName();
⋮----
public Class<? extends Aggregate> getAggregateClass() {
⋮----
public void handleCommandRecord(CommandRecord commandRecord,
⋮----
CommandType<?> commandType = getCommandType(commandRecord);
⋮----
if (commandType.create()) {
⋮----
if (stateRecordSupplier.get() != null) {
log.warn("Command {} wants to create a {} Aggregate with id {}, but the state already exists. Generating a AggregateAlreadyExistsError",
commandRecord.name(),
getName(),
commandRecord.aggregateId());
aggregateAlreadyExists(commandRecord, protocolRecordConsumer);
⋮----
handleCreateCommand(commandType, commandRecord, protocolRecordConsumer, domainEventIndexer);
⋮----
if (commandHandlers.containsKey(commandType)) {
handleCommand(commandType, commandRecord, protocolRecordConsumer, domainEventIndexer, stateRecordSupplier);
⋮----
commandExecutionError(commandRecord, protocolRecordConsumer, "No handler found for command " + commandRecord.name());
⋮----
log.error("Exception while handling command, sending CommandExecutionError", t);
commandExecutionError(commandRecord, protocolRecordConsumer, t);
⋮----
private void aggregateAlreadyExists(ProtocolRecord commandOrDomainEventRecord,
⋮----
AggregateAlreadyExistsErrorEvent errorEvent = new AggregateAlreadyExistsErrorEvent(commandOrDomainEventRecord.aggregateId(), this.getName());
DomainEventType<?> type = getDomainEventType(AggregateAlreadyExistsErrorEvent.class);
DomainEventRecord eventRecord = new DomainEventRecord(
commandOrDomainEventRecord.tenantId(),
type.typeName(),
type.version(),
serialize(errorEvent),
getEncoding(type),
errorEvent.getAggregateId(),
commandOrDomainEventRecord.correlationId(),
⋮----
protocolRecordConsumer.accept(eventRecord);
⋮----
private void commandExecutionError(CommandRecord commandRecord,
⋮----
commandExecutionError(commandRecord, protocolRecordConsumer, exception.getMessage());
⋮----
CommandExecutionErrorEvent errorEvent = new CommandExecutionErrorEvent(
commandRecord.aggregateId(),
this.getName(),
⋮----
commandRecord.tenantId(),
⋮----
commandRecord.correlationId(),
⋮----
private CommandType<?> getCommandType(CommandRecord commandRecord) {
CommandType<?> commandType = commandTypes.getOrDefault(commandRecord.name(), emptyList()).stream()
.filter(ct -> ct.version() == commandRecord.version())
.findFirst().orElseThrow(RuntimeException::new);
⋮----
private void indexDomainEventIfRequired(DomainEventRecord domainEventRecord,
⋮----
if (type.indexed()) {
domainEventIndexer.accept(domainEventRecord, new IndexParams(type.indexName(), state.getIndexKey(), createIndex));
⋮----
private void handleCreateCommand(CommandType<?> commandType,
⋮----
Command command = materialize(commandType, commandRecord);
⋮----
Stream<DomainEvent> domainEvents = commandCreateHandler.apply(command, null);
⋮----
Iterator<DomainEvent> itr = domainEvents.iterator();
DomainEvent domainEvent = itr.next();
⋮----
AggregateState state = createStateHandler.apply(domainEvent, null);
⋮----
AggregateStateRecord stateRecord = new AggregateStateRecord(
⋮----
serialize(state),
⋮----
state.getAggregateId(),
⋮----
protocolRecordConsumer.accept(stateRecord);
⋮----
DomainEventType<?> type = getDomainEventType(domainEvent.getClass());
⋮----
serialize(domainEvent),
⋮----
domainEvent.getAggregateId(),
⋮----
stateRecord.generation());
⋮----
indexDomainEventIfRequired(eventRecord, state, domainEventIndexer, true);
⋮----
while (itr.hasNext()) {
DomainEvent nextDomainEvent = itr.next();
currentStateRecord = processDomainEvent(
⋮----
private void handleCommand(CommandType<?> commandType,
⋮----
AggregateStateRecord currentStateRecord = stateRecordSupplier.get();
AggregateState currentState = materialize(currentStateRecord);
Stream<DomainEvent> domainEvents = commandHandlers.get(commandType).apply(command, currentState);
for (DomainEvent domainEvent : domainEvents.toList())
currentStateRecord = processDomainEvent(commandRecord.correlationId(),
⋮----
private void handleCreateEvent(DomainEventType<?> eventType,
⋮----
DomainEvent externalEvent = materialize(eventType, domainEventRecord);
⋮----
Stream<DomainEvent> domainEvents = eventCreateHandler.apply(externalEvent, null);
⋮----
domainEventRecord.tenantId(),
⋮----
domainEventRecord.correlationId(),
⋮----
private void handleEvent(DomainEventType<?> eventType,
⋮----
Stream<DomainEvent> domainEvents = eventHandlers.get(eventType).apply(externalEvent, currentState);
⋮----
private void handleBridgedEvent(DomainEventType<?> eventType,
⋮----
eventBridgeHandlers.get(eventType).apply(externalEvent, commandBus);
⋮----
private AggregateStateRecord processDomainEvent(String correlationId,
⋮----
DomainEventType<?> domainEventType = getDomainEventType(domainEvent.getClass());
⋮----
AggregateState nextState = eventSourcingHandlers.get(domainEventType).apply(domainEvent, currentState);
⋮----
AggregateStateRecord nextStateRecord = new AggregateStateRecord(
currentStateRecord.tenantId(),
⋮----
serialize(nextState),
⋮----
currentStateRecord.aggregateId(),
⋮----
currentStateRecord.generation() + 1L);
protocolRecordConsumer.accept(nextStateRecord);
⋮----
domainEventType.typeName(),
domainEventType.version(),
⋮----
getEncoding(domainEventType),
⋮----
nextStateRecord.generation());
⋮----
indexDomainEventIfRequired(eventRecord, nextState, domainEventIndexer, false);
⋮----
public void handleExternalDomainEventRecord(DomainEventRecord eventRecord,
⋮----
DomainEventType<?> domainEventType = getDomainEventType(eventRecord);
⋮----
if (eventCreateHandler != null && eventCreateHandler.getEventType().equals(domainEventType)) {
⋮----
log.warn("External DomainEvent {} wants to create a {} Aggregate with id {}, but the state already exists. Generate a AggregateAlreadyExistsError",
eventRecord.name(),
⋮----
eventRecord.aggregateId());
aggregateAlreadyExists(eventRecord, protocolRecordConsumer);
⋮----
handleCreateEvent(domainEventType, eventRecord, protocolRecordConsumer, domainEventIndexer);
⋮----
if (eventHandlers.containsKey(domainEventType)) {
handleEvent(domainEventType, eventRecord, protocolRecordConsumer, domainEventIndexer, stateRecordSupplier);
} else if(eventBridgeHandlers.containsKey(domainEventType)) {
handleBridgedEvent(domainEventType, eventRecord, commandBus);
⋮----
private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {
⋮----
return domainEvents.entrySet().stream()
.filter(entry -> entry.getValue().external())
.filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
.filter(entry -> entry.getValue().version() <= eventRecord.version())
.max(Comparator.comparingInt(entry -> entry.getValue().version()))
.map(Map.Entry::getValue).orElse(null);
⋮----
public Collection<DomainEventType<?>> getAllDomainEventTypes() {
return this.domainEvents.values();
⋮----
public Collection<DomainEventType<?>> getProducedDomainEventTypes() {
return this.domainEvents.values().stream().filter(domainEventType -> !domainEventType.external()).collect(Collectors.toSet());
⋮----
public Collection<DomainEventType<?>> getExternalDomainEventTypes() {
return this.domainEvents.values().stream().filter(DomainEventType::external).collect(Collectors.toSet());
⋮----
public Collection<CommandType<?>> getAllCommandTypes() {
return this.commandTypes.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());
⋮----
public Collection<CommandType<?>> getLocalCommandTypes() {
return this.commandTypes.values().stream().flatMap(Collection::stream).filter(commandType -> !commandType.external()).collect(Collectors.toSet());
⋮----
public Collection<CommandType<?>> getExternalCommandTypes() {
return this.commandTypes.values().stream().flatMap(Collection::stream).filter(CommandType::external).collect(Collectors.toSet());
⋮----
private DomainEventType<?> getDomainEventType(Class<?> domainEventClass) {
return domainEvents.get(domainEventClass);
⋮----
public CommandType<?> getLocalCommandType(String type, int version) {
return commandTypes.getOrDefault(type, Collections.emptyList()).stream()
.filter(commandType -> commandType.version() == version)
.findFirst()
.orElseThrow(() -> new IllegalArgumentException("No CommandType found for type " + type + " and version " + version));
⋮----
public boolean shouldGenerateGDPRKey(CommandRecord commandRecord) {
return getCommandType(commandRecord).create() && generateGDPRKeyOnCreate;
⋮----
public boolean shouldGenerateGDPRKey(DomainEventRecord eventRecord) {
return Optional.ofNullable(getDomainEventType(eventRecord))
.map(domainEventType -> domainEventType.create() && generateGDPRKeyOnCreate).orElse(false);
⋮----
public boolean requiresGDPRContext(DomainEventRecord eventRecord) {
⋮----
return domainEventType.piiData() || (!eventBridgeHandlers.containsKey(domainEventType) && type.piiData());
⋮----
public boolean requiresGDPRContext(CommandRecord commandRecord) {
return this.type.piiData() || getCommandType(commandRecord).piiData();
⋮----
public boolean shouldHandlePIIData() {
⋮----
public void registerAndValidate(DomainEventType<?> domainEventType, boolean forceRegisterOnIncompatible) throws SchemaException {
⋮----
JsonSchema localSchema = schemaRegistry.registerAndValidate(domainEventType, forceRegisterOnIncompatible);
⋮----
domainEventSchemas.put(domainEventType.typeClass(), localSchema);
⋮----
public void registerAndValidate(CommandType<?> commandType,  boolean forceRegisterOnIncompatible) throws SchemaException {
if (!commandSchemas.containsKey(commandType.typeClass())) {
JsonSchema localSchema = schemaRegistry.registerAndValidate(commandType, forceRegisterOnIncompatible);
commandSchemas.put(commandType.typeClass(), localSchema);
if (commandType.external()) addCommand(commandType);
⋮----
public Command materialize(CommandType<?> type, CommandRecord commandRecord) throws IOException {
return objectMapper.readValue(commandRecord.payload(), type.typeClass());
⋮----
private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
⋮----
private AggregateState materialize(AggregateStateRecord stateRecord) throws IOException {
return objectMapper.readValue(stateRecord.payload(), getAggregateStateType(stateRecord).typeClass());
⋮----
private byte[] serialize(AggregateState state) throws IOException {
return objectMapper.writeValueAsBytes(state);
⋮----
private byte[] serialize(DomainEvent domainEvent) throws SerializationException {
JsonNode jsonNode = objectMapper.convertValue(domainEvent, JsonNode.class);
⋮----
domainEventSchemas.get(domainEvent.getClass()).validate(jsonNode);
return objectMapper.writeValueAsBytes(jsonNode);
⋮----
throw new SerializationException("Validation Failed while Serializing DomainEventClass " + domainEvent.getClass().getName(), e);
⋮----
throw new SerializationException("Serialization Failed while Serializing DomainEventClass " + domainEvent.getClass().getName(), e);
⋮----
public byte[] serialize(Command command) throws SerializationException {
JsonNode jsonNode = objectMapper.convertValue(command, JsonNode.class);
⋮----
commandSchemas.get(command.getClass()).validate(jsonNode);
return objectMapper.writeValueAsBytes(command);
⋮----
throw new SerializationException("Validation Failed while Serializing CommandClass " + command.getClass().getName(), e);
⋮----
throw new SerializationException("Serialization Failed while Serializing CommandClass " + command.getClass().getName(), e);
⋮----
private PayloadEncoding getEncoding(CommandType<?> type) {
⋮----
private PayloadEncoding getEncoding(DomainEventType<?> type) {
⋮----
private PayloadEncoding getEncoding(AggregateStateType<?> type) {
⋮----
private AggregateStateType<?> getAggregateStateType(AggregateStateRecord record) {
⋮----
private void addCommand(CommandType<?> commandType) {
this.commandTypes.computeIfAbsent(commandType.typeName(), typeName -> new ArrayList<>()).add(commandType);
⋮----
public static class Builder {
⋮----
public Builder setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
⋮----
public Builder setObjectMapper(ObjectMapper objectMapper) {
⋮----
public Builder setStateType(AggregateStateType<?> stateType) {
⋮----
public Builder setAggregateClass(Class<? extends Aggregate> aggregateClass) {
⋮----
public Builder setCommandCreateHandler(CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler) {
⋮----
public Builder setEventCreateHandler(EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler) {
⋮----
public Builder setEventSourcingCreateHandler(EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler) {
⋮----
public Builder addDomainEvent(DomainEventType<?> domainEvent) {
this.domainEvents.put(domainEvent.typeClass(), domainEvent);
⋮----
public Builder addCommand(CommandType<?> commandType) {
this.commandTypes.computeIfAbsent(commandType.typeName(), s -> new ArrayList<>()).add(commandType);
⋮----
public Builder addCommandHandler(CommandType<?> commandType, CommandHandlerFunction<AggregateState, Command, DomainEvent> commandHandler) {
this.commandHandlers.put(commandType, commandHandler);
⋮----
public Builder addExternalEventHandler(DomainEventType<?> eventType, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventHandler) {
this.eventHandlers.put(eventType, eventHandler);
⋮----
public Builder addEventSourcingHandler(DomainEventType<?> eventType, EventSourcingHandlerFunction<AggregateState, DomainEvent> eventSourcingHandler) {
this.eventSourcingHandlers.put(eventType, eventSourcingHandler);
⋮----
public Builder addEventBridgeHandler(DomainEventType<?> eventType, EventBridgeHandlerFunction<AggregateState, DomainEvent> eventBridgeHandler) {
this.eventBridgeHandlers.put(eventType, eventBridgeHandler);
⋮----
public Builder setGenerateGDPRKeyOnCreate(boolean generateGDPRKeyOnCreate) {
⋮----
public KafkaAggregateRuntime build() {
⋮----
final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation) ||
commandTypes.values().stream().flatMap(List::stream).map(CommandType::typeClass)
.anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
⋮----
return new KafkaAggregateRuntime(

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/kafka/PartitionUtils.java
================
public final class PartitionUtils {
⋮----
public static Map<Integer, AggregatePartition> toAggregatePartitions(Collection<TopicPartition> topicPartitions) {
Set<Integer> partitions = topicPartitions.stream().map(TopicPartition::partition).collect(Collectors.toSet());
⋮----
partitions.forEach(partition -> {
Set<TopicPartition> aggregatePartition = topicPartitions.stream().filter(topicPartition ->
topicPartition.partition() == partition &&
(topicPartition.topic().endsWith(COMMANDS_SUFFIX) ||
topicPartition.topic().endsWith(DOMAINEVENTS_SUFFIX) ||
topicPartition.topic().endsWith(AGGREGRATESTATE_SUFFIX))).collect(Collectors.toSet());
if (aggregatePartition.size() == 3) {
TopicPartition command = aggregatePartition.stream().filter(topicPartition ->
topicPartition.topic().endsWith(COMMANDS_SUFFIX)).findFirst().orElseThrow();
TopicPartition domainEvent = aggregatePartition.stream().filter(topicPartition ->
topicPartition.topic().endsWith(DOMAINEVENTS_SUFFIX)).findFirst().orElseThrow();
TopicPartition aggregateState = aggregatePartition.stream().filter(topicPartition ->
topicPartition.topic().endsWith(AGGREGRATESTATE_SUFFIX)).findFirst().orElseThrow();
⋮----
throw new NoSuchElementException("Partition " + partition + " is incomplete, found " + aggregatePartition);
⋮----
public static TopicPartition toCommandTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition(aggregate.getName() + COMMANDS_SUFFIX, partition);
⋮----
public static TopicPartition toDomainEventTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition(aggregate.getName() + DOMAINEVENTS_SUFFIX, partition);
⋮----
public static TopicPartition toAggregateStateTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition(aggregate.getName() + AGGREGRATESTATE_SUFFIX, partition);
⋮----
public static TopicPartition toGDPRKeysTopicPartition(AggregateRuntime aggregate, int partition) {
return new TopicPartition("Akces-GDPRKeys", partition);
⋮----
public static List<TopicPartition> toExternalDomainEventTopicPartitions(AggregateRuntime aggregate, int partition) {
return aggregate.getExternalDomainEventTypes().stream().map(externalDomainEvent ->
new TopicPartition(externalDomainEvent.typeName() + DOMAINEVENTS_SUFFIX, partition)).collect(Collectors.toList());
⋮----
public static TopicPartition parseReplyToTopicPartition(String replyTo) {
⋮----
int lastIndex = replyTo.lastIndexOf('-');
String topic = replyTo.substring(0, lastIndex);
int partition = Integer.parseInt(replyTo.substring(lastIndex + 1));
return new TopicPartition(topic, partition);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepository.java
================
public interface AggregateStateRepository extends Closeable {
default long getOffset() {
⋮----
void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture);
⋮----
void commit();
⋮----
void rollback();
⋮----
void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords);
⋮----
AggregateStateRecord get(String aggregateId);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepositoryException.java
================
public class AggregateStateRepositoryException extends RuntimeException {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepositoryFactory.java
================
public interface AggregateStateRepositoryFactory {
AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/InMemoryAggregateStateRepository.java
================
public class InMemoryAggregateStateRepository implements AggregateStateRepository {
private static final Logger log = LoggerFactory.getLogger(InMemoryAggregateStateRepository.class);
⋮----
public void close() {
stateRecordMap.clear();
transactionStateRecordMap.clear();
⋮----
public void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture) {
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata(record, recordMetadataFuture));
⋮----
public void commit() {
⋮----
if (!transactionStateRecordMap.isEmpty()) {
⋮----
this.offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
log.trace("Committing {} records and offset {}", transactionStateRecordMap.size(), this.offset);
transactionStateRecordMap.values().forEach(recordAndMetadata -> stateRecordMap.put(recordAndMetadata.record().aggregateId(), recordAndMetadata.record()));
⋮----
public void rollback() {
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
AggregateStateRecord record = (AggregateStateRecord) consumerRecord.value();
⋮----
stateRecordMap.put(record.aggregateId(), record);
⋮----
stateRecordMap.remove(consumerRecord.key());
⋮----
this.offset = consumerRecord.offset();
⋮----
public AggregateStateRecord get(String aggregateId) {
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
return stateRecordMap.get(aggregateId);
⋮----
public long getOffset() {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/InMemoryAggregateStateRepositoryFactory.java
================
public class InMemoryAggregateStateRepositoryFactory implements AggregateStateRepositoryFactory {
⋮----
public AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId) {
return new InMemoryAggregateStateRepository();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/RocksDBAggregateStateRepository.java
================
public class RocksDBAggregateStateRepository implements AggregateStateRepository {
private static final Logger log = LoggerFactory.getLogger(RocksDBAggregateStateRepository.class);
⋮----
RocksDB.loadLibrary();
final Options options = new Options();
final TransactionDBOptions transactionDBOptions = new TransactionDBOptions();
options.setCreateIfMissing(true);
⋮----
this.rocksDBDataDir = new File(baseDir, partitionId);
⋮----
Files.createDirectories(this.rocksDBDataDir.getParentFile().toPath());
Files.createDirectories(this.rocksDBDataDir.getAbsoluteFile().toPath());
db = TransactionDB.open(options, transactionDBOptions, this.rocksDBDataDir.getAbsolutePath());
⋮----
initializeOffset();
log.info("RocksDB for partition {} initialized in folder {}", partitionId, this.rocksDBDataDir.getAbsolutePath());
⋮----
throw new AggregateStateRepositoryException("Error initializing RocksDB", e);
⋮----
public void close() {
⋮----
db.syncWal();
⋮----
log.error("Error syncing WAL. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
db.close();
⋮----
private void initializeOffset() {
⋮----
byte[] offsetBytes = db.get(OFFSET);
⋮----
lastOffset = Longs.fromByteArray(offsetBytes);
⋮----
throw new AggregateStateRepositoryException("Error initializing offset", e);
⋮----
private void updateOffset(long offset) {
⋮----
log.trace("Updated offset to {}", offset);
⋮----
public long getOffset() {
⋮----
public void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture) {
checkAggregateIdType(record.aggregateId());
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata(record, recordMetadataFuture));
⋮----
public void commit() {
if (!transactionStateRecordMap.isEmpty()) {
⋮----
Transaction transaction = db.beginTransaction(new WriteOptions());
⋮----
for (RecordAndMetadata recordAndMetadata : transactionStateRecordMap.values()) {
transaction.put(keyBytes(recordAndMetadata.record().aggregateId()), serializer.serialize(topicName, recordAndMetadata.record()));
⋮----
long offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
transaction.put(OFFSET, Longs.toByteArray(offset));
transaction.commit();
transaction.close();
updateOffset(offset);
⋮----
throw new AggregateStateRepositoryException("Error committing records", e);
⋮----
transactionStateRecordMap.clear();
⋮----
public void rollback() {
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
long offset = consumerRecords.stream()
.map(ConsumerRecord::offset)
⋮----
transaction.put(keyBytes(consumerRecord.key()), serializer.serialize(topicName, consumerRecord.value()));
⋮----
throw new AggregateStateRepositoryException("Error processing records", e);
⋮----
public AggregateStateRecord get(String aggregateId) {
checkAggregateIdType(aggregateId);
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
byte[] keyBytes = keyBytes(aggregateId);
if (db.keyExists(keyBytes)) {
⋮----
return (AggregateStateRecord) deserializer.deserialize(topicName, db.get(keyBytes));
⋮----
throw new AggregateStateRepositoryException("Problem reading record with aggregateId " + aggregateId, e);
⋮----
private byte[] keyBytes(String aggregateId) {
⋮----
UUID aggregateUUID = UUID.fromString(aggregateId);
return ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array();
⋮----
return aggregateId.getBytes(Charsets.UTF_8);
⋮----
private void checkAggregateIdType(String aggregateId) {
⋮----
UUID.fromString(aggregateId);

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/state/RocksDBAggregateStateRepositoryFactory.java
================
public class RocksDBAggregateStateRepositoryFactory implements AggregateStateRepositoryFactory {
⋮----
public AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId) {
return new RocksDBAggregateStateRepository(
⋮----
aggregateRuntime.getName() + "-AggregateState-" + partitionId.toString(),
aggregateRuntime.getName() + "-AggregateState",
serde.serializer(),
serde.deserializer());

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/AggregateServiceApplication.java
================
public class AggregateServiceApplication {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
⋮----
public static void main(String[] args) {
SpringApplication application = new SpringApplication(AggregateServiceApplication.class);
⋮----
application.setSources(Set.of(args));
⋮----
application.run();
⋮----
public static AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
return new AggregateBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public AkcesControlRecordSerde akcesControlRecordSerde(ObjectMapper objectMapper) {
return new AkcesControlRecordSerde(objectMapper);
⋮----
public KafkaAdmin kafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
⋮----
public SchemaRegistryClient schemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
⋮----
public KafkaSchemaRegistry schemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient,
⋮----
return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
⋮----
public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
⋮----
public ProducerFactory<String, ProtocolRecord> producerFactory(KafkaProperties properties) {
return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), serde.serializer());
⋮----
public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
⋮----
public ProducerFactory<String, AkcesControlRecord> controlProducerFactory(KafkaProperties properties,
⋮----
return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), controlSerde.serializer());
⋮----
public AggregateStateRepositoryFactory aggregateStateRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBAggregateStateRepositoryFactory(serde, baseDir);
⋮----
public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
⋮----
public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
return new EnvironmentPropertiesPrinter();

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/AkcesAggregateController.java
================
public class AkcesAggregateController extends Thread implements AutoCloseable, ConsumerRebalanceListener, AkcesRegistry, EnvironmentAware, ApplicationContextAware {
private static final Logger logger = LoggerFactory.getLogger(AkcesAggregateController.class);
⋮----
private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
⋮----
private final CountDownLatch shutdownLatch = new CountDownLatch(1);
⋮----
super(aggregateRuntime.getName() + "-AkcesController");
⋮----
this.executorService = Executors.newCachedThreadPool(new CustomizableThreadFactory(aggregateRuntime.getName() + "AggregatePartitionThread-"));
⋮----
public void run() {
⋮----
controlRecordConsumerFactory.createConsumer(
aggregateRuntime.getName() + "-Akces-Control",
aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-Akces-Control",
⋮----
controlConsumer.subscribe(List.of("Akces-Control"), this);
⋮----
process();
⋮----
logger.info("Closing {} AggregatePartitions", aggregatePartitions.size());
aggregatePartitions.values().forEach(aggregatePartition -> {
⋮----
aggregatePartition.close();
⋮----
logger.error("Error closing AggregatePartition " + aggregatePartition.getId(), e);
⋮----
controlConsumer.close(Duration.ofSeconds(5));
⋮----
logger.error("Error closing controlConsumer", e);
⋮----
applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
⋮----
shutdownLatch.countDown();
⋮----
logger.error("Error in AkcesController", e);
⋮----
private void process() {
⋮----
processControlRecords();
⋮----
logger.error("Unrecoverable exception in AkcesController", e);
⋮----
TopicDescription controlTopicDescription = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control");
partitions = controlTopicDescription.partitions().size();
replicationFactor = (short) controlTopicDescription.partitions().getFirst().replicas().size();
⋮----
for (DomainEventType<?> domainEventType : aggregateRuntime.getProducedDomainEventTypes()) {
⋮----
aggregateRuntime.registerAndValidate(domainEventType);
⋮----
if (protocolRecordTypeNotYetProduced(domainEventType, PartitionUtils::toDomainEventTopicPartition)) {
aggregateRuntime.registerAndValidate(domainEventType, true);
⋮----
logger.warn("Cannot update schema for DomainEvent {} v{} because it has already been produced",
domainEventType.typeName(), domainEventType.version());
⋮----
for (CommandType<?> commandType : aggregateRuntime.getLocalCommandTypes()) {
⋮----
aggregateRuntime.registerAndValidate(commandType);
⋮----
if (protocolRecordTypeNotYetProduced(commandType, PartitionUtils::toCommandTopicPartition)) {
aggregateRuntime.registerAndValidate(commandType, true);
⋮----
logger.warn("Cannot update schema for Command {} v{} because it has already been produced",
commandType.typeName(), commandType.version());
⋮----
publishControlRecord(partitions);
⋮----
if (!partitionsToAssign.isEmpty()) {
⋮----
controlConsumer.seekToBeginning(partitionsToAssign);
⋮----
Map<TopicPartition, Long> initializedEndOffsets = controlConsumer.endOffsets(partitionsToAssign);
⋮----
ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
while (!initializedEndOffsets.isEmpty()) {
consumerRecords.forEach(record -> {
AkcesControlRecord controlRecord = record.value();
⋮----
if (aggregateServices.putIfAbsent(record.key(), aggregateServiceRecord) == null) {
logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
⋮----
logger.info("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
⋮----
if (consumerRecords.isEmpty()) {
initializedEndOffsets.entrySet().removeIf(entry -> entry.getValue() <= controlConsumer.position(entry.getKey()));
⋮----
consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
⋮----
for (DomainEventType<?> domainEventType : aggregateRuntime.getExternalDomainEventTypes()) {
⋮----
logger.error("Error registering external domain event type: {}:{}", domainEventType.typeName(), domainEventType.version(), e);
⋮----
for (CommandType<?> commandType : aggregateRuntime.getExternalCommandTypes()) {
⋮----
logger.error("Error registering external command type: {}:{}", commandType.typeName(), commandType.version(), e);
⋮----
AggregatePartition aggregatePartition = aggregatePartitions.remove(topicPartition.partition());
⋮----
logger.info("Stopping AggregatePartition {}", aggregatePartition.getId());
⋮----
logger.error("Error closing AggregatePartition", e);
⋮----
partitionsToRevoke.clear();
⋮----
AggregatePartition aggregatePartition = new AggregatePartition(
⋮----
topicPartition.partition(),
toCommandTopicPartition(aggregateRuntime, topicPartition.partition()),
toDomainEventTopicPartition(aggregateRuntime, topicPartition.partition()),
toAggregateStateTopicPartition(aggregateRuntime, topicPartition.partition()),
toGDPRKeysTopicPartition(aggregateRuntime, topicPartition.partition()),
aggregateRuntime.getExternalDomainEventTypes(),
⋮----
aggregatePartitions.put(aggregatePartition.getId(), aggregatePartition);
logger.info("Starting AggregatePartition {}", aggregatePartition.getId());
executorService.submit(aggregatePartition);
⋮----
partitionsToAssign.clear();
⋮----
private boolean protocolRecordTypeNotYetProduced(SchemaType schemaType,
⋮----
try (Consumer<String, ProtocolRecord> consumer = consumerFactory.createConsumer(
aggregateRuntime.getName() + "-Akces-Control-TypeCheck",
aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-Akces-Control-TypeCheck",
⋮----
List<TopicPartition> partitionsList = IntStream.range(0, partitions)
.mapToObj(i -> createTopicPartition.apply(aggregateRuntime, i))
.toList();
consumer.assign(partitionsList);
consumer.seekToBeginning(partitionsList);
⋮----
Map<TopicPartition, Long> endOffsets = consumer.endOffsets(partitionsList);
while (!endOffsets.isEmpty()) {
⋮----
for (ConsumerRecord<String, ProtocolRecord> record : consumer.poll(Duration.ofMillis(10))) {
if (record.value().name().equals(schemaType.typeName()) &&
record.value().version() == schemaType.version()) {
⋮----
endOffsets.entrySet().removeIf(entry -> entry.getValue() <= consumer.position(entry.getKey()));
⋮----
logger.error(
⋮----
schemaType.typeName(),
schemaType.version(),
⋮----
private void processControlRecords() {
⋮----
if (!consumerRecords.isEmpty()) {
⋮----
if (!aggregateServices.containsKey(record.key())) {
⋮----
aggregateServices.put(record.key(), aggregateServiceRecord);
⋮----
private Boolean createIndexTopic(String indexName, String indexKey) {
⋮----
kafkaAdmin.createOrModifyTopics(
createCompactedTopic(getIndexTopicName(indexName, indexKey), 1, replicationFactor));
⋮----
logger.error("Error creating index topic: {}", indexName, e);
⋮----
private void publishControlRecord(int partitions) {
String transactionalId = aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-control";
try (Producer<String, AkcesControlRecord> controlProducer = controlProducerFactory.createProducer(transactionalId)) {
⋮----
AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
aggregateRuntime.getName(),
aggregateRuntime.getName() + COMMANDS_SUFFIX,
aggregateRuntime.getName() + DOMAINEVENTS_SUFFIX,
aggregateRuntime.getAllCommandTypes().stream()
.map(commandType ->
new AggregateServiceCommandType(
commandType.typeName(),
commandType.version(),
commandType.create(),
"commands." + commandType.typeName())).toList(),
aggregateRuntime.getProducedDomainEventTypes().stream().map(domainEventType ->
new AggregateServiceDomainEventType(
domainEventType.typeName(),
domainEventType.version(),
domainEventType.create(),
domainEventType.external(),
"domainevents." + domainEventType.typeName())).toList(),
aggregateRuntime.getExternalDomainEventTypes().stream().map(externalDomainEventType ->
⋮----
externalDomainEventType.typeName(),
externalDomainEventType.version(),
externalDomainEventType.create(),
externalDomainEventType.external(),
"domainevents." + externalDomainEventType.typeName())).toList());
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, aggregateRuntime.getName(), aggregateServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
logger.error("Error publishing CommandServiceRecord", e);
⋮----
public void close() throws Exception {
logger.info("Shutting down AkcesAggregateController");
⋮----
if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
logger.info("AkcesAggregateController has been shutdown");
⋮----
logger.warn("AkcesAggregateController did not shutdown within 10 seconds");
⋮----
public void onPartitionsRevoked(Collection<TopicPartition> topicPartitions) {
⋮----
if (!topicPartitions.isEmpty()) {
⋮----
partitionsToRevoke.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, revoking partitions: {}",
topicPartitions.stream().map(TopicPartition::partition).toList());
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, revoking partitions: {}",
⋮----
public void onPartitionsAssigned(Collection<TopicPartition> topicPartitions) {
⋮----
partitionsToAssign.addAll(topicPartitions);
⋮----
logger.info("Switching from RUNNING to REBALANCING, assigning partitions : {}",
⋮----
logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, assigning partitions : {}",
⋮----
public CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass) {
⋮----
CommandInfo commandInfo = commandClass.getAnnotation(CommandInfo.class);
⋮----
List<AggregateServiceRecord> services = aggregateServices.values().stream()
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandInfo))
⋮----
if (services.size() == 1) {
AggregateServiceRecord aggregateServiceRecord = services.get(0);
if (aggregateRuntime.getName().equals(aggregateServiceRecord.aggregateName())) {
⋮----
return aggregateRuntime.getLocalCommandType(commandInfo.type(), commandInfo.version());
⋮----
return new CommandType<>(commandInfo.type(),
commandInfo.version(),
⋮----
hasPIIDataAnnotation(commandClass));
⋮----
throw new IllegalStateException("Cannot determine where to send command " + commandClass.getName());
⋮----
throw new IllegalStateException("Command class " + commandClass.getName() + " is not annotated with @CommandInfo");
⋮----
private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, CommandInfo commandInfo) {
⋮----
if (supportedCommand.typeName().equals(commandInfo.type()) &&
supportedCommand.version() == commandInfo.version()) {
⋮----
private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, CommandType<?> commandType) {
⋮----
if (supportedCommand.typeName().equals(commandType.typeName()) &&
supportedCommand.version() == commandType.version()) {
⋮----
private boolean producesDomainEvent(List<AggregateServiceDomainEventType> producedEvents, DomainEventType<?> externalDomainEventType) {
⋮----
if (producedEvent.typeName().equals(externalDomainEventType.typeName()) &&
producedEvent.version() == externalDomainEventType.version()) {
⋮----
public String resolveTopic(@Nonnull Class<? extends Command> commandClass) {
return resolveTopic(resolveType(commandClass));
⋮----
public String resolveTopic(@Nonnull CommandType<?> commandType) {
⋮----
.filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType))
⋮----
return services.getFirst().commandTopic();
⋮----
throw new IllegalStateException("Cannot determine where to send command " + commandType.typeName() + " v" + commandType.version());
⋮----
public String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType) {
⋮----
.filter(commandServiceRecord -> producesDomainEvent(commandServiceRecord.producedEvents(), externalDomainEventType))
⋮----
return services.getFirst().domainEventTopic();
⋮----
throw new IllegalStateException("Cannot determine which service produces DomainEvent " + externalDomainEventType.typeName() + " v" + externalDomainEventType.version());
⋮----
public Integer resolvePartition(@Nonnull String aggregateId) {
return Math.abs(hashFunction.hashString(aggregateId, UTF_8).asInt()) % partitions;
⋮----
public boolean isRunning() {
return processState == RUNNING && aggregatePartitions.values().stream().allMatch(AggregatePartition::isProcessing);
⋮----
public void setEnvironment(Environment environment) {
this.forceRegisterOnIncompatible = environment.getProperty("akces.aggregate.schemas.forceRegister", Boolean.class, false);
⋮----
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {

================
File: main/runtime/src/main/java/org/elasticsoftware/akces/AkcesControllerState.java
================


================
File: main/runtime/src/main/resources/protobuf/AggregateStateRecord.proto
================
// org.elasticsoftware.akces.protocol.AggregateStateRecord

// Message for org.elasticsoftware.akces.protocol.AggregateStateRecord
message AggregateStateRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional int64 generation = 7;
  optional string tenantId = 8;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: main/runtime/src/main/resources/protobuf/CommandRecord.proto
================
// org.elasticsoftware.akces.protocol.CommandRecord

// Message for org.elasticsoftware.akces.protocol.CommandRecord
message CommandRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional string tenantId = 7;
  optional string id = 8;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: main/runtime/src/main/resources/protobuf/DomainEventRecord.proto
================
// org.elasticsoftware.akces.protocol.DomainEventRecord

// Message for org.elasticsoftware.akces.protocol.DomainEventRecord
message DomainEventRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional int64 generation = 7;
  optional string tenantId = 8;
  optional string id = 9;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: main/runtime/src/main/resources/akces-aggregateservice.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: main/runtime/src/test/java/org/elasticsoftware/akces/beans/AotServicesTest.java
================
public class AotServicesTest {
⋮----
void testLoadAotServices() {
AotServices.Loader loader = AotServices.factories();
List<ValueCodeGenerator.Delegate> additionalDelegates = loader.load(ValueCodeGenerator.Delegate.class).asList();
Assertions.assertFalse(additionalDelegates.isEmpty());

================
File: main/runtime/src/test/java/org/elasticsoftware/akces/beans/MinInsyncReplicasTest.java
================
public class MinInsyncReplicasTest {
⋮----
void testScenarios() {
⋮----
Assertions.assertEquals(1, calculateQuorum((short) 1));
Assertions.assertEquals(2, calculateQuorum((short) 2));
Assertions.assertEquals(2, calculateQuorum((short) 3));
Assertions.assertEquals(3, calculateQuorum((short) 4));
Assertions.assertEquals(3, calculateQuorum((short) 5));
Assertions.assertEquals(4, calculateQuorum((short) 6));
Assertions.assertEquals(4, calculateQuorum((short) 7));
Assertions.assertEquals(5, calculateQuorum((short) 8));
Assertions.assertEquals(5, calculateQuorum((short) 9));
Assertions.assertEquals(6, calculateQuorum((short) 10));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/Account.java
================
public final class Account implements Aggregate<AccountState> {
⋮----
public String getName() {
⋮----
public Class<AccountState> getStateClass() {
⋮----
public Stream<AccountCreatedEvent> create(CreateAccountCommand cmd, AccountState isNull) {
return Stream.of(new AccountCreatedEvent(cmd.userId(), cmd.country(), cmd.firstName(), cmd.lastName(), cmd.email()));
⋮----
public AccountState create(@NotNull AccountCreatedEvent event, AccountState isNull) {
return new AccountState(event.userId(), event.country(), event.firstName(), event.lastName(), event.email());

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountState.java
================
@NotNull @PIIData String email) implements AggregateState {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/CreateAccountCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return orderId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderPlacedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return orderId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderProcess.java
================
String clientReference) implements OrderProcess {
⋮----
public String getProcessId() {
return orderId();
⋮----
public DomainEvent handle(InsufficientFundsErrorEvent error) {
return new BuyOrderRejectedEvent(error.walletId(), orderId(), clientReference());
⋮----
public DomainEvent handle(InvalidCurrencyErrorEvent error) {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderRejectedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/FxMarket.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcess.java
================
public sealed interface OrderProcess extends AkcesProcess permits BuyOrderProcess {
String orderId();
⋮----
FxMarket market();
⋮----
BigDecimal quantity();
⋮----
BigDecimal limitPrice();
⋮----
String clientReference();
⋮----
DomainEvent handle(InsufficientFundsErrorEvent error);
⋮----
DomainEvent handle(InvalidCurrencyErrorEvent error);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcessManager.java
================
public class OrderProcessManager implements Aggregate<OrderProcessManagerState> {
⋮----
public String getName() {
⋮----
public Class<OrderProcessManagerState> getStateClass() {
⋮----
public Stream<UserOrderProcessesCreatedEvent> create(AccountCreatedEvent event, OrderProcessManagerState isNull) {
return Stream.of(new UserOrderProcessesCreatedEvent(event.userId()));
⋮----
public OrderProcessManagerState create(UserOrderProcessesCreatedEvent event, OrderProcessManagerState isNull) {
return new OrderProcessManagerState(event.userId());
⋮----
public OrderProcessManagerState handle(BuyOrderCreatedEvent event, OrderProcessManagerState state) {
return new OrderProcessManagerState(state.userId(), new ArrayList<>(state.runningProcesses()) {{
add(new BuyOrderProcess(
event.orderId(),
event.market(),
event.quantity(),
event.limitPrice(),
event.clientReference()));
⋮----
public OrderProcessManagerState handle(BuyOrderRejectedEvent event, OrderProcessManagerState state) {
⋮----
removeIf(process -> process.orderId().equals(event.orderId()));
⋮----
public OrderProcessManagerState handle(BuyOrderPlacedEvent event, OrderProcessManagerState state) {
⋮----
public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {
⋮----
String orderId = UUID.randomUUID().toString();
⋮----
getCommandBus().send(new ReserveAmountCommand(
state.userId(),
command.market().quoteCurrency(),
command.quantity().multiply(command.limitPrice()),
⋮----
return Stream.of(new BuyOrderCreatedEvent(
⋮----
command.market(),
command.quantity(),
command.limitPrice(),
command.clientReference()));
⋮----
public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {
⋮----
OrderProcess orderProcess = state.getAkcesProcess(event.referenceId());
⋮----
return Stream.of(new BuyOrderPlacedEvent(state.userId(), orderProcess.orderId(), orderProcess.market(), orderProcess.quantity(), orderProcess.limitPrice()));
⋮----
return Stream.empty();
⋮----
public Stream<DomainEvent> handle(InsufficientFundsErrorEvent errorEvent, OrderProcessManagerState state) {
return Stream.of(state.getAkcesProcess(errorEvent.referenceId()).handle(errorEvent));
⋮----
public Stream<DomainEvent> handle(InvalidCurrencyErrorEvent errorEvent, OrderProcessManagerState state) {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcessManagerState.java
================
this(userId, List.of());
⋮----
public String getAggregateId() {
return userId();
⋮----
public OrderProcess getAkcesProcess(String processId) {
return runningProcesses.stream().filter(p -> p.orderId().equals(processId)).findFirst()
.orElseThrow(() -> new UnknownAkcesProcessException("OrderProcessManager", userId(), processId));
⋮----
public boolean hasAkcesProcess(String processId) {
return runningProcesses.stream().anyMatch(p -> p.orderId().equals(processId));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/PlaceBuyOrderCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/UserOrderProcessesCreatedEvent.java
================
public record UserOrderProcessesCreatedEvent(@NotNull @AggregateIdentifier String userId) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/AmountReservedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/BalanceAlreadyExistsErrorEvent.java
================
@NotNull String currency) implements ErrorEvent {
⋮----
public String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/BalanceCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreateBalanceCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreateWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreditWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/ExternalAccountCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InsufficientFundsErrorEvent.java
================
) implements ErrorEvent {
⋮----
public @Nonnull String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidAccountCreatedEvent.java
================
public record InvalidAccountCreatedEvent(String middleName, String currency) implements DomainEvent {
⋮----
public String getAggregateId() {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidAmountErrorEvent.java
================
) implements ErrorEvent {
⋮----
public @Nonnull String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidCurrencyErrorEvent.java
================
) implements ErrorEvent {
⋮----
public String getAggregateId() {
return walletId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/ReserveAmountCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/Wallet.java
================
public final class Wallet implements Aggregate<WalletState> {
⋮----
public String getName() {
⋮----
public Class<WalletState> getStateClass() {
⋮----
public @NotNull Stream<DomainEvent> create(@NotNull CreateWalletCommand cmd, WalletState isNull) {
return Stream.of(new WalletCreatedEvent(cmd.id()), new BalanceCreatedEvent(cmd.id(), cmd.currency()));
⋮----
public @NotNull Stream<DomainEvent> create(@NotNull AccountCreatedEvent event, WalletState isNull) {
⋮----
return Stream.of(new WalletCreatedEvent(event.getAggregateId()), new BalanceCreatedEvent(event.getAggregateId(), "EUR"));
⋮----
public @NotNull Stream<DomainEvent> createBalance(@NotNull CreateBalanceCommand cmd, @NotNull WalletState currentState) {
boolean balanceExists = currentState.balances().stream()
.anyMatch(balance -> balance.currency().equals(cmd.currency()));
⋮----
return Stream.of(new BalanceAlreadyExistsErrorEvent(cmd.id(), cmd.currency()));
⋮----
return Stream.of(new BalanceCreatedEvent(cmd.id(), cmd.currency()));
⋮----
public Stream<DomainEvent> credit(@NotNull CreditWalletCommand cmd, @NotNull WalletState currentState) {
WalletState.Balance balance = currentState.balances().stream().filter(b -> b.currency().equals(cmd.currency())).findFirst().orElse(null);
⋮----
return Stream.of(new InvalidCurrencyErrorEvent(cmd.id(), cmd.currency()));
⋮----
if (cmd.amount().compareTo(BigDecimal.ZERO) < 0) {
⋮----
return Stream.of(new InvalidAmountErrorEvent(cmd.id(), cmd.currency()));
⋮----
return Stream.of(new WalletCreditedEvent(currentState.id(), cmd.currency(), cmd.amount(), balance.amount().add(cmd.amount())));
⋮----
public Stream<DomainEvent> makeReservation(ReserveAmountCommand command, WalletState state) {
WalletState.Balance balance = state.balances().stream().filter(b -> b.currency().equals(command.currency())).findFirst().orElse(null);
⋮----
return Stream.of(new InvalidCurrencyErrorEvent(command.userId(), command.currency(), command.referenceId()));
⋮----
if (command.amount().compareTo(BigDecimal.ZERO) < 0) {
⋮----
return Stream.of(new InvalidAmountErrorEvent(command.userId(), command.currency()));
⋮----
if (balance.getAvailableAmount().compareTo(command.amount()) >= 0) {
return Stream.of(new AmountReservedEvent(command.userId(), command.currency(), command.amount(), command.referenceId()));
⋮----
return Stream.of(new InsufficientFundsErrorEvent(command.userId(), command.currency(), balance.getAvailableAmount(), command.amount(), command.referenceId()));
⋮----
public @NotNull WalletState create(@NotNull WalletCreatedEvent event, WalletState isNull) {
return new WalletState(event.id(), new ArrayList<>());
⋮----
public @NotNull WalletState createBalance(@NotNull BalanceCreatedEvent event, WalletState state) {
List<WalletState.Balance> balances = new ArrayList<>(state.balances());
balances.add(new WalletState.Balance(event.currency(), BigDecimal.ZERO));
return new WalletState(state.id(), balances);
⋮----
public @NotNull WalletState credit(@NotNull WalletCreditedEvent event, @NotNull WalletState state) {
return new WalletState(state.id(), state.balances().stream().map(b -> {
if (b.currency().equals(event.currency())) {
return new WalletState.Balance(b.currency(), b.amount().add(event.amount()));
⋮----
}).toList());
⋮----
public @NotNull WalletState reserveAmount(@NotNull AmountReservedEvent event, @NotNull WalletState state) {
⋮----
return new WalletState.Balance(b.currency(), b.amount(), b.reservedAmount().add(event.amount()));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletCreatedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletCreditedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletState.java
================
) implements AggregateState {
⋮----
public String getAggregateId() {
return id();
⋮----
public BigDecimal getAvailableAmount() {
return amount.subtract(reservedAmount);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/control/AkcesAggregateControllerTests.java
================
public class AkcesAggregateControllerTests {
⋮----
public void testSerialization() throws IOException {
ObjectMapper objectMapper = new ObjectMapper();
AggregateServiceRecord record = new AggregateServiceRecord(
⋮----
List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
Collections.emptyList());
assertNotNull(record);
⋮----
public void testDeserialization() throws JsonProcessingException {
⋮----
AkcesControlRecord deserialized = objectMapper.readValue(serializedRecord, AggregateServiceRecord.class);
assertNotNull(deserialized);
assertTrue(deserialized instanceof AggregateServiceRecord);
assertEquals("Account", ((AggregateServiceRecord) deserialized).aggregateName());
assertEquals("Account-Commands", ((AggregateServiceRecord) deserialized).commandTopic());
⋮----
public void testSerde() {
AkcesControlRecordSerde serde = new AkcesControlRecordSerde(new ObjectMapper());
⋮----
byte[] serialized = serde.serializer().serialize("Akces-Control", record);
assertNotNull(serialized);
⋮----
AkcesControlRecord deserialized = serde.deserializer().deserialize("Akces-Control", serialized);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/BalanceCreatedEvent.java
================
public record BalanceCreatedEvent(@AggregateIdentifier @NotNull String id, String currency) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/BuyOrderPlacedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return orderId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/CreateWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/old/WalletCreditedEvent.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/protocol/ProtocolTests.java
================
public class ProtocolTests {
⋮----
public void testJacksonProtobuf() throws IOException {
⋮----
DomainEventRecord testRecord = new DomainEventRecord("tenant1", "WalletCreated", 1, "{}".getBytes(StandardCharsets.UTF_8), JSON, "1", UUID.randomUUID().toString(), 1L);
⋮----
ProtobufSchema schema = ProtobufSchemaLoader.std.load(new StringReader(protobuf));
ObjectMapper objectMapper = new ProtobufMapper();
byte[] serializedTestRecord = objectMapper.writer(schema).writeValueAsBytes(testRecord);
⋮----
DomainEventRecord deserializedTestRecord = objectMapper.readerFor(DomainEventRecord.class).with(schema).readValue(serializedTestRecord);
⋮----
assertEquals(deserializedTestRecord.name(), testRecord.name());
assertEquals(deserializedTestRecord.version(), testRecord.version());
assertArrayEquals(deserializedTestRecord.payload(), testRecord.payload());
assertEquals(deserializedTestRecord.encoding(), testRecord.encoding());
assertEquals(deserializedTestRecord.aggregateId(), testRecord.aggregateId());
⋮----
public void generateDomainEventRecordProtobufSchema() throws JsonMappingException {
ProtobufMapper mapper = new ProtobufMapper();
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(DomainEventRecord.class);
NativeProtobufSchema nativeProtobufSchema = schemaWrapper.getSource();
⋮----
String asProtofile = nativeProtobufSchema.toString();
⋮----
System.out.println(asProtofile);
⋮----
public void generateCommandRecordProtobufSchema() throws JsonMappingException {
⋮----
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(CommandRecord.class);
⋮----
public void generateAggregateStateRecordProtobufSchema() throws JsonMappingException {
⋮----
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(AggregateStateRecord.class);
⋮----
public void generateCommandResponseRecordProtobufSchema() throws JsonMappingException {
⋮----
ProtobufSchema schemaWrapper = mapper.generateSchemaFor(CommandResponseRecord.class);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEvent.java
================
@NotNull AccountTypeV1 type) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEventV2.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEventV3.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountTypeV1.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountTypeV2.java
================


================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/CreditWalletCommand.java
================
) implements Command {
⋮----
public String getAggregateId() {
return id();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/JsonSchemaTests.java
================
public class JsonSchemaTests {
private static @NotNull SchemaGenerator createSchemaGenerator() {
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
return new SchemaGenerator(configBuilder.build());
⋮----
public void testSchemaCompatibility() throws IOException {
SchemaGenerator generator = createSchemaGenerator();
⋮----
JsonNode schemaV1 = generator.generateSchema(AccountCreatedEvent.class);
JsonNode schemaV2 = generator.generateSchema(AccountCreatedEventV2.class);
⋮----
System.out.println(schemaV1.toString());
System.out.println(schemaV2.toString());
⋮----
JsonSchema schema1 = new JsonSchema(schemaV1);
JsonSchema schema2 = new JsonSchema(schemaV2);
⋮----
assertEquals(schema2.isCompatible(CompatibilityLevel.BACKWARD_TRANSITIVE, List.of(new SimpleParsedSchemaHolder(schema1))).size(), 0);
⋮----
schema2.validate(schema2.toJson(new AccountCreatedEvent("1", "Musk", AccountTypeV1.PREMIUM)));
⋮----
schema2.validate(schema2.toJson(new AccountCreatedEventV2("1", "Musk", AccountTypeV2.PREMIUM, "Elon", "US")));
⋮----
public void testNullableString() throws IOException {
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_7, OptionPreset.PLAIN_JSON);
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS, JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator generator = new SchemaGenerator(config);
⋮----
JsonNode schema = generator.generateSchema(InvalidAmountErrorEvent.class);
JsonSchema jsonSchema = new JsonSchema(schema);
⋮----
jsonSchema.validate(jsonSchema.toJson(new InvalidAmountErrorEvent(UUID.randomUUID().toString(), "USD")));
⋮----
public void testNullForNotNullField() {
⋮----
ValidationException exception = Assertions.assertThrows(ValidationException.class, () -> {
jsonSchema.validate(jsonSchema.toJson(new InvalidAmountErrorEvent(UUID.randomUUID().toString(), null)));
⋮----
assertEquals("#/currency", exception.getPointerToViolation());
assertEquals("#/currency: expected type: String, found: Null", exception.getMessage());
⋮----
public void testCommandWithBigDecimal() throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
⋮----
JsonNode schema = generator.generateSchema(CreditWalletCommand.class);
System.out.println(schema);
⋮----
JsonNode jsonNode = objectMapper.valueToTree(new CreditWalletCommand("de6f87c6-0e4a-4f20-8c06-659fe5bcb7bc", "USD", new BigDecimal("100.00"), null));
jsonSchema.validate(jsonNode);
⋮----
String serialized = objectMapper.writeValueAsString(jsonNode);
System.out.println(serialized);
⋮----
public void testDeepEquals() throws JsonProcessingException {
⋮----
Assertions.assertEquals(registeredSchemaString, localSchemaString);
⋮----
JsonSchema localSchema = new JsonSchema(objectMapper.readValue(localSchemaString, Schema.class));
JsonSchema registeredSchema = new JsonSchema(objectMapper.readValue(registeredSchemaString, Schema.class));
⋮----
Assertions.assertTrue(registeredSchema.deepEquals(localSchema));

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/schemas/NotCompatibleAccountCreatedEventV4.java
================
) implements DomainEvent {
⋮----
public String getAggregateId() {
return userId();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/state/RocksDBAggregateStateRepositoryTests.java
================
public class RocksDBAggregateStateRepositoryTests {
private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
private final ObjectMapper objectMapper = new ObjectMapper();
⋮----
private final Future<RecordMetadata> producerResponse = mock(Future.class);
⋮----
public static void cleanUp() throws IOException {
⋮----
Files.walk(Paths.get("/tmp/rocksdb"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
public void testCreate() throws RocksDBException, IOException {
⋮----
new RocksDBAggregateStateRepository("/tmp/rocksdb",
⋮----
serde.serializer(),
serde.deserializer())) {
AggregateStateRecord record = repository.get("1234");
Assertions.assertNull(record);
⋮----
public void testSingleUpdate() throws RocksDBException, IOException, ExecutionException, InterruptedException {
⋮----
WalletState state = new WalletState(id, List.of(new WalletState.Balance("USD", BigDecimal.ZERO)));
AggregateStateRecord record = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(state),
⋮----
UUID.randomUUID().toString(),
⋮----
repository.prepare(record, producerResponse);
⋮----
when(producerResponse.get()).thenReturn(new RecordMetadata(new TopicPartition("Wallet-AggregateState", 0), 12, 0, System.currentTimeMillis(), 16, 345));
repository.commit();
⋮----
AggregateStateRecord result = repository.get(id);
Assertions.assertNotNull(result);
⋮----
Assertions.assertEquals(12, repository.getOffset());

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/util/HostUtilsTests.java
================
public class HostUtilsTests {
⋮----
public void testGetHostName() {
String hostName = HostUtils.getHostName();
System.out.println(hostName);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/AggregateServiceApplicationTests.java
================
public class AggregateServiceApplicationTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void cleanUp() throws IOException {
if (Files.exists(Paths.get("/tmp/akces"))) {
⋮----
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.forEach(File::delete);
⋮----
public void testAggregateLoading() {
⋮----
Assertions.assertNotNull(applicationContext.getBean("AccountAggregateRuntimeFactory"));
Assertions.assertNotNull(akcesAggregateController);
Assertions.assertNotNull(consumerFactory);
Assertions.assertNotNull(producerFactory);
Assertions.assertNotNull(controlConsumerFactory);
Assertions.assertThrows(NoSuchBeanDefinitionException.class, () -> applicationContext.getBean("WalletAggregateRuntimeFactory"));
Assertions.assertThrows(NoSuchBeanDefinitionException.class, () -> applicationContext.getBean("OrderProcessManagerAggregateRuntimeFactory"));
⋮----
Consumer<String, AkcesControlRecord> controlConsumer = controlConsumerFactory.createConsumer("Test-AkcesControl", "test-akces-control");
⋮----
controlConsumer.subscribe(List.of("Akces-Control"));
controlConsumer.poll(Duration.ofMillis(1000));
controlConsumer.seekToBeginning(controlConsumer.assignment());
⋮----
ConsumerRecords<String, AkcesControlRecord> controlRecords = new ConsumerRecords<>(Collections.emptyMap());
while (controlRecords.isEmpty()) {
controlRecords = controlConsumer.poll(Duration.ofMillis(1000));
⋮----
controlConsumer.close();
⋮----
while (!akcesAggregateController.isRunning()) {
Thread.onSpinWait();
⋮----
public static class Initializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
SchemaRegistryClient src = new CachedSchemaRegistryClient("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), 100);
prepareExternalSchemas(src, List.of(AccountCreatedEvent.class));
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/RuntimeConfiguration.java
================
public class RuntimeConfiguration {

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/RuntimeTests.java
================
public class RuntimeTests {
⋮----
private static final Network network = Network.newNetwork();
⋮----
new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
.withKraft()
.withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
.withNetwork(network)
.withNetworkAliases("kafka");
⋮----
new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
⋮----
.withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
.withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
.withExposedPorts(8081)
.withNetworkAliases("schema-registry")
.dependsOn(kafka);
⋮----
public static void prepareExternalServices(String bootstrapServers) {
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(new ObjectMapper());
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
⋮----
List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
List.of());
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", aggregateServiceRecord));
⋮----
controlProducer.commitTransaction();
⋮----
public static void cleanUp() throws IOException {
⋮----
if (Files.exists(Paths.get("/tmp/akces"))) {
Files.walk(Paths.get("/tmp/akces"))
.sorted(Comparator.reverseOrder())
.map(Path::toFile)
.filter(File::isDirectory)
.forEach(file -> System.out.println(file.getAbsolutePath()));
⋮----
.forEach(File::delete);
⋮----
public static Stream<TopicPartition> generateTopicPartitions(String topic, int partitions) {
return IntStream.range(0, partitions)
.mapToObj(i -> new TopicPartition(topic, i));
⋮----
public static <C extends Command> void prepareOldCommandSchemas(SchemaRegistryClient src) {
Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
src.register("commands.CreateWallet",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.CreateWalletCommand.class), List.of(), Map.of(), 1),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
public static <D extends DomainEvent> void prepareOldDomainEventSchemas(SchemaRegistryClient src) {
⋮----
src.register("domainevents.BalanceCreated",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.BalanceCreatedEvent.class), List.of(), Map.of(), 1),
⋮----
src.register("domainevents.BuyOrderPlaced",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.BuyOrderPlacedEvent.class), List.of(), Map.of(), 1),
⋮----
src.register("domainevents.WalletCredited",
new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.WalletCreditedEvent.class), List.of(), Map.of(), 1),
⋮----
public void testKafkaAdminClient() {
assertNotNull(adminClient);
⋮----
adminClient.describeTopics(
⋮----
assertNotNull(topics);
assertFalse(topics.isEmpty());
⋮----
public void createSchemas() throws RestClientException, IOException {
while (!walletAggregateController.isRunning() ||
!accountAggregateController.isRunning() ||
!orderProcessManagerAggregateController.isRunning() ||
!akcesClient.isRunning()) {
Thread.onSpinWait();
⋮----
System.out.println(schemaRegistryClient.getAllSubjects());
⋮----
public void testAkcesControl() throws JsonProcessingException {
assertNotNull(walletAggregateController);
assertNotNull(accountAggregateController);
assertNotNull(orderProcessManagerAggregateController);
assertNotNull(akcesClient);
⋮----
Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test");
Consumer<String, AkcesControlRecord> controlConsumer = controlConsumerFactory.createConsumer("Test-AkcesControl", "test-akces-control");
⋮----
TopicPartition controlParition = new TopicPartition("Akces-Control", 0);
controlConsumer.assign(List.of(controlParition));
controlConsumer.seekToBeginning(controlConsumer.assignment());
Map<TopicPartition, Long> endOffsets = controlConsumer.endOffsets(controlConsumer.assignment());
⋮----
while (endOffsets.getOrDefault(controlParition, 0L) > controlConsumer.position(controlParition)) {
ConsumerRecords<String, AkcesControlRecord> controlRecords = controlConsumer.poll(Duration.ofMillis(1000));
if (!controlRecords.isEmpty()) {
for (ConsumerRecord<String, AkcesControlRecord> record : controlRecords.records(controlParition)) {
System.out.println(objectMapper.writeValueAsString(record.value()));
⋮----
controlConsumer.close();
⋮----
while (!walletAggregateController.isRunning()) {
⋮----
CreateWalletCommand command = new CreateWalletCommand(userId, "USD");
CommandRecord commandRecord = new CommandRecord(null, "CreateWallet", 1, objectMapper.writeValueAsBytes(command), PayloadEncoding.JSON, command.getAggregateId(), null, null);
String topicName = walletAggregateController.resolveTopic(command.getClass());
int partition = walletAggregateController.resolvePartition(command.getAggregateId());
⋮----
testProducer.beginTransaction();
testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
testProducer.commitTransaction();
⋮----
TopicPartition aggregateStatePartition = new TopicPartition("Wallet-AggregateState", partition);
TopicPartition domainEventsPartition = new TopicPartition("Wallet-DomainEvents", partition);
⋮----
testConsumer.assign(List.of(aggregateStatePartition, domainEventsPartition));
⋮----
testConsumer.seekToBeginning(testConsumer.assignment());
ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
while (records.isEmpty()) {
⋮----
records = testConsumer.poll(Duration.ofMillis(250));
⋮----
assertFalse(records.isEmpty());
⋮----
CreditWalletCommand creditCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("100.00"));
CommandRecord creditCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(creditCommand), PayloadEncoding.JSON, creditCommand.getAggregateId(), null, null);
⋮----
testProducer.send(new ProducerRecord<>(topicName, partition, creditCommandRecord.aggregateId(), creditCommandRecord));
⋮----
CreditWalletCommand invalidCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("-100.00"));
CommandRecord invalidCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(invalidCommand), PayloadEncoding.JSON, invalidCommand.getAggregateId(), null, null);
⋮----
testProducer.send(new ProducerRecord<>(topicName, partition, invalidCommandRecord.aggregateId(), invalidCommandRecord));
⋮----
assertTrue(records.records(aggregateStatePartition).isEmpty());
⋮----
assertEquals(1, records.records(domainEventsPartition).size());
⋮----
DomainEventRecord protocolRecord = (DomainEventRecord) records.records(domainEventsPartition).getFirst().value();
assertEquals("InvalidAmountError", protocolRecord.name());
⋮----
testConsumer.close();
testProducer.close();
⋮----
public void testBatchedCommands() throws JsonProcessingException {
⋮----
List<String> userIds = List.of(
⋮----
Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
⋮----
Map<TopicPartition, Long> endOffsets = testConsumer.endOffsets(
Stream.concat(
generateTopicPartitions("Wallet-AggregateState", 3),
generateTopicPartitions("Wallet-DomainEvents", 3))
.toList());
⋮----
testConsumer.subscribe(List.of("Wallet-AggregateState", "Wallet-DomainEvents"), new ConsumerRebalanceListener() {
⋮----
public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
⋮----
public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
partitions.forEach(partition -> testConsumer.seek(partition, endOffsets.get(partition)));
⋮----
while (allRecords.size() < 40) {
records.forEach(record -> allRecords.add(record.value()));
⋮----
assertEquals(40, allRecords.size());
⋮----
public void testCreateViaExternalDomainEvent() throws JsonProcessingException {
⋮----
CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Fahim", "Zuijderwijk", "FahimZuijderwijk@jourrapide.com");
CommandRecord commandRecord = new CommandRecord(
⋮----
objectMapper.writeValueAsBytes(command),
⋮----
command.getAggregateId(),
⋮----
while (allRecords.size() < 4) {
⋮----
assertEquals(4, allRecords.size());
⋮----
public void testGDPREncryption() throws IOException {
⋮----
generateTopicPartitions("Account-AggregateState", 3),
generateTopicPartitions("Account-DomainEvents", 3))
⋮----
testConsumer.subscribe(List.of("Account-AggregateState", "Account-DomainEvents"), new ConsumerRebalanceListener() {
⋮----
while (allRecords.size() < 2) {
⋮----
assertEquals(2, allRecords.size());
⋮----
assertTrue(allRecords.get(1) instanceof DomainEventRecord);
DomainEventRecord domainEventRecord = (DomainEventRecord) allRecords.get(1);
AccountCreatedEvent accountCreatedEvent = objectMapper.readValue(domainEventRecord.payload(), AccountCreatedEvent.class);
assertNotEquals("Fahim", accountCreatedEvent.firstName());
assertNotEquals("Zuijderwijk", accountCreatedEvent.lastName());
assertNotEquals("FahimZuijderwijk@jourrapide.com", accountCreatedEvent.email());
AggregateStateRecord stateRecord = (AggregateStateRecord) allRecords.get(0);
AccountState accountState = objectMapper.readValue(stateRecord.payload(), AccountState.class);
assertNotEquals("Fahim", accountState.firstName());
assertNotEquals("Zuijderwijk", accountState.lastName());
assertNotEquals("FahimZuijderwijk@jourrapide.com", accountState.email());
⋮----
public void testDomainEventIndexing() throws IOException {
⋮----
TopicDescription topicDescription = getTopicDescription("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex");
⋮----
Thread.sleep(1000);
⋮----
Thread.currentThread().interrupt();
⋮----
topicDescription = getTopicDescription("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex");
⋮----
testConsumer.assign(generateTopicPartitions("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex", 1).toList());
⋮----
List<String> names = allRecords.stream().map(ProtocolRecord::name).toList();
⋮----
assertEquals("AccountCreated", names.getFirst());
assertTrue(names.indexOf("WalletCreated") < names.indexOf("BalanceCreated"));
assertTrue(names.contains("UserOrderProcessesCreated"));
⋮----
public void testDomainEventIndexingWithErrorEvents() throws IOException {
⋮----
TopicDescription topicDescription = getTopicDescription("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex");
⋮----
topicDescription = getTopicDescription("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex");
⋮----
testConsumer.assign(generateTopicPartitions("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex", 1).toList());
⋮----
while (allRecords.size() < 3) {
⋮----
assertEquals(3, allRecords.size());
⋮----
assertEquals("WalletCreated", allRecords.getFirst().name());
assertEquals("BalanceCreated", allRecords.get(1).name());
assertEquals("WalletCredited", allRecords.getLast().name());
⋮----
public void testWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
List<DomainEvent> result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);
⋮----
Assertions.assertNotNull(result);
Assertions.assertEquals(2, result.size());
assertInstanceOf(WalletCreatedEvent.class, result.getFirst());
assertInstanceOf(BalanceCreatedEvent.class, result.getLast());
⋮----
public void testOrderFlowWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Bella", "Fowler", "bella.fowler@example.com");
⋮----
Assertions.assertEquals(1, result.size());
assertInstanceOf(AccountCreatedEvent.class, result.getFirst());
⋮----
akcesClient.sendAndForget("TEST_TENANT", new CreditWalletCommand(userId, "EUR", new BigDecimal("100.00")));
⋮----
PlaceBuyOrderCommand orderCommand = new PlaceBuyOrderCommand(userId, new FxMarket("USDEUR", "USD", "EUR"), new BigDecimal("90.00"), new BigDecimal("1.05"), "trade-1");
result = akcesClient.send("TEST_TENANT", orderCommand).toCompletableFuture().get(10, TimeUnit.SECONDS);
⋮----
assertInstanceOf(BuyOrderCreatedEvent.class, result.getFirst());
⋮----
public void testAggregateAlreadyExistsErrorWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {
⋮----
CreateAccountCommand command = new CreateAccountCommand(userId, "USA", "Angelo", "Plummer", "AngeloRPlummer@rhyta.com");
⋮----
result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);
⋮----
assertInstanceOf(AggregateAlreadyExistsErrorEvent.class, result.getFirst());
⋮----
public TopicDescription getTopicDescription(String topic) {
⋮----
return adminClient.describeTopics(topic).get(topic);
⋮----
if (e.getCause().getClass().equals(ExecutionException.class) &&
e.getCause().getCause().getClass().equals(UnknownTopicOrPartitionException.class)) {
⋮----
public static class DataSourceInitializer
⋮----
public void initialize(ConfigurableApplicationContext applicationContext) {
⋮----
prepareKafka(kafka.getBootstrapServers());
SchemaRegistryClient src = new CachedSchemaRegistryClient("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), 100);
prepareExternalSchemas(src, List.of(AccountCreatedEvent.class));
prepareOldCommandSchemas(src);
prepareOldDomainEventSchemas(src);
⋮----
prepareAggregateServiceRecords(kafka.getBootstrapServers());
⋮----
throw new RuntimeException(e);
⋮----
TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
⋮----
"spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
"akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/TestUtils.java
================
public class TestUtils {
public static void prepareKafka(String bootstrapServers) {
KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
kafkaAdmin.createOrModifyTopics(
createCompactedTopic("Akces-Control", 3),
createTopic("Akces-CommandResponses", 3, 604800000L),
createCompactedTopic("Akces-GDPRKeys", 3),
createTopic("Wallet-Commands", 3),
createTopic("Wallet-DomainEvents", 3),
createTopic("Account-Commands", 3),
createTopic("Account-DomainEvents", 3),
createTopic("OrderProcessManager-Commands", 3),
createTopic("OrderProcessManager-DomainEvents", 3),
createCompactedTopic("Wallet-AggregateState", 3),
createCompactedTopic("Account-AggregateState", 3),
createCompactedTopic("OrderProcessManager-AggregateState", 3));
⋮----
private static NewTopic createTopic(String name, int numPartitions) {
return createTopic(name, numPartitions, -1L);
⋮----
private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
return topic.configs(Map.of(
⋮----
"retention.ms", Long.toString(retentionMs),
⋮----
private static NewTopic createCompactedTopic(String name, int numPartitions) {
⋮----
public static <E extends DomainEvent> void prepareExternalSchemas(SchemaRegistryClient src, List<Class<E>> externalDomainEvents) {
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_7, OptionPreset.PLAIN_JSON);
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS, JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
SchemaGeneratorConfig config = configBuilder.build();
SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
⋮----
DomainEventInfo info = eventClass.getAnnotation(DomainEventInfo.class);
src.register("domainevents." + info.type(),
new JsonSchema(jsonSchemaGenerator.generateSchema(eventClass), List.of(), Map.of(), info.version()),
info.version(),
⋮----
throw new ApplicationContextException("Problem populating SchemaRegistry", e);
⋮----
public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
ObjectMapper objectMapper = builder.build();
AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
Map<String, Object> controlProducerProps = Map.of(
⋮----
try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
controlProducer.initTransactions();
AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
controlProducer.beginTransaction();
⋮----
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
⋮----
controlProducer.commitTransaction();

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/WalletConfiguration.java
================
public class WalletConfiguration {
⋮----
public AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
return new AggregateBeanFactoryPostProcessor();
⋮----
public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
⋮----
builder.modulesToInstall(new AkcesGDPRModule());
builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
⋮----
public SchemaRegistryClient createSchemaRegistryClient() {
return new MockSchemaRegistryClient();
⋮----
public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient src,
⋮----
return new KafkaSchemaRegistry(src, objectMapper);

================
File: main/runtime/src/test/java/org/elasticsoftware/akcestest/WalletTests.java
================
public class WalletTests {
⋮----
public void testFindBeans() {
assertEquals(4, applicationContext.getBeansOfType(CommandHandlerFunction.class).size());
assertEquals(1, applicationContext.getBeansOfType(EventHandlerFunction.class).size());
assertEquals(4, applicationContext.getBeansOfType(EventSourcingHandlerFunction.class).size());
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_create_CreateWallet_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_credit_CreditWallet_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_makeReservation_ReserveAmount_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_createBalance_CreateBalance_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_eh_create_AccountCreated_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_create_WalletCreated_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_createBalance_BalanceCreated_1"));
Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_credit_WalletCredited_1"));
⋮----
public void testValidateDomainEventsWithMissingExternalDomainEventSchema() throws Exception {
AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);
assertThrows(SchemaNotFoundException.class, () -> {
for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
walletAggregate.registerAndValidate(domainEventType);
⋮----
System.out.println(schemaRegistryClient.getAllSubjects());
⋮----
public void testValidateDomainEvents() throws Exception {
⋮----
schemaRegistryClient.register("domainevents.AccountCreated",
schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, AccountCreatedEvent.class, true, true, false, true)),
⋮----
public void testValidateDomainEventsWithExistingSchemas() throws Exception {
⋮----
schemaRegistryClient.register("domainevents.WalletCreated",
schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCreated", 1, WalletCreatedEvent.class, true, false, false, false)),
⋮----
schemaRegistryClient.register("domainevents.WalletCredited",
schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCredited", 1, WalletCreditedEvent.class, false, false, false, false)),
⋮----
public void testValidateDomainEventsWithExistingSchemasAndExternalEventSubset() throws Exception {
⋮----
schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false, false)),
⋮----
public void testValidateDomainEventsWithExistingSchemasAndInvalidExternalEvent() throws Exception {
⋮----
schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, false, false, false)),
⋮----
Assertions.assertThrows(IncompatibleSchemaException.class, () ->
walletAggregate.registerAndValidate(new DomainEventType<>("AccountCreated", 1, InvalidAccountCreatedEvent.class, true, true, false, false)));
⋮----
public void testRegisterAndValidateMultipleVersionsOfEvent() throws Exception {
⋮----
walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 2, AccountCreatedEventV2.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false, false));
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas("domainevents.TestAccountCreated", false, false);
assertEquals(3, registeredSchemas.size());
⋮----
public void testRegisterAndValidateMultipleVersionsOfEventWithSkippedVersion() throws Exception {
⋮----
walletAggregate.registerAndValidate(new DomainEventType<>("AnotherTestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false, false));
Assertions.assertThrows(InvalidSchemaVersionException.class, () ->
walletAggregate.registerAndValidate(new DomainEventType<>("AnotherTestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false, false)));
⋮----
public void testRegisterAndValidateMultipleVersionsOfEventWithNonCompatibleEvent() throws Exception {
⋮----
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 2, AccountCreatedEventV2.class, true, false, false, false));
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false, false));
SchemaNotBackwardsCompatibleException exception = Assertions.assertThrows(SchemaNotBackwardsCompatibleException.class, () -> {
walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 4, NotCompatibleAccountCreatedEventV4.class, true, false, false, false));
⋮----
assertEquals("Schema not backwards compatible with previous version: 3", exception.getMessage());
⋮----
public void testCreateWalletByCommand() throws Exception {
⋮----
walletAggregate.handleCommandRecord(
new CommandRecord(
⋮----
objectMapper.writeValueAsBytes(
new CreateWalletCommand(aggregateId, "EUR")),
⋮----
(eventRecord, index) -> indexedEvents.add(eventRecord),
⋮----
assertEquals(4, producedRecords.size());
AggregateStateRecord actualRecord = (AggregateStateRecord) producedRecords.get(0);
AggregateStateRecord expectedRecord = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(new WalletState(aggregateId, new ArrayList<>())),
⋮----
assertEquals(expectedRecord.generation(), actualRecord.generation());
assertEquals(expectedRecord.aggregateId(), actualRecord.aggregateId());
assertEquals(expectedRecord.correlationId(), actualRecord.correlationId());
assertArrayEquals(expectedRecord.payload(), actualRecord.payload());
assertEquals(expectedRecord.encoding(), actualRecord.encoding());
assertEquals(expectedRecord.name(), actualRecord.name());
assertEquals(expectedRecord.version(), actualRecord.version());
⋮----
DomainEventRecord actual = (DomainEventRecord) producedRecords.get(1);
⋮----
assertEquals(1, actual.generation());
assertEquals(aggregateId, actual.aggregateId());
assertEquals(correlationId, actual.correlationId());
assertArrayEquals(objectMapper.writeValueAsBytes(new WalletCreatedEvent(aggregateId)), actual.payload());
assertEquals(PayloadEncoding.JSON, actual.encoding());
assertEquals("WalletCreated", actual.name());
assertEquals(1, actual.version());
⋮----
actualRecord = (AggregateStateRecord) producedRecords.get(2);
expectedRecord = new AggregateStateRecord(
⋮----
objectMapper.writeValueAsBytes(new WalletState(aggregateId, List.of(new WalletState.Balance("EUR", BigDecimal.ZERO)))),
⋮----
actual = (DomainEventRecord) producedRecords.get(3);
⋮----
assertEquals(2, actual.generation());
⋮----
assertArrayEquals(objectMapper.writeValueAsBytes(new BalanceCreatedEvent(aggregateId, "EUR")), actual.payload());
⋮----
assertEquals("BalanceCreated", actual.name());
⋮----
public void testIndexWalletEventsFromCommand() throws Exception {
⋮----
assertEquals(2, indexedEvents.size());
DomainEventRecord actual = indexedEvents.get(0);
⋮----
actual = (DomainEventRecord) indexedEvents.get(1);
⋮----
public void testCreateWalletByExternalDomainEvent() throws Exception {
⋮----
walletAggregate.handleExternalDomainEventRecord(
new DomainEventRecord(
⋮----
new AccountCreatedEvent(aggregateId, "NL", "7hdU_mfA_bvkRRgCekTZ0A==", "ioxbJd-hSLj6KNJpdYzN4g==", "6KLIDo3Ii2d-oVZtiv1h3OYNgW5lXYAnCnxPK2fprUU=")),
⋮----
public void testIndexWalletEventsByExternalDomainEvent() throws Exception {

================
File: main/runtime/src/test/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
akces.client.domainEventsPackage=org.elasticsoftware.akcestest.aggregate

================
File: main/runtime/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.kafka" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.gdpr" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.state" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.client" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: main/runtime/src/test/resources/test-application.yaml
================
spring:
  main:
    allow-bean-definition-overriding: true

================
File: main/runtime/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.8.13-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-runtime</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: Runtime</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>com.google.protobuf</groupId>
            <artifactId>protobuf-java</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.semver4j</groupId>
            <artifactId>semver4j</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-client</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <executions>
                    <execution>
                        <goals>
                            <goal>test-jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceCommandType.java
================
public <C extends Command> CommandType<C> toLocalCommandType(Class<C> typeClass) {
return new CommandType<>(typeName, version, typeClass, create, true, hasPIIDataAnnotation(typeClass));

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceDomainEventType.java
================
public <E extends DomainEvent> DomainEventType<E> toLocalDomainEventType(Class<E> typeClass, boolean error) {
return new DomainEventType<>(typeName, version, typeClass, create, external, error, hasPIIDataAnnotation(typeClass));

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceRecord.java
================
) implements AkcesControlRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AkcesControlRecord.java
================
public sealed interface AkcesControlRecord permits AggregateServiceRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/control/AkcesRegistry.java
================
public interface AkcesRegistry {
CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass);
⋮----
String resolveTopic(@Nonnull Class<? extends Command> commandClass);
⋮----
String resolveTopic(@Nonnull CommandType<?> commandType);
⋮----
String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType);
⋮----
Integer resolvePartition(@Nonnull String aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/errors/AggregateAlreadyExistsErrorEvent.java
================
) implements ErrorEvent {
⋮----
public String getAggregateId() {
return aggregateId();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/errors/CommandExecutionErrorEvent.java
================
) implements ErrorEvent {
⋮----
public String getAggregateId() {
return aggregateId();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/AkcesGDPRModule.java
================
public class AkcesGDPRModule extends Module {
private static Version extractVersion() {
String className = format("/%s.class", AkcesGDPRModule.class.getName().replace('.', '/'));
URL resource = AkcesGDPRModule.class.getResource(className);
⋮----
String classPath = resource.toString();
if (!classPath.startsWith("jar")) {
⋮----
return Version.unknownVersion();
⋮----
String manifestPath = classPath.substring(0, classPath.lastIndexOf("!") + 1) +
⋮----
Manifest manifest = new Manifest(new URL(manifestPath).openStream());
Attributes attr = manifest.getMainAttributes();
String value = attr.getValue("Implementation-Version");
⋮----
return generateVersion(Semver.parse(value));
⋮----
public static Version generateVersion(Semver semver) {
⋮----
return new Version(
semver.getMajor(),
semver.getMinor(),
semver.getPatch(),
!semver.getPreRelease().isEmpty() ? semver.getPreRelease().get(0) : null,
⋮----
public String getModuleName() {
⋮----
public Version version() {
return extractVersion();
⋮----
public void setupModule(SetupContext setupContext) {
setupContext.addBeanSerializerModifier(new PIIDataSerializerModifier());
setupContext.addBeanDeserializerModifier(new PIIDataDeserializerModifier());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataDeserializerModifier.java
================
public class PIIDataDeserializerModifier extends BeanDeserializerModifier {
private final PIIDataJsonDeserializer instance = new PIIDataJsonDeserializer();
⋮----
public BeanDeserializerBuilder updateBuilder(DeserializationConfig config,
⋮----
Iterator<SettableBeanProperty> it = builder.getProperties();
⋮----
while (it.hasNext()) {
SettableBeanProperty p = it.next();
if (p.getAnnotation(PIIData.class) != null) {
builder.addOrReplaceProperty(p.withValueDeserializer(instance), true);
⋮----
if (builder.getValueInstantiator() != null) {
SettableBeanProperty[] constructorArguments = builder.getValueInstantiator().getFromObjectArguments(config);
⋮----
if (constructorArguments[i].getAnnotation(PIIData.class) != null) {
updatedArguments[i] = constructorArguments[i].withValueDeserializer(instance);
⋮----
builder.setValueInstantiator(new PersonalDataValueInstantiator((StdValueInstantiator) builder.getValueInstantiator(), updatedArguments));
⋮----
static class PersonalDataValueInstantiator extends StdValueInstantiator {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataJsonDeserializer.java
================
public class PIIDataJsonDeserializer extends StringDeserializer {
⋮----
public String deserialize(JsonParser p, DeserializationContext ctxt) throws IOException {
String encryptedString = super.deserialize(p, ctxt);
GDPRContext gdprContext = GDPRContextHolder.getCurrentGDPRContext();
return gdprContext != null ? gdprContext.decrypt(encryptedString) : encryptedString;

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataJsonSerializer.java
================
public class PIIDataJsonSerializer extends JsonSerializer<Object> {
⋮----
public void serialize(Object value, JsonGenerator gen, SerializerProvider provider) throws IOException {
GDPRContext gdprContext = GDPRContextHolder.getCurrentGDPRContext();
⋮----
gen.writeString(gdprContext.encrypt((String) value));
⋮----
gen.writeString((String) value);
⋮----
public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint) throws JsonMappingException {
visitor.expectStringFormat(typeHint);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataSerializerModifier.java
================
public class PIIDataSerializerModifier extends BeanSerializerModifier {
private final PIIDataJsonSerializer instance = new PIIDataJsonSerializer();
⋮----
public List<BeanPropertyWriter> changeProperties(final SerializationConfig config,
⋮----
if (null == writer.getAnnotation(PIIData.class)) {
newWriters.add(writer);
⋮----
newWriters.add(new PersonalDataPropertyWriter(writer, instance));
⋮----
static class PersonalDataPropertyWriter extends BeanPropertyWriter {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/EncryptingGDPRContext.java
================
public final class EncryptingGDPRContext implements GDPRContext {
private static final Logger logger = LoggerFactory.getLogger(EncryptingGDPRContext.class);
⋮----
throw new IllegalArgumentException("Key size needs to be 32 bytes");
⋮----
SecretKeySpec keySpec = new SecretKeySpec(encryptionKey, "AES");
⋮----
UUID aggregateUUID = UUID.fromString(aggregateId);
ivParameterSpec = new IvParameterSpec(ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array());
⋮----
encryptingCipher = Cipher.getInstance("AES/" + aesMode + "/PKCS5PADDING");
decryptingCipher = Cipher.getInstance("AES/" + aesMode + "/PKCS5PADDING");
encryptingCipher.init(Cipher.ENCRYPT_MODE, keySpec, ivParameterSpec, GDPRKeyUtils.secureRandom());
decryptingCipher.init(Cipher.DECRYPT_MODE, keySpec, ivParameterSpec, GDPRKeyUtils.secureRandom());
⋮----
throw new SerializationException(e);
⋮----
throw new IllegalArgumentException(e);
⋮----
public String encrypt(@Nullable String data) {
⋮----
logger.trace("Encrypting data for aggregateId '{}' with algorithm {} and encryptionKey (hash) {}",
⋮----
encryptingCipher.getAlgorithm(),
HexFormat.of().formatHex(encryptionKey));
return Base64.getUrlEncoder().encodeToString(encryptingCipher.doFinal(data.getBytes(StandardCharsets.UTF_8)));
⋮----
public String decrypt(@Nullable String encryptedData) {
⋮----
if (encryptedData.length() % 4 == 0) {
byte[] encryptedBytes = Base64.getUrlDecoder().decode(encryptedData);
⋮----
logger.trace("Decrypting data for aggregateId '{}' with algorithm {} and encryptionKey (hash) {}",
⋮----
decryptingCipher.getAlgorithm(),
⋮----
return new String(decryptingCipher.doFinal(encryptedBytes), StandardCharsets.UTF_8);
⋮----
public String getAggregateId() {
⋮----
public byte[] getEncryptionKey() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRAnnotationUtils.java
================
public final class GDPRAnnotationUtils {
⋮----
public static Boolean hasPIIDataAnnotation(Class<?> type) {
return hasAnnotation(type, PIIData.class);
⋮----
public static Boolean hasAnnotation(Class<?> type, final Class<? extends Annotation> annotation) {
return hasAnnotationInternal(type, annotation, new HashSet<>());
⋮----
private static Boolean hasAnnotationInternal(Class<?> type, final Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
visitedClasses.add(type);
if (methodsHaveAnnotation(type, annotation, visitedClasses)) {
⋮----
if (constructorParametersHaveAnnotation(type, annotation, visitedClasses)) {
⋮----
if (fieldsHaveAnnotation(type, annotation, visitedClasses)) {
⋮----
private static boolean methodsHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
⋮----
if (Stream.of(klass.getDeclaredMethods()).anyMatch(method -> method.isAnnotationPresent(annotation) ||
(!BeanUtils.isSimpleValueType(method.getReturnType()) &&
!visitedClasses.contains(method.getReturnType()) &&
hasAnnotationInternal(method.getReturnType(), annotation, visitedClasses)))) {
⋮----
if (klass.isArray()) {
klass = klass.getComponentType();
⋮----
klass = klass.getSuperclass();
⋮----
private static boolean constructorParametersHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
return stream(type.getConstructors())
.flatMap(constructor -> stream(constructor.getParameters()))
.anyMatch(parameter -> parameter.isAnnotationPresent(annotation) ||
(!BeanUtils.isSimpleValueType(parameter.getType()) &&
!visitedClasses.contains(parameter.getType()) &&
hasAnnotationInternal(parameter.getType(), annotation, visitedClasses)));
⋮----
private static boolean fieldsHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
return Arrays.stream(type.getDeclaredFields())
.anyMatch(field -> field.isAnnotationPresent(annotation) ||
(!BeanUtils.isSimpleValueType(field.getType()) &&
!visitedClasses.contains(field.getType()) &&
hasAnnotationInternal(field.getType(), annotation, visitedClasses)));

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContext.java
================
public sealed interface GDPRContext permits NoopGDPRContext, EncryptingGDPRContext {
⋮----
String encrypt(@Nullable String data);
⋮----
String decrypt(@Nullable String encryptedData);
⋮----
String getAggregateId();
⋮----
default byte[] getEncryptionKey() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextHolder.java
================
public final class GDPRContextHolder {
⋮----
public static GDPRContext getCurrentGDPRContext() {
return currentContext.get();
⋮----
public static GDPRContext setCurrentGDPRContext(@Nullable GDPRContext gdprContext) {
final GDPRContext ctx = currentContext.get();
currentContext.set(gdprContext);
⋮----
public static GDPRContext resetCurrentGDPRContext() {
⋮----
currentContext.remove();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepository.java
================
public interface GDPRContextRepository extends Closeable {
default long getOffset() {
⋮----
void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture);
⋮----
void commit();
⋮----
void rollback();
⋮----
void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords);
⋮----
boolean exists(String aggregateId);
⋮----
GDPRContext get(String aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepositoryException.java
================
public class GDPRContextRepositoryException extends RuntimeException {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepositoryFactory.java
================
public interface GDPRContextRepositoryFactory {
GDPRContextRepository create(String runtimeName, Integer partitionId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRKeyUtils.java
================
public class GDPRKeyUtils {
⋮----
Pattern.compile("^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$");
⋮----
secureRandom = SecureRandom.getInstance("NativePRNGNonBlocking");
⋮----
secureRandom = new SecureRandom();
⋮----
public static SecretKeySpec createKey() {
⋮----
secureRandom.nextBytes(keyBytes);
return new SecretKeySpec(keyBytes, "AES");
⋮----
public static SecureRandom secureRandom() {
⋮----
public static boolean isUUID(String possibleUUID) {
return UUID_REGEX.matcher(possibleUUID).matches();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/InMemoryGDPRContextRepository.java
================
public class InMemoryGDPRContextRepository implements GDPRContextRepository {
private static final Logger log = LoggerFactory.getLogger(InMemoryGDPRContextRepository.class);
⋮----
private final LoadingCache<String, GDPRContext> gdprContexts = Caffeine.newBuilder()
.maximumSize(1000)
.build(this::createGDPRContext);
⋮----
public void close() {
stateRecordMap.clear();
transactionStateRecordMap.clear();
gdprContexts.invalidateAll();
gdprContexts.cleanUp();
⋮----
public void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture) {
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
⋮----
gdprContexts.invalidate(record.aggregateId());
⋮----
public void commit() {
⋮----
if (!transactionStateRecordMap.isEmpty()) {
⋮----
this.offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
log.trace("Committing {} records and offset {}", transactionStateRecordMap.size(), this.offset);
transactionStateRecordMap.values().forEach(recordAndMetadata -> stateRecordMap.put(recordAndMetadata.record().aggregateId(), (GDPRKeyRecord) recordAndMetadata.record()));
⋮----
public void rollback() {
⋮----
gdprContexts.invalidateAll(transactionStateRecordMap.keySet());
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
GDPRKeyRecord record = (GDPRKeyRecord) consumerRecord.value();
⋮----
stateRecordMap.put(record.aggregateId(), record);
⋮----
stateRecordMap.remove(consumerRecord.key());
⋮----
gdprContexts.invalidate(consumerRecord.key());
this.offset = consumerRecord.offset();
⋮----
public boolean exists(String aggregateId) {
return getGDPRKeyRecord(aggregateId) != null;
⋮----
public GDPRContext get(String aggregateId) {
return gdprContexts.get(aggregateId);
⋮----
private GDPRContext createGDPRContext(String aggregateId) {
GDPRKeyRecord record = getGDPRKeyRecord(aggregateId);
⋮----
checkAggregateIdType(aggregateId);
return new EncryptingGDPRContext(record.aggregateId(), record.payload(), aggregateIdIsUUID);
⋮----
return new NoopGDPRContext(aggregateId);
⋮----
private GDPRKeyRecord getGDPRKeyRecord(String aggregateId) {
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
return stateRecordMap.get(aggregateId);
⋮----
public long getOffset() {
⋮----
private void checkAggregateIdType(String aggregateId) {
⋮----
UUID.fromString(aggregateId);
⋮----
log.trace("AggregateId '{}' is a UUID", aggregateId);
⋮----
log.trace("AggregateId '{}' is not a UUID", aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/InMemoryGDPRContextRepositoryFactory.java
================
public class InMemoryGDPRContextRepositoryFactory implements GDPRContextRepositoryFactory {
⋮----
public GDPRContextRepository create(String runtimeName, Integer partitionId) {
return new InMemoryGDPRContextRepository();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/NoopGDPRContext.java
================
public final class NoopGDPRContext implements GDPRContext {
⋮----
public String encrypt(@Nullable String data) {
⋮----
public String decrypt(@Nullable String encryptedData) {
⋮----
public String getAggregateId() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepository.java
================
public class RocksDBGDPRContextRepository implements GDPRContextRepository {
private static final Logger log = LoggerFactory.getLogger(RocksDBGDPRContextRepository.class);
⋮----
private final LoadingCache<String, GDPRContext> gdprContexts = Caffeine.newBuilder()
.maximumSize(1000)
.build(this::createGDPRContext);
⋮----
RocksDB.loadLibrary();
final Options options = new Options();
final TransactionDBOptions transactionDBOptions = new TransactionDBOptions();
options.setCreateIfMissing(true);
⋮----
options.setAllowFAllocate(false);
this.rocksDBDataDir = new File(baseDir, partitionId);
⋮----
Files.createDirectories(this.rocksDBDataDir.getParentFile().toPath());
Files.createDirectories(this.rocksDBDataDir.getAbsoluteFile().toPath());
db = TransactionDB.open(options, transactionDBOptions, this.rocksDBDataDir.getAbsolutePath());
⋮----
initializeOffset();
log.info("RocksDBGDPRContextRepository for partition {} initialized in folder {}", partitionId, this.rocksDBDataDir.getAbsolutePath());
⋮----
throw new GDPRContextRepositoryException("Error initializing RocksDB", e);
⋮----
public void close() {
⋮----
db.syncWal();
⋮----
log.error("Error syncing WAL. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
db.close();
⋮----
private void initializeOffset() {
⋮----
byte[] offsetBytes = db.get(OFFSET);
⋮----
lastOffset = Longs.fromByteArray(offsetBytes);
⋮----
throw new GDPRContextRepositoryException("Error initializing offset", e);
⋮----
private void updateOffset(long offset) {
⋮----
log.trace("Updated offset to {}", offset);
⋮----
public long getOffset() {
⋮----
public void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture) {
checkAggregateIdType(record.aggregateId());
transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
⋮----
public void commit() {
if (!transactionStateRecordMap.isEmpty()) {
⋮----
Transaction transaction = db.beginTransaction(new WriteOptions());
⋮----
for (RecordAndMetadata<?> recordAndMetadata : transactionStateRecordMap.values()) {
transaction.put(keyBytes(recordAndMetadata.record().aggregateId()), serializer.serialize(topicName, recordAndMetadata.record()));
⋮----
long offset = transactionStateRecordMap.values().stream()
.map(RecordAndMetadata::metadata)
.map(recordMetadataFuture -> {
⋮----
return recordMetadataFuture.get();
⋮----
log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
⋮----
.map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
.max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
transaction.put(OFFSET, Longs.toByteArray(offset));
transaction.commit();
transaction.close();
updateOffset(offset);
⋮----
throw new GDPRContextRepositoryException("Error committing records", e);
⋮----
transactionStateRecordMap.clear();
⋮----
public void rollback() {
⋮----
public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
⋮----
long offset = consumerRecords.stream()
.map(ConsumerRecord::offset)
⋮----
if(consumerRecord.value() != null) {
⋮----
transaction.put(keyBytes(consumerRecord.key()), serializer.serialize(topicName, consumerRecord.value()));
log.trace("{} Wrote record with key {}", rocksDBDataDir.getAbsolutePath(), consumerRecord.key());
⋮----
transaction.delete(keyBytes(consumerRecord.key()));
⋮----
consumerRecords.forEach(consumerRecord -> gdprContexts.invalidate(consumerRecord.key()));
⋮----
throw new GDPRContextRepositoryException("Error processing records", e);
⋮----
public boolean exists(String aggregateId) {
return transactionStateRecordMap.containsKey(aggregateId) || db.keyExists(keyBytes(aggregateId));
⋮----
public GDPRContext get(String aggregateId) {
return gdprContexts.get(aggregateId);
⋮----
private GDPRContext createGDPRContext(String aggregateId) {
GDPRKeyRecord record = getGDPRKeyRecord(aggregateId);
⋮----
checkAggregateIdType(aggregateId);
return new EncryptingGDPRContext(record.aggregateId(), record.payload(), aggregateIdIsUUID);
⋮----
return new NoopGDPRContext(aggregateId);
⋮----
private GDPRKeyRecord getGDPRKeyRecord(String aggregateId) {
log.trace("{} Getting record for aggregateId {}",rocksDBDataDir.getAbsolutePath(), aggregateId);
⋮----
if (transactionStateRecordMap.containsKey(aggregateId)) {
return transactionStateRecordMap.get(aggregateId).record();
⋮----
byte[] keyBytes = keyBytes(aggregateId);
if (db.keyExists(keyBytes)) {
⋮----
return (GDPRKeyRecord) deserializer.deserialize(topicName, db.get(keyBytes));
⋮----
throw new GDPRContextRepositoryException("Problem reading record with aggregateId " + aggregateId, e);
⋮----
private byte[] keyBytes(String aggregateId) {
⋮----
UUID aggregateUUID = UUID.fromString(aggregateId);
return ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array();
⋮----
return aggregateId.getBytes(StandardCharsets.UTF_8);
⋮----
private void checkAggregateIdType(String aggregateId) {
⋮----
UUID.fromString(aggregateId);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepositoryFactory.java
================
public class RocksDBGDPRContextRepositoryFactory implements GDPRContextRepositoryFactory {
⋮----
public GDPRContextRepository create(String runtimeName, Integer partitionId) {
return new RocksDBGDPRContextRepository(
⋮----
runtimeName + "-Akces-GDPRKeys-" + partitionId.toString(),
⋮----
serde.serializer(),
serde.deserializer());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/kafka/CustomKafkaConsumerFactory.java
================
public class CustomKafkaConsumerFactory<K, V> extends DefaultKafkaConsumerFactory<K, V> {
⋮----
protected Consumer<K, V> createKafkaConsumer(Map<String, Object> configProps) {
⋮----
configProps.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, configProps.get(ConsumerConfig.CLIENT_ID_CONFIG));
return super.createKafkaConsumer(configProps);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/kafka/CustomKafkaProducerFactory.java
================
public class CustomKafkaProducerFactory<K, V> extends DefaultKafkaProducerFactory<K, V> {
⋮----
protected Producer<K, V> createTransactionalProducer(String transactionId) {
⋮----
Producer<K, V> newProducer = createRawProducer(getTxProducerConfigs(transactionId));
⋮----
newProducer.initTransactions();
⋮----
newProducer.close(ProducerFactory.DEFAULT_PHYSICAL_CLOSE_TIMEOUT);
⋮----
KafkaException newEx = new KafkaException("initTransactions() failed and then close() failed", ex);
newEx.addSuppressed(ex2);
⋮----
throw new KafkaException("initTransactions() failed", ex);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/kafka/RecordAndMetadata.java
================


================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/AggregateStateRecord.java
================
) implements ProtocolRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/CommandRecord.java
================
) implements ProtocolRecord {
⋮----
this(UUID.randomUUID().toString(),

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/CommandResponseRecord.java
================
) implements ProtocolRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/DomainEventRecord.java
================
) implements ProtocolRecord {
⋮----
this(UUID.randomUUID().toString(), tenantId, name, version, payload, encoding, aggregateId, correlationId, generation);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/GDPRKeyRecord.java
================
) implements ProtocolRecord {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/PayloadEncoding.java
================


================
File: main/shared/src/main/java/org/elasticsoftware/akces/protocol/ProtocolRecord.java
================
public sealed interface ProtocolRecord permits AggregateStateRecord, CommandRecord, DomainEventRecord, GDPRKeyRecord, CommandResponseRecord {
String tenantId();
⋮----
String name();
⋮----
int version();
⋮----
byte[] payload();
⋮----
PayloadEncoding encoding();
⋮----
String aggregateId();
⋮----
String correlationId();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/IncompatibleSchemaException.java
================
public class IncompatibleSchemaException extends SchemaException {
⋮----
public int getSchemaVersion() {
⋮----
public List<Difference> getDifferences() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/InvalidSchemaVersionException.java
================
public class InvalidSchemaVersionException extends SchemaException {
⋮----
public int getSchemaVersion() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/KafkaSchemaRegistry.java
================
public class KafkaSchemaRegistry {
private static final Logger logger = LoggerFactory.getLogger(KafkaSchemaRegistry.class);
⋮----
this.schemaGeneratorTheadLocal = ThreadLocal.withInitial(() -> createJsonSchemaGenerator(objectMapper));
⋮----
public JsonSchema validate(CommandType<?> commandType) throws SchemaException {
return validate(commandType, true);
⋮----
public JsonSchema validate(DomainEventType<?> domainEventType) throws SchemaException {
return validate(domainEventType, false);
⋮----
private JsonSchema validate(SchemaType schemaType, boolean strict) throws SchemaException {
⋮----
logger.info("Validating schema {} v{}", schemaType.getSchemaName(), schemaType.version());
JsonSchema localSchema = generateJsonSchema(schemaType);
⋮----
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas(schemaType.getSchemaName(), false, false);
if (!registeredSchemas.isEmpty()) {
logger.trace("Found {} schemas for type {}", registeredSchemas.size(), schemaType.typeName());
⋮----
ParsedSchema registeredSchema = registeredSchemas.stream()
.filter(parsedSchema -> getSchemaVersion(schemaType, parsedSchema) == schemaType.version())
.findFirst().orElse(null);
⋮----
logger.trace("Found schema for type {} v{}", schemaType.typeName(), schemaType.version());
⋮----
List<Difference> differences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());
if (!differences.isEmpty()) {
⋮----
List<Difference> violatingDifferences = differences.stream()
.filter(difference -> !difference.getType().equals(Difference.Type.PROPERTY_REMOVED_FROM_CLOSED_CONTENT_MODEL))
.toList();
if (!violatingDifferences.isEmpty()) {
⋮----
throw new IncompatibleSchemaException(
schemaType.getSchemaName(),
schemaType.version(),
schemaType.typeClass(),
⋮----
throw new SchemaVersionNotFoundException(
⋮----
schemaType.typeClass());
⋮----
throw new SchemaNotFoundException(
⋮----
throw new SchemaException(
⋮----
public JsonSchema registerAndValidate(SchemaType schemaType, boolean forceRegisterOnIncompatibleSchema) throws SchemaException {
⋮----
String schemaName = schemaType.getSchemaName();
List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas(
⋮----
if (registeredSchemas.isEmpty()) {
if (!schemaType.external()) {
if (schemaType.version() == 1) {
⋮----
schemaRegistryClient.register(
⋮----
throw new PreviousSchemaVersionMissingException(
⋮----
if (schemaType.external() && schemaType.relaxExternalValidation()) {
⋮----
if (!registeredSchema.deepEquals(localSchema)) {
⋮----
if (!Objects.equals(registeredSchema.toString(), localSchema.toString())) {
List<Difference> violatingDifferences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());
⋮----
logger.warn("Found an incompatible schema for {} v{} but forceRegisterOnIncompatibleSchema=true. Overwriting existing entry in SchemaRegistry", schemaName, schemaType.version());
⋮----
schemaRegistryClient.deleteSchemaVersion(
⋮----
"" + schemaType.version());
// then do a hard delete of the version
⋮----
"" + schemaType.version(),
⋮----
// and recreate it
⋮----
logger.error(
⋮----
} else if (schemaType.external()) {
// we did not find any schema with the exact version.
// since we are registering the type ourselves, this is an error
⋮----
// ensure we have an ordered list of schemas
registeredSchemas.sort(Comparator.comparingInt(ParsedSchema::version));
// see if the new version is exactly one higher than the last version
if (schemaType.version() != registeredSchemas.getLast().version() + 1) {
throw new InvalidSchemaVersionException(
⋮----
registeredSchemas.getLast().version(),
⋮----
// see if the new schema is backwards compatible with the previous ones
List<String> compatibilityErrors = localSchema.isCompatible(CompatibilityLevel.BACKWARD_TRANSITIVE,
registeredSchemas.stream().map(SimpleParsedSchemaHolder::new)
.collect(Collectors.toList()));
if (compatibilityErrors.isEmpty()) {
// register the new schema
⋮----
// incomp
throw new SchemaNotBackwardsCompatibleException(
⋮----
// schema is fine, return
⋮----
public JsonSchema generateJsonSchema(SchemaType schemaType) {
return new JsonSchema(schemaGeneratorTheadLocal.get().generateSchema(schemaType.typeClass()), List.of(), Map.of(), schemaType.version());
⋮----
private int getSchemaVersion(SchemaType schemaType, ParsedSchema parsedSchema) {
⋮----
return schemaRegistryClient.getVersion(schemaType.getSchemaName(), parsedSchema);
⋮----
throw new RuntimeException(e);
⋮----
private SchemaGenerator createJsonSchemaGenerator(ObjectMapper objectMapper) {
SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapper,
⋮----
configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
⋮----
configBuilder.with(new JacksonModule());
configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
⋮----
configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
JsonNode typeNode = collectedTypeAttributes.get("type");
if (typeNode.isArray()) {
((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
⋮----
collectedTypeAttributes.put("type", "string");
⋮----
return new SchemaGenerator(configBuilder.build());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/PreviousSchemaVersionMissingException.java
================
public class PreviousSchemaVersionMissingException extends SchemaException {
⋮----
public int getSchemaVersion() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaException.java
================
public class SchemaException extends RuntimeException {
⋮----
public String getSchemaIdentifier() {
⋮----
public Class<?> getImplementationClass() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaNotBackwardsCompatibleException.java
================
public class SchemaNotBackwardsCompatibleException extends SchemaException {
⋮----
public int getPreviousSchemaVersion() {
⋮----
public int getSchemaVersion() {
⋮----
public List<String> getDifferences() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaNotFoundException.java
================
public class SchemaNotFoundException extends SchemaException {
⋮----
super("Schema "+schemaIdentifier+" for class "+implementationClass.getName()+" Not Found", schemaIdentifier, implementationClass);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaVersionNotFoundException.java
================
public class SchemaVersionNotFoundException extends SchemaException {
⋮----
public int getSchemaVersion() {

================
File: main/shared/src/main/java/org/elasticsoftware/akces/serialization/AkcesControlRecordSerde.java
================
public final class AkcesControlRecordSerde implements Serde<AkcesControlRecord> {
⋮----
this.serializer = new SerializerImpl(objectMapper);
this.deserializer = new DeserializerImpl(objectMapper);
⋮----
public Serializer<AkcesControlRecord> serializer() {
⋮----
public Deserializer<AkcesControlRecord> deserializer() {
⋮----
private static class SerializerImpl implements Serializer<AkcesControlRecord> {
⋮----
public byte[] serialize(String topic, AkcesControlRecord data) {
⋮----
return objectMapper.writeValueAsBytes(csr);
⋮----
throw new SerializationException("Unsupported AkcesControlRecord type " + data.getClass().getSimpleName());
⋮----
throw new SerializationException(e);
⋮----
private static class DeserializerImpl implements Deserializer<AkcesControlRecord> {
⋮----
public AkcesControlRecord deserialize(String topic, byte[] data) {
⋮----
} else if (topic.endsWith("Akces-Control")) {
return objectMapper.readValue(data, AkcesControlRecord.class);
⋮----
throw new SerializationException("Unsupported topic " + topic);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/serialization/BigDecimalSerializer.java
================
public final class BigDecimalSerializer extends StdSerializer<BigDecimal> {
⋮----
public void serialize(BigDecimal value, JsonGenerator gen, SerializerProvider provider) throws IOException {
gen.writeString(value.toPlainString());
⋮----
public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint)
⋮----
visitor.expectStringFormat(typeHint);

================
File: main/shared/src/main/java/org/elasticsoftware/akces/serialization/ProtocolRecordSerde.java
================
public final class ProtocolRecordSerde implements Serde<ProtocolRecord> {
⋮----
private final ObjectMapper objectMapper = new ProtobufMapper();
⋮----
ProtobufSchema domainEventRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(domainEventRecordProto));
ProtobufSchema aggregateStateRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(aggregateStateRecordProto));
ProtobufSchema commandRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(commandRecordProto));
ProtobufSchema gdprKeyRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(gdprKeyRecordProto));
ProtobufSchema commandResponseRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(commandResponseRecordProto));
serializer = new SerializerImpl(objectMapper.writer(domainEventRecordSchema),
objectMapper.writer(aggregateStateRecordSchema),
objectMapper.writer(commandRecordSchema),
objectMapper.writer(gdprKeyRecordSchema),
objectMapper.writer(commandResponseRecordSchema));
deserializer = new DeserializerImpl(objectMapper.readerFor(DomainEventRecord.class).with(domainEventRecordSchema),
objectMapper.readerFor(AggregateStateRecord.class).with(aggregateStateRecordSchema),
objectMapper.readerFor(CommandRecord.class).with(commandRecordSchema),
objectMapper.readerFor(GDPRKeyRecord.class).with(gdprKeyRecordSchema),
objectMapper.readerFor(CommandResponseRecord.class).with(commandResponseRecordSchema));
⋮----
throw new SerializationException(e);
⋮----
public void configure(Map<String, ?> configs, boolean isKey) {
⋮----
public void close() {
⋮----
public Serializer<ProtocolRecord> serializer() {
⋮----
public Deserializer<ProtocolRecord> deserializer() {
⋮----
private static class SerializerImpl implements Serializer<ProtocolRecord> {
⋮----
public byte[] serialize(String topic, ProtocolRecord data) {
⋮----
return domainEventRecordWriter.writeValueAsBytes(r);
⋮----
return aggregateStateRecordWriter.writeValueAsBytes(r);
⋮----
return commandRecordWriter.writeValueAsBytes(r);
⋮----
return commandResponseRecordWriter.writeValueAsBytes(e);
⋮----
return gdprKeyRecordWriter.writeValueAsBytes(r);
⋮----
throw new SerializationException("Unsupported ProtocolRecord type " + data.getClass().getSimpleName());
⋮----
private static class DeserializerImpl implements Deserializer<ProtocolRecord> {
⋮----
public ProtocolRecord deserialize(String topic, byte[] data) {
⋮----
} else if (topic.endsWith("DomainEvents") || topic.endsWith("DomainEventIndex")) {
return domainEventRecordReader.readValue(data);
} else if (topic.endsWith("AggregateState")) {
return aggregateStateRecordReader.readValue(data);
} else if (topic.endsWith("Commands")) {
return commandRecordReader.readValue(data);
} else if (topic.endsWith("CommandResponses")) {
return commandResponseRecordReader.readValue(data);
} else if (topic.endsWith("GDPRKeys")) {
return gdprKeyRecordReader.readValue(data);
⋮----
throw new SerializationException("Unsupported topic name " + topic + " cannot determine ProtocolRecordType");

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/EnvironmentPropertiesPrinter.java
================
public class EnvironmentPropertiesPrinter {
private static final Logger logger = LoggerFactory.getLogger(EnvironmentPropertiesPrinter.class);
⋮----
public void handleContextRefreshed(ContextRefreshedEvent event) {
ConfigurableEnvironment env = (ConfigurableEnvironment) event.getApplicationContext().getEnvironment();
logger.info("******* Environment Properties *******");
env.getPropertySources()
.stream()
.filter(ps -> ps instanceof MapPropertySource)
.map(ps -> ((MapPropertySource) ps).getSource().keySet())
.flatMap(Collection::stream)
.distinct()
.filter(key -> key.startsWith("akces.") ||
key.startsWith("spring.") ||
key.startsWith("management.") ||
key.startsWith("server."))
.sorted()
.forEach(key -> logger.info("{}={}", key, env.getProperty(key)));
logger.info("**************************************");

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/HostUtils.java
================
public class HostUtils {
⋮----
public static String getHostName() {
⋮----
String hostName = System.getenv("HOSTNAME");
⋮----
if (hostName == null || hostName.isEmpty()) {
⋮----
InetAddress addr = InetAddress.getLocalHost();
hostName = addr.getHostName();

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/KafkaSender.java
================
public class KafkaSender {
⋮----
public static <K, V> Future<RecordMetadata> send(Producer<K, V> producer, ProducerRecord<K, V> producerRecord) {
return send(producer, producerRecord, null);
⋮----
public static <K, V> Future<RecordMetadata> send(Producer<K, V> producer, ProducerRecord<K, V> producerRecord, Callback callback) {
Future<RecordMetadata> future = producer.send(producerRecord, callback);
if (future.isDone()) {
⋮----
future.get();
⋮----
if (e.getCause() instanceof ApiException) {
throw (ApiException) e.getCause();
} else if (e.getCause() instanceof RuntimeException) {
throw (RuntimeException) e.getCause();
⋮----
throw new RuntimeException(e.getCause());

================
File: main/shared/src/main/java/org/elasticsoftware/akces/util/KafkaUtils.java
================
public final class KafkaUtils {
⋮----
public static String getIndexTopicName(String indexName, String indexKey) {
⋮----
public static NewTopic createCompactedTopic(String name, int numPartitions, short replicationFactor) {
NewTopic topic = new NewTopic(name, numPartitions, replicationFactor);
⋮----
String minInSyncReplicas = String.valueOf(calculateQuorum(replicationFactor));
return topic.configs(Map.of(
⋮----
public static int calculateQuorum(short replicationFactor) {

================
File: main/shared/src/test/java/org/elasticsoftware/akces/gdpr/jackson/AkcesGDPRModuleTests.java
================
public class AkcesGDPRModuleTests {
⋮----
public void testVersion() {
Version version = AkcesGDPRModule.generateVersion(Semver.parse("0.11.0-SNAPSHOT"));
Assertions.assertNotEquals(Version.unknownVersion(), version);
Assertions.assertEquals(new Version(0, 11, 0, "SNAPSHOT", "org.elasticsoftwarefoundation.akces", "akces-runtime"), version);
⋮----
public void testVersionFromManifest() {
⋮----
AkcesGDPRModule akcesGDPRModule = new AkcesGDPRModule();
Version version = akcesGDPRModule.version();
Assertions.assertEquals(Version.unknownVersion(), version);

================
File: main/shared/src/test/java/org/elasticsoftware/akces/gdpr/GDPRContextTests.java
================
public class GDPRContextTests {
⋮----
public void testEncryptDecrypt() {
GDPRContext ctx = new EncryptingGDPRContext("ef234add-e0df-4769-b5f4-612a3207bad3", GDPRKeyUtils.createKey().getEncoded(), true);
String encryptedData = ctx.encrypt("test");
System.out.println(encryptedData);
String decryptedData = ctx.decrypt(encryptedData);
Assertions.assertEquals("test", decryptedData);
⋮----
public void testEncrypt() {
⋮----
String name = ctx.encrypt("Jasin Terlouw");
String street = ctx.encrypt("Gershwinstraat 125");
System.out.println("name: " + name);
System.out.println("street: " + street);
Assertions.assertEquals("Jasin Terlouw", ctx.decrypt(name));
Assertions.assertEquals("Gershwinstraat 125", ctx.decrypt(street));
⋮----
public void testEncryptForTests() {
⋮----
String firstName = ctx.encrypt("Fahim");
String lastName = ctx.encrypt("Zuijderwijk");
String email = ctx.encrypt("FahimZuijderwijk@jourrapide.com");
System.out.println(firstName);
System.out.println(lastName);
System.out.println(email);
Assertions.assertEquals("Fahim", ctx.decrypt(firstName));
Assertions.assertEquals("Zuijderwijk", ctx.decrypt(lastName));
Assertions.assertEquals("FahimZuijderwijk@jourrapide.com", ctx.decrypt(email));
⋮----
public void testBadPaddingException() {
⋮----
GDPRContext ctx = new EncryptingGDPRContext(aggregateId, key, true);
⋮----
ctx.decrypt(encrypted);
⋮----
String encryptedFirstName = ctx.encrypt("Fahim");
String descryptedFirstName = ctx.decrypt(encryptedFirstName);
⋮----
Assertions.assertEquals("Fahim", descryptedFirstName);
⋮----
public void testECBEncryption() {
⋮----
GDPRContext ctx = new EncryptingGDPRContext(aggregateId, GDPRKeyUtils.createKey().getEncoded(), false);
⋮----
public void testECBEncryptionWithOtherContext() {
⋮----
SecretKeySpec secretKey = GDPRKeyUtils.createKey();
GDPRContext one = new EncryptingGDPRContext(aggregateId, secretKey.getEncoded(), false);
GDPRContext two = new EncryptingGDPRContext(aggregateId, secretKey.getEncoded(), false);
⋮----
String encryptedFirstName = one.encrypt("Fahim");
String decryptedFirstName = two.decrypt(encryptedFirstName);
⋮----
Assertions.assertEquals("Fahim", decryptedFirstName);

================
File: main/shared/src/test/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepositoryTests.java
================
public class RocksDBGDPRContextRepositoryTests {
⋮----
public void testWriteInTransaction() {
ProtocolRecordSerde serde = new ProtocolRecordSerde();
GDPRContextRepository repository = new RocksDBGDPRContextRepository(
⋮----
serde.serializer(),
serde.deserializer());
⋮----
repository.process(List.of(new ConsumerRecord<>(
⋮----
new GDPRKeyRecord(
⋮----
GDPRKeyUtils.createKey().getEncoded()))));
⋮----
Assertions.assertTrue(repository.exists("4117b11f-3dde-4b71-b80c-fa20a12d9add"));

================
File: main/shared/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.8.13-SNAPSHOT</version>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Shared Classes</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-shared</artifactId>

    <properties>
        <maven.compiler.source>17</maven.compiler.source>
        <maven.compiler.target>17</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.semver4j</groupId>
            <artifactId>semver4j</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.annotation</groupId>
            <artifactId>jakarta.annotation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.rocksdb</groupId>
            <artifactId>rocksdbjni</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.ben-manes.caffeine</groupId>
            <artifactId>caffeine</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

================
File: main/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-parent</artifactId>
        <version>0.8.13-SNAPSHOT</version>
    </parent>

    <artifactId>akces-framework-main</artifactId>
    <packaging>pom</packaging>

    <name>Elastic Software Foundation :: Akces :: Main module</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <protobuf.version>4.30.0</protobuf.version>
        <log4j.version>2.24.3</log4j.version>
        <jackson.version>2.18.3</jackson.version>
        <slf4j.version>2.0.17</slf4j.version>
        <javassist.version>3.30.2-GA</javassist.version>
        <lz4.version>1.8.0</lz4.version>
        <kafka.version>3.8.1</kafka.version>
        <java-uuid-generator.version>5.1.0</java-uuid-generator.version>
        <rocksdb.version>9.10.0</rocksdb.version>
        <semver4j.version>5.6.0</semver4j.version>

        <testng-version>7.11.0</testng-version>
        <awaitility.version>4.3.0</awaitility.version>
        <persistence-api.version>2.2</persistence-api.version>
        <plexus-utils.version>4.0.2</plexus-utils.version>
        <netty.version>4.1.119.Final</netty.version>
        <micrometer.version>1.14.4</micrometer.version>
        <fast-uuid.version>0.2.0</fast-uuid.version>
        <confluent.version>7.9.0</confluent.version>
        <victools.version>4.37.0</victools.version>
        <joda-time.version>2.13.1</joda-time.version>
        <logback.version>1.5.17</logback.version>
        <testcontainers.version>1.20.6</testcontainers.version>
    </properties>

    <scm>
        <connection>scm:git:git@github.com:elasticsoftwarefoundation/elasticactors.git</connection>
        <developerConnection>scm:git:git@github.com:elasticsoftwarefoundation/elasticactors.git
        </developerConnection>
        <url>https://github.com/elasticsoftwarefoundation/elasticactors</url>
        <tag>v0.3.4</tag>
    </scm>

    <distributionManagement>
        <repository>
            <id>github</id>
            <name>GitHub Packages</name>
            <url>https://maven.pkg.github.com/elasticsoftwarefoundation/akces-framework</url>
        </repository>








    </distributionManagement>

    <repositories>
        <repository>
            <id>central</id>
            <url>https://repo.maven.apache.org/maven2</url>
        </repository>
        <repository>
            <id>Confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
    </repositories>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>com.fasterxml.jackson</groupId>
                <artifactId>jackson-bom</artifactId>
                <version>${jackson.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>io.netty</groupId>
                <artifactId>netty-bom</artifactId>
                <version>${netty.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>com.google.guava</groupId>
                <artifactId>guava-bom</artifactId>
                <version>${guava.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>io.micrometer</groupId>
                <artifactId>micrometer-bom</artifactId>
                <version>${micrometer.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>jakarta.validation</groupId>
                <artifactId>jakarta.validation-api</artifactId>
                <version>3.1.1</version>
            </dependency>
            <dependency>
                <groupId>jakarta.inject</groupId>
                <artifactId>jakarta.inject-api</artifactId>
                <version>2.0.1</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-generator</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-module-jakarta-validation</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-module-jackson</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>commons-validator</groupId>
                <artifactId>commons-validator</artifactId>
                <version>1.9.0</version>
            </dependency>
            <dependency>
                <groupId>org.apache.commons</groupId>
                <artifactId>commons-compress</artifactId>
                <version>1.27.1</version>
            </dependency>
            <dependency>
                <groupId>commons-io</groupId>
                <artifactId>commons-io</artifactId>
                <version>2.18.0</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-json-serializer</artifactId>
                <version>${confluent.version}</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-json-schema-serializer</artifactId>
                <version>${confluent.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>joda-time</groupId>
                        <artifactId>joda-time</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.jetbrains.kotlin</groupId>
                        <artifactId>kotlin-scripting-compiler-embeddable</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.jetbrains</groupId>
                        <artifactId>annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.scala-lang</groupId>
                        <artifactId>scala-library</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>commons-validator</groupId>
                        <artifactId>commons-validator</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.checkerframework</groupId>
                        <artifactId>checker-qual</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.guava</groupId>
                        <artifactId>guava</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>joda-time</groupId>
                <artifactId>joda-time</artifactId>
                <version>${joda-time.version}</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-protobuf-serializer</artifactId>
                <version>${confluent.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.jetbrains.kotlin</groupId>
                        <artifactId>*</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.errorprone</groupId>
                        <artifactId>error_prone_annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.apache.commons</groupId>
                        <artifactId>commons-compress</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.checkerframework</groupId>
                        <artifactId>checker-qual</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.squareup.okio</groupId>
                        <artifactId>okio-jvm</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.j2objc</groupId>
                        <artifactId>j2objc-annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.protobuf</groupId>
                        <artifactId>protobuf-java</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.lz4</groupId>
                <artifactId>lz4-java</artifactId>
                <version>${lz4.version}</version>
            </dependency>
            <dependency>
                <groupId>com.eatthepath</groupId>
                <artifactId>fast-uuid</artifactId>
                <version>${fast-uuid.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-beans</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-context</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.aspectj</groupId>
                <artifactId>aspectjrt</artifactId>
                <version>${aspectj.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-tx</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-core</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-context-support</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-api</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>jcl-over-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>jul-to-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>log4j-over-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>ch.qos.logback</groupId>
                <artifactId>logback-classic</artifactId>
                <version>${logback.version}</version>
            </dependency>
            <dependency>
                <groupId>ch.qos.logback</groupId>
                <artifactId>logback-core</artifactId>
                <version>${logback.version}</version>
            </dependency>
            <dependency>
                <groupId>org.javassist</groupId>
                <artifactId>javassist</artifactId>
                <version>${javassist.version}</version>
            </dependency>
            <dependency>
                <groupId>com.fasterxml.uuid</groupId>
                <artifactId>java-uuid-generator</artifactId>
                <version>${java-uuid-generator.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-webmvc</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-aspects</artifactId>
                <version>${spring.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.aspectj</groupId>
                        <artifactId>aspectjweaver</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-aop</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>javax.persistence</groupId>
                <artifactId>javax.persistence-api</artifactId>
                <version>${persistence-api.version}</version>
            </dependency>
            <dependency>
                <groupId>com.google.protobuf</groupId>
                <artifactId>protobuf-java</artifactId>
                <version>${protobuf.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-clients</artifactId>
                <version>${kafka.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-streams</artifactId>
                <version>${kafka.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.rocksdb</groupId>
                        <artifactId>rocksdbjni</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.awaitility</groupId>
                <artifactId>awaitility</artifactId>
                <version>${awaitility.version}</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.plexus</groupId>
                <artifactId>plexus-utils</artifactId>
                <version>${plexus-utils.version}</version>
            </dependency>
            <dependency>
                <groupId>javax.annotation</groupId>
                <artifactId>javax.annotation-api</artifactId>
                <version>${javax-annotation-api.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testng</groupId>
                <artifactId>testng</artifactId>
                <version>${testng-version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.yaml</groupId>
                        <artifactId>snakeyaml</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.mockito</groupId>
                <artifactId>mockito-core</artifactId>
                <version>${mockito.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-test</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>kafka</artifactId>
                <version>${testcontainers.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>junit-jupiter</artifactId>
                <version>${testcontainers.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.ben-manes.caffeine</groupId>
                <artifactId>caffeine</artifactId>
                <version>${caffeine.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>com.google.errorprone</groupId>
                        <artifactId>error_prone_annotations</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.semver4j</groupId>
                <artifactId>semver4j</artifactId>
                <version>${semver4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-simple</artifactId>
                <version>${slf4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.apache.logging.log4j</groupId>
                <artifactId>log4j-core</artifactId>
                <version>${log4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>${spring-boot.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
            <dependency>
                <groupId>org.rocksdb</groupId>
                <artifactId>rocksdbjni</artifactId>
                <version>${rocksdb.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <modules>
        <module>api</module>
        <module>runtime</module>
        <module>client</module>
        <module>shared</module>
        <module>query-support</module>
    </modules>

    <build>
        <resources>
            <resource>
                <filtering>false</filtering>
                <directory>${basedir}/src/main/resources</directory>
                <includes>
                    <include>*.xml</include>
                    <include>*.yaml</include>
                    <include>*.properties</include>
                    <include>META-INF/*</include>
                    <include>META-INF/services/*</include>
                    <include>META-INF/spring/*</include>
                    <include>protobuf/*.proto</include>
                </includes>
            </resource>
        </resources>
        <testResources>
            <testResource>
                <filtering>true</filtering>
                <directory>${basedir}/src/test/resources</directory>
                <includes>
                    <include>*.xml</include>
                    <include>*.properties</include>
                    <include>**/*.yaml</include>
                    <include>META-INF/*</include>
                </includes>
            </testResource>
        </testResources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>

================
File: FRAMEWORK_OVERVIEW.md
================
# Comprehensive Review of Akces Framework

After carefully analyzing the codebase, I can provide an updated comprehensive overview of the Akces Framework, building upon and refining the existing FRAMEWORK_OVERVIEW.md.

## Main Purpose

Akces is a sophisticated event-sourcing and CQRS (Command Query Responsibility Segregation) framework built on Apache Kafka. Its primary purpose is to provide developers with a structured approach to building distributed, event-driven applications with a clear separation between write and read concerns. The framework handles the complex infrastructure required for event-sourcing while allowing developers to focus on domain logic.

Key goals of the framework include:
- Simplifying the implementation of event-sourced applications
- Providing a scalable architecture for distributed processing
- Enforcing clean separation between command and query responsibilities
- Supporting privacy-by-design through GDPR-compliant data handling
- Enabling schema evolution with backward compatibility checks

## Core Architectural Components

Akces is organized around several key architectural components that work together:

### 1. Aggregate Module

The aggregate module is responsible for handling commands and maintaining state through event sourcing. Key components include:

- **Aggregates**: Domain entities that encapsulate business logic and respond to commands
- **Aggregate States**: Immutable representations of aggregate state
- **Command Handlers**: Process commands and emit domain events
- **Event Sourcing Handlers**: Apply events to update aggregate state
- **Event Handlers**: React to events to produce further events
- **Event Bridge Handlers**: Connect events from one aggregate to commands on another

The runtime leverages Kafka partitioning for horizontal scaling, with each partition handling a subset of aggregates based on their IDs.

### 2. Command Processing

The command processing pipeline includes:

- **Command Bus**: Routes commands to appropriate aggregates
- **Command Validation**: Schema-based validation using JSON Schema
- **Command Execution**: Transactional processing that produces events
- **Command Response Handling**: Collects and returns resulting events

### 3. Query Model System

For the read side, Akces provides:

- **Query Models**: Domain-specific projections optimized for reading
- **Query Model States**: Immutable state representations 
- **Query Model Event Handlers**: Update query models based on domain events
- **Database Models**: Persistent storage models updated from events
- **JDBC/JPA Integration**: Support for different database technologies

### 4. Process Managers

To orchestrate complex workflows:

- **Process Managers**: Coordinate interactions between multiple aggregates
- **Process States**: Track the status of long-running processes
- **AkcesProcess**: Domain-specific process representation

### 5. GDPR Compliance Layer

For handling sensitive data:

- **PIIData Annotation**: Mark fields containing personal data
- **GDPR Context**: Encryption/decryption context for an aggregate
- **Transparent Serialization**: Automatic encryption/decryption during serialization
- **Key Management**: Secure handling of encryption keys

## Technical Implementation Details

### Event Sourcing Implementation

The event sourcing mechanism in Akces follows these principles:

1. **Immutable Events**: All domain events are immutable records of facts
2. **Event Storage**: Events are stored in Kafka topics, partitioned by aggregate ID
3. **State Reconstruction**: Aggregate state is derived by replaying events
4. **State Snapshots**: RocksDB is used to maintain efficient state snapshots

The `KafkaAggregateRuntime` class manages the event sourcing logic, handling:
- Command processing and validation
- Event application to state
- State persistence
- Event publishing

### Partitioning and Scalability

Akces achieves scalability through:

1. **Partition-based Processing**: Each instance processes specific partitions
2. **Consistent Hashing**: Aggregate IDs are consistently hashed to partitions
3. **Parallel Processing**: Multiple partitions can be processed concurrently
4. **Atomic Transactions**: Kafka transactions ensure atomicity of operations

The `AggregatePartition` class handles partition-specific processing, managing:
- Command handling for a partition
- Event processing for a partition
- State management for a partition

### Schema Evolution

Akces provides sophisticated schema evolution through:

1. **Schema Registry Integration**: Works with Confluent Schema Registry
2. **Schema Versioning**: Clear versioning of all commands and events
3. **Compatibility Checking**: Ensures backward compatibility
4. **JSON Schema Generation**: Automatic schema generation from classes

The `KafkaSchemaRegistry` class handles schema management, providing:
- Schema registration and validation
- Compatibility checking
- Schema versioning support

### GDPR Compliance Implementation

The GDPR compliance layer uses:

1. **AES Encryption**: For sensitive data fields
2. **Jackson Serialization Integration**: Custom serializers/deserializers
3. **Key Management**: Secure storage of encryption keys in Kafka
4. **Annotation-based Marking**: Easy identification of sensitive fields

### Query Model Implementation

The query model system provides:

1. **Event-driven Updates**: Query models updated via events
2. **State Hydration**: Efficient state loading and caching
3. **Database Integration**: Support for JDBC and JPA databases
4. **Partition-aware Processing**: Scalable across nodes

## Module Structure

Akces is organized into several Maven modules:

1. **api**: Core interfaces and annotations defining the programming model
   - Aggregate, Command, and Event interfaces
   - Handler annotations for commands and events
   - Query model and database model interfaces

2. **runtime**: Implements the core event sourcing runtime
   - Aggregate runtime for command/event handling
   - State repositories (RocksDB/in-memory)
   - Kafka integration
   - Command handling pipeline

3. **shared**: Common utilities and shared functionality
   - Protocol records for communication
   - GDPR compliance utilities
   - Serialization/deserialization support
   - Schema registry integration

4. **client**: Client-side library for interacting with aggregates
   - Command sending
   - Response handling
   - Discovery of available aggregates

5. **query-support**: Support for query models and database models
   - Query model runtime 
   - Database model support
   - Event handling for models
   - State hydration

## Programming Model

Akces provides a clean, annotation-based programming model:

### Defining Aggregates

```java
@AggregateInfo(value = "Wallet", version = 1)
public class Wallet implements Aggregate<WalletState> {
    @CommandHandler(create = true)
    public Stream<DomainEvent> create(CreateWalletCommand cmd, WalletState isNull) {
        // Command handling logic
    }
    
    @EventSourcingHandler(create = true)
    public WalletState create(WalletCreatedEvent event, WalletState isNull) {
        // Event application logic
    }
}
```

### Defining Commands and Events

```java
@CommandInfo(type = "CreateWallet", version = 1)
public record CreateWalletCommand(@AggregateIdentifier String id, String currency) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@DomainEventInfo(type = "WalletCreated", version = 1)
public record WalletCreatedEvent(@AggregateIdentifier String id) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### Defining Query Models

```java
@QueryModelInfo(value = "WalletQuery", version = 1, indexName = "Wallets")
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
    @QueryModelEventHandler(create = true)
    public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
        // Create logic
    }
}
```

### GDPR Annotation

```java
public record UserInfo(
    @AggregateIdentifier String userId,
    @PIIData String firstName,
    @PIIData String lastName,
    @PIIData String email
) implements AggregateState {
    // ...
}
```

## Runtime Components

The key runtime components include:

1. **AkcesAggregateController**: Manages aggregate partitions and lifecycle
2. **AkcesClientController**: Handles command sending and response processing
3. **AkcesQueryModelController**: Manages query model state hydration
4. **AkcesDatabaseModelController**: Handles database model updates
5. **AggregatePartition**: Processes commands and events for a partition

## Enhanced Features Not Fully Captured in Original Overview

### 1. Robust Partition Management

The framework provides sophisticated partition management with:
- Automatic rebalancing when nodes join/leave
- Coordinated partition shutdown and cleanup
- Atomic transaction processing within partitions
- Optimized state loading from RocksDB

### 2. Advanced Schema Handling

Schema management is more sophisticated than initially described:
- Diff-based compatibility checking 
- Schema evolution with strict version checking
- Automatic schema registration with configurable validation
- Support for external schemas with relaxed validation

### 3. Transaction Support

The transactional model provides:
- Exactly-once processing semantics
- Atomic command handling with consistent state updates
- Transaction isolation for command processing
- Consistent offset management

### 4. Optimized State Handling

State management is highly optimized:
- Efficient RocksDB storage with custom serialization
- In-memory caching for high-performance scenarios
- Partition-aware state access patterns
- Transactional state updates coordinated with events

### 5. Flexible Deployment Models

The framework supports different deployment topologies:
- Separate command and query services
- Combined services for smaller applications
- Scalable partitioning across multiple nodes
- Stream processing integration

### 6. Process Manager Capabilities

Process managers provide orchestration capabilities:
- Coordination of multi-step processes
- State tracking for long-running operations
- Event-driven process advancement
- Error handling and compensation

## Conclusion

Akces Framework provides a comprehensive solution for building event-sourced, CQRS-based applications with a focus on scalability, resilience, and privacy. Its clean programming model, combined with a robust runtime implementation, makes it well-suited for complex domain problems requiring sophisticated state management and high scalability.

The framework's integration with Kafka provides a reliable foundation for distributed processing, while its schema management and GDPR compliance features address important enterprise concerns. The separation between command handling and query models follows best practices for complex domain modeling while maintaining high performance for read operations.

================
File: README.md
================
# Akces Framework

## Overview

Akces Framework is a robust CQRS (Command Query Responsibility Segregation) and Event Sourcing framework built on Apache Kafka. The framework provides a comprehensive infrastructure for building distributed, event-driven applications with a clean separation between command and query operations.

At its core, Akces implements the full event sourcing pattern, capturing all changes to application state as a sequence of events. These events can be replayed to reconstruct the state at any point in time, providing a complete audit trail and enabling powerful temporal queries.

The framework leverages Kafka's distributed architecture for reliable event storage and processing, making it highly scalable and resilient. It also provides built-in support for personal data protection (GDPR compliance), schema evolution, and efficient state management.

## Core Concepts

- **Aggregates**: Domain entities that encapsulate business logic and maintain state through events
- **Commands**: Requests to change the state of an aggregate
- **Domain Events**: Facts that have occurred, representing state changes
- **Command Handlers**: Process commands and produce events
- **Event Sourcing Handlers**: Apply events to update aggregate state
- **Query Models**: Read-optimized projections of aggregate state
- **Database Models**: Persistent storage of aggregate data

## Main Features

### Command Handling
- **Command Bus**: A distributed command bus for routing commands to appropriate aggregates
- **Command Validation**: Automatic schema validation using JSON Schema
- **Command Handlers**: Annotation-based command handling with automatic event publishing
- **Transaction Support**: Transactional processing of commands

### Event Sourcing
- **Event Store**: Kafka-based event store for persisting all domain events
- **Event Handlers**: Annotation-based event handling for processing domain events
- **Event Sourcing Handlers**: Automatic state reconstruction from domain events
- **Event Bridging**: Bridge events between different aggregates

### Aggregate Management
- **Aggregate Runtimes**: Lifecycle management for aggregates
- **State Management**: Efficient state storage using RocksDB
- **Partitioning**: Automatic partitioning of aggregates across nodes for scalability
- **Event Indexing**: Automatic indexing of events for efficient querying

### Query Support
- **Query Models**: Build specialized read models from domain events
- **Database Models**: Automatically sync data to databases for efficient querying
- **Materialized Views**: Build and maintain materialized views of aggregate state
- **State Hydration**: Efficiently load and cache query model state

### Privacy & GDPR Support
- **PII Data Handling**: Built-in support for Personal Identifiable Information (PII)
- **Data Encryption**: Transparent encryption/decryption of sensitive data
- **Annotation-based PII Marking**: Easy identification of sensitive fields
- **Key Management**: Secure management of encryption keys

### Schema Management
- **Schema Evolution**: Support for evolving schemas with backward compatibility
- **Schema Registry Integration**: Works with Confluent Schema Registry for schema management
- **Schema Validation**: Automatic validation of commands and events against their schemas
- **Schema Compatibility Checks**: Ensure backward compatibility of schema changes

## Architecture

The framework consists of several modules:

- **api**: Core interfaces and annotations defining the framework's programming model
- **shared**: Common utilities, serialization, and GDPR support
- **runtime**: The runtime environment for aggregates, including command handling and event sourcing
- **client**: Client library for interacting with aggregate services
- **query-support**: Support for query models and database models

## Setup Instructions

### Prerequisites

- Java 17 or higher
- Apache Kafka 3.x
- Confluent Schema Registry
- Maven 3.6+

### Maven Dependencies

Add the following to your `pom.xml`:

```xml
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-api</artifactId>
    <version>0.8.1</version>
</dependency>

<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-client</artifactId>
    <version>0.8.1</version>
</dependency>

<!-- For running aggregates -->
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-runtime</artifactId>
    <version>0.8.1</version>
</dependency>

<!-- For query models -->
<dependency>
    <groupId>org.elasticsoftwarefoundation.akces</groupId>
    <artifactId>akces-query-support</artifactId>
    <version>0.8.1</version>
</dependency>
```

### Configuration

Create an `application.yaml` file with the following configuration:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      enable-auto-commit: false
      isolation-level: read_committed
      max-poll-records: 500
      heartbeat-interval: 2000
      auto-offset-reset: latest
      properties:
        max.poll.interval.ms: 10000
        session.timeout.ms: 30000
        partition.assignment.strategy: org.apache.kafka.clients.consumer.CooperativeStickyAssignor
    producer:
      acks: all
      retries: 2147483647
      properties:
        enable.idempotence: true
        max.in.flight.requests.per.connection: 1

akces:
  schemaregistry:
    url: http://localhost:8081
  rocksdb:
    baseDir: /tmp/akces
```

## Usage Examples

### Define an Aggregate

```java
@AggregateInfo(value = "Wallet", version = 1, generateGDPRKeyOnCreate = true, indexed = true, indexName = "Wallets")
public final class Wallet implements Aggregate<WalletState> {
    @Override
    public Class<WalletState> getStateClass() {
        return WalletState.class;
    }

    @CommandHandler(create = true, produces = {WalletCreatedEvent.class, BalanceCreatedEvent.class})
    public Stream<DomainEvent> create(CreateWalletCommand cmd, WalletState isNull) {
        return Stream.of(new WalletCreatedEvent(cmd.id()), new BalanceCreatedEvent(cmd.id(), cmd.currency()));
    }

    @EventSourcingHandler(create = true)
    public WalletState create(WalletCreatedEvent event, WalletState isNull) {
        return new WalletState(event.id(), new ArrayList<>());
    }
    
    @EventSourcingHandler
    public WalletState createBalance(BalanceCreatedEvent event, WalletState state) {
        List<WalletState.Balance> balances = new ArrayList<>(state.balances());
        balances.add(new WalletState.Balance(event.currency(), BigDecimal.ZERO));
        return new WalletState(state.id(), balances);
    }
    
    @CommandHandler(produces = {WalletCreditedEvent.class, InvalidAmountErrorEvent.class, InvalidCurrencyErrorEvent.class})
    public Stream<DomainEvent> credit(CreditWalletCommand cmd, WalletState currentState) {
        WalletState.Balance balance = currentState.balances().stream()
                .filter(b -> b.currency().equals(cmd.currency()))
                .findFirst().orElse(null);
                
        if (balance == null) {
            return Stream.of(new InvalidCurrencyErrorEvent(cmd.id(), cmd.currency()));
        }
        
        if (cmd.amount().compareTo(BigDecimal.ZERO) < 0) {
            return Stream.of(new InvalidAmountErrorEvent(cmd.id(), cmd.currency()));
        }
        
        return Stream.of(new WalletCreditedEvent(currentState.id(), cmd.currency(), cmd.amount(), 
                balance.amount().add(cmd.amount())));
    }
}
```

### Define an Aggregate State

```java
public record WalletState(String id, List<Balance> balances) implements AggregateState {
    @Override
    public String getAggregateId() {
        return id();
    }

    public record Balance(String currency, BigDecimal amount, BigDecimal reservedAmount) {
        public Balance(String currency, BigDecimal amount) {
            this(currency, amount, BigDecimal.ZERO);
        }

        public BigDecimal getAvailableAmount() {
            return amount.subtract(reservedAmount);
        }
    }
}
```

### Create Commands

```java
@CommandInfo(type = "CreateWallet", version = 1)
public record CreateWalletCommand(@AggregateIdentifier @NotNull String id, 
                                  @NotNull String currency) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@CommandInfo(type = "CreditWallet", version = 1)
public record CreditWalletCommand(@AggregateIdentifier @NotNull String id,
                                 @NotNull String currency,
                                 @NotNull BigDecimal amount) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### Create Events

```java
@DomainEventInfo(type = "WalletCreated", version = 1)
public record WalletCreatedEvent(@AggregateIdentifier @NotNull String id) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@DomainEventInfo(type = "BalanceCreated", version = 1)
public record BalanceCreatedEvent(@AggregateIdentifier @NotNull String id, 
                                 @NotNull String currency) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

@DomainEventInfo(type = "WalletCredited", version = 1)
public record WalletCreditedEvent(@AggregateIdentifier @NotNull String id,
                                 @NotNull String currency,
                                 @NotNull BigDecimal amount,
                                 @NotNull BigDecimal newBalance) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}
```

### Error Events

```java
@DomainEventInfo(type = "InvalidCurrencyError", version = 1)
public record InvalidCurrencyErrorEvent(@AggregateIdentifier @NotNull String walletId,
                                       @NotNull String currency) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return walletId();
    }
}

@DomainEventInfo(type = "InvalidAmountError", version = 1)
public record InvalidAmountErrorEvent(@AggregateIdentifier @NotNull String walletId,
                                     @NotNull String currency) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return walletId();
    }
}
```

### Sending Commands

```java
@Autowired
private AkcesClient akcesClient;

public void createWallet() {
    String walletId = UUID.randomUUID().toString();
    CreateWalletCommand command = new CreateWalletCommand(walletId, "USD");
    
    // Send command and get events synchronously
    List<DomainEvent> events = akcesClient.send("DEFAULT_TENANT", command)
        .toCompletableFuture()
        .join();
    
    // Or send command asynchronously
    akcesClient.sendAndForget("DEFAULT_TENANT", command);
}

public void creditWallet(String walletId, String currency, BigDecimal amount) {
    CreditWalletCommand command = new CreditWalletCommand(walletId, currency, amount);
    
    try {
        List<DomainEvent> events = akcesClient.send("DEFAULT_TENANT", command)
            .toCompletableFuture()
            .join();
            
        // Check if we received an error event
        if (events.stream().anyMatch(event -> event instanceof ErrorEvent)) {
            ErrorEvent error = (ErrorEvent) events.stream()
                .filter(event -> event instanceof ErrorEvent)
                .findFirst()
                .orElse(null);
            // Handle error
        }
    } catch (Exception e) {
        // Handle exceptions (validation errors, connectivity issues, etc.)
    }
}
```

### Create a Query Model

```java
@QueryModelInfo(value = "WalletQuery", version = 1, indexName = "Wallets")
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
    @Override
    public Class<WalletQueryModelState> getStateClass() {
        return WalletQueryModelState.class;
    }
    
    @Override
    public String getIndexName() {
        return "Wallets";
    }

    @QueryModelEventHandler(create = true)
    public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
        return new WalletQueryModelState(event.id(), List.of());
    }
    
    @QueryModelEventHandler
    public WalletQueryModelState createBalance(BalanceCreatedEvent event, WalletQueryModelState currentState) {
        WalletQueryModelState.Balance balance = new WalletQueryModelState.Balance(event.currency(), BigDecimal.ZERO);
        List<WalletQueryModelState.Balance> balances = new ArrayList<>(currentState.balances());
        balances.add(balance);
        return new WalletQueryModelState(currentState.walletId(), balances);
    }
    
    @QueryModelEventHandler
    public WalletQueryModelState creditWallet(WalletCreditedEvent event, WalletQueryModelState currentState) {
        return new WalletQueryModelState(
                currentState.walletId(),
                currentState.balances().stream().map(balance -> {
                    if (balance.currency().equals(event.currency())) {
                        return new WalletQueryModelState.Balance(
                                balance.currency(),
                                balance.amount().add(event.amount()),
                                balance.reservedAmount()
                        );
                    }
                    return balance;
                }).toList());
    }
}

public record WalletQueryModelState(String walletId, List<Balance> balances) implements QueryModelState {
    @Override
    public String getIndexKey() {
        return walletId();
    }
    
    public record Balance(String currency, BigDecimal amount, BigDecimal reservedAmount) {
        public Balance(String currency, BigDecimal amount) {
            this(currency, amount, BigDecimal.ZERO);
        }
        
        public BigDecimal getAvailableAmount() {
            return amount.subtract(reservedAmount);
        }
    }
}
```

### Query a Model

```java
@Autowired
private QueryModels queryModels;

public WalletQueryModelState getWallet(String walletId) {
    return queryModels.getHydratedState(WalletQueryModel.class, walletId)
        .toCompletableFuture()
        .join();
}

public void displayWalletBalances(String walletId) {
    try {
        WalletQueryModelState wallet = queryModels.getHydratedState(WalletQueryModel.class, walletId)
            .toCompletableFuture()
            .get(5, TimeUnit.SECONDS);
            
        wallet.balances().forEach(balance -> {
            System.out.printf("Currency: %s, Amount: %s, Available: %s%n", 
                balance.currency(), 
                balance.amount().toPlainString(), 
                balance.getAvailableAmount().toPlainString());
        });
    } catch (QueryModelIdNotFoundException e) {
        System.out.println("Wallet not found: " + e.getModelId());
    } catch (Exception e) {
        System.out.println("Error retrieving wallet: " + e.getMessage());
    }
}
```

### Create a Database Model

```java
@DatabaseModelInfo(value = "WalletDatabase", version = 1)
public class WalletDatabaseModel extends JdbcDatabaseModel {
    @DatabaseModelEventHandler
    public void handle(WalletCreatedEvent event) {
        jdbcTemplate.update("""
            INSERT INTO wallets (wallet_id, created_date) 
            VALUES (?, NOW())
            """, 
            event.id());
    }
    
    @DatabaseModelEventHandler
    public void handle(BalanceCreatedEvent event) {
        jdbcTemplate.update("""
            INSERT INTO wallet_balances (wallet_id, currency, amount) 
            VALUES (?, ?, ?)
            """, 
            event.id(), event.currency(), 0.00);
    }
    
    @DatabaseModelEventHandler
    public void handle(WalletCreditedEvent event) {
        jdbcTemplate.update("""
            UPDATE wallet_balances
            SET amount = ?
            WHERE wallet_id = ? AND currency = ?
            """,
            event.newBalance(), event.id(), event.currency());
    }
}
```

## GDPR and PII Data

The framework provides built-in support for Personal Identifiable Information (PII):

```java
@AggregateStateInfo(value = "AccountState", version = 1)
public record AccountState(@AggregateIdentifier String userId, 
                          String country, 
                          @PIIData String firstName, 
                          @PIIData String lastName, 
                          @PIIData String email) implements AggregateState {
    @Override
    public String getAggregateId() {
        return userId();
    }
}
```

PII data is automatically encrypted when stored and decrypted when retrieved. The encryption is transparent to the application code.

## Process Managers

Akces also supports process managers for coordinating complex workflows across multiple aggregates:

```java
@AggregateInfo(value = "OrderProcessManager", version = 1)
public class OrderProcessManager implements ProcessManager<OrderProcessManagerState, OrderProcess> {
    @Override
    public Class<OrderProcessManagerState> getStateClass() {
        return OrderProcessManagerState.class;
    }
    
    @EventHandler(create = true)
    public Stream<UserOrderProcessesCreatedEvent> create(AccountCreatedEvent event, OrderProcessManagerState isNull) {
        return Stream.of(new UserOrderProcessesCreatedEvent(event.userId()));
    }
    
    @CommandHandler
    public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {
        String orderId = UUID.randomUUID().toString();
        // Start a multi-step process
        getCommandBus().send(new ReserveAmountCommand(
                state.userId(),
                command.market().quoteCurrency(),
                command.quantity().multiply(command.limitPrice()),
                orderId));
        
        return Stream.of(new BuyOrderCreatedEvent(
                state.userId(),
                orderId,
                command.market(),
                command.quantity(),
                command.limitPrice(),
                command.clientReference()));
    }
    
    @EventHandler
    public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {
        OrderProcess orderProcess = state.getAkcesProcess(event.referenceId());
        
        if (orderProcess instanceof BuyOrderProcess) {
            return Stream.of(new BuyOrderPlacedEvent(
                    state.userId(), 
                    orderProcess.orderId(), 
                    orderProcess.market(), 
                    orderProcess.quantity(), 
                    orderProcess.limitPrice()));
        }
        
        return Stream.empty();
    }
    
    @EventHandler
    public Stream<DomainEvent> handle(InsufficientFundsErrorEvent errorEvent, OrderProcessManagerState state) {
        return Stream.of(state.getAkcesProcess(errorEvent.referenceId()).handle(errorEvent));
    }
}
```

## Running the Framework

### Aggregate Service

```java
@SpringBootApplication
public class AggregateServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(AggregateServiceApplication.class, args);
    }
}
```

### Query Service

```java
@SpringBootApplication
public class QueryServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(QueryServiceApplication.class, args);
    }
}
```

### Client Application

```java
@SpringBootApplication
public class ClientApplication {
    public static void main(String[] args) {
        SpringApplication.run(ClientApplication.class, args);
    }
    
    @Bean
    public CommandLineRunner commandLineRunner(AkcesClient akcesClient) {
        return args -> {
            // Create a new wallet
            String walletId = UUID.randomUUID().toString();
            CreateWalletCommand createCommand = new CreateWalletCommand(walletId, "USD");
            
            List<DomainEvent> createEvents = akcesClient.send(createCommand)
                .toCompletableFuture()
                .join();
                
            System.out.println("Wallet created: " + walletId);
            
            // Credit the wallet
            CreditWalletCommand creditCommand = new CreditWalletCommand(walletId, "USD", new BigDecimal("1000.00"));
            
            List<DomainEvent> creditEvents = akcesClient.send(creditCommand)
                .toCompletableFuture()
                .join();
                
            System.out.println("Wallet credited: " + walletId);
        };
    }
}
```

## Benefits of Using Akces Framework

- **Scalability**: Built on Kafka for horizontal scalability across distributed nodes
- **Reliability**: Event sourcing ensures data integrity and provides complete audit trails
- **Flexibility**: Clean separation of commands and queries following CQRS principles
- **Performance**: Efficient state management with RocksDB and optimized query models
- **Security**: Built-in support for data privacy and GDPR compliance
- **Evolution**: Schema evolution with backward compatibility checks
- **Developer Experience**: Intuitive annotation-based programming model
- **Observability**: Transparent view of all commands and events flowing through the system
- **Temporal Queries**: Ability to reconstruct state at any point in time

## License

Apache License 2.0

================
File: RELEASE.md
================
## Release process

This project uses the Maven Release Plugin and GitHub Actions to create releases.\
Just run `mvn release:prepare release:perform && git push` in the root to select the version to be released and create a
VCS tag.

GitHub Actions will
start [the build process](https://github.com/elasticsoftwarefoundation/akces-framework/actions/workflows/maven-publish.yml).

If successful, the build will be automatically published
to [Github Packages](https://maven.pkg.github.com/elasticsoftwarefoundation/akces-framework/).



================================================================
End of Codebase
================================================================
