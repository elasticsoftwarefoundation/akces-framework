This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix. The content has been processed where comments have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.java, **/*.xml, **/*.properties, **/*.proto, **/*.imports, **/*.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types

Additional Info:
----------------

================================================================
Directory Structure
================================================================
api/
  src/
    main/
      java/
        org/
          elasticsoftware/
            akces/
              aggregate/
                Aggregate.java
                AggregateState.java
                AggregateStateType.java
                CommandHandlerFunction.java
                CommandType.java
                DomainEventType.java
                EventHandlerFunction.java
                EventSourcingHandlerFunction.java
                SchemaType.java
              annotations/
                AggregateIdentifier.java
                AggregateInfo.java
                AggregateStateInfo.java
                CommandHandler.java
                CommandInfo.java
                DomainEventInfo.java
                EventHandler.java
                EventSourcingHandler.java
                PIIData.java
                QueryModelEventHandler.java
                QueryModelInfo.java
                QueryModelStateInfo.java
              commands/
                Command.java
                CommandBus.java
                CommandBusHolder.java
              events/
                DomainEvent.java
                ErrorEvent.java
              processmanager/
                AkcesProcess.java
                ProcessManager.java
                ProcessManagerState.java
                UnknownAkcesProcessException.java
              query/
                QueryModel.java
                QueryModelEventHandlerFunction.java
                QueryModelState.java
                QueryModelStateType.java
              AkcesException.java
  pom.xml
client/
  src/
    main/
      java/
        org/
          elasticsoftware/
            akces/
              client/
                AkcesClient.java
                AkcesClientAutoConfiguration.java
                AkcesClientCommandException.java
                AkcesClientController.java
                AkcesClientControllerState.java
                CommandRefusedException.java
                CommandSendingFailedException.java
                CommandSerializationException.java
                CommandServiceApplication.java
                CommandValidationException.java
                MissingDomainEventException.java
                UnknownSchemaException.java
                UnroutableCommandException.java
      resources/
        META-INF/
          spring/
            org.springframework.boot.autoconfigure.AutoConfiguration.imports
        akces-client.properties
    test/
      java/
        org/
          elasticsoftware/
            akces/
              client/
                commands/
                  CreateAccountCommand.java
                  InvalidCommand.java
                  UnroutableCommand.java
                events/
                  AccountCreatedEvent.java
                AkcesClientTestConfiguration.java
                AkcesClientTests.java
      resources/
        akces-client.properties
        logback-test.xml
  pom.xml
query-support/
  src/
    main/
      java/
        org/
          elasticsoftware/
            akces/
              query/
                models/
                  beans/
                    QueryModelBeanFactoryPostProcessor.java
                    QueryModelEventHandlerFunctionAdapter.java
                  AkcesQueryModelAutoConfiguration.java
                  AkcesQueryModelController.java
                  AkcesQueryModelControllerState.java
                  KafkaQueryModelRuntime.java
                  QueryModelExecutionCancelledException.java
                  QueryModelExecutionDisabledException.java
                  QueryModelExecutionException.java
                  QueryModelIdNotFoundException.java
                  QueryModelImplementationPresentCondition.java
                  QueryModelNotFoundException.java
                  QueryModelRuntime.java
                  QueryModelRuntimeFactory.java
                  QueryModels.java
                QueryServiceApplication.java
      resources/
        META-INF/
          spring/
            org.springframework.boot.autoconfigure.AutoConfiguration.imports
        akces-querymodel.properties
    test/
      java/
        org/
          elasticsoftware/
            akces/
              query/
                models/
                  account/
                    AccountQueryModel.java
                    AccountQueryModelState.java
                  wallet/
                    WalletQueryModel.java
                    WalletQueryModelState.java
                  QueryModelRuntimeTests.java
                  QueryModelTestConfiguration.java
      resources/
        logback-test.xml
  pom.xml
runtime/
  src/
    main/
      java/
        org/
          elasticsoftware/
            akces/
              aggregate/
                AggregateRuntime.java
                AggregateRuntimeBase.java
                IndexParams.java
              beans/
                AggregateBeanFactoryPostProcessor.java
                CommandHandlerFunctionAdapter.java
                DomainEventTypeValueCodeGeneratorDelegate.java
                EventHandlerFunctionAdapter.java
                EventSourcingHandlerFunctionAdapter.java
              control/
                AkcesRegistry.java
              kafka/
                AggregatePartition.java
                AggregatePartitionCommandBus.java
                AggregatePartitionState.java
                AggregateRuntimeFactory.java
                KafkaAggregateRuntime.java
                PartitionUtils.java
              state/
                AggregateStateRepository.java
                AggregateStateRepositoryException.java
                AggregateStateRepositoryFactory.java
                InMemoryAggregateStateRepository.java
                InMemoryAggregateStateRepositoryFactory.java
                RocksDBAggregateStateRepository.java
                RocksDBAggregateStateRepositoryFactory.java
              AggregateServiceApplication.java
              AkcesAggregateController.java
              AkcesControllerState.java
      resources/
        protobuf/
          AggregateStateRecord.proto
          CommandRecord.proto
          DomainEventRecord.proto
        akces-aggregateservice.properties
    test/
      java/
        org/
          elasticsoftware/
            akces/
              beans/
                AotServicesTest.java
                MinInsyncReplicasTest.java
            akcestest/
              aggregate/
                account/
                  Account.java
                  AccountCreatedEvent.java
                  AccountState.java
                  CreateAccountCommand.java
                orders/
                  BuyOrderCreatedEvent.java
                  BuyOrderPlacedEvent.java
                  BuyOrderProcess.java
                  BuyOrderRejectedEvent.java
                  FxMarket.java
                  OrderProcess.java
                  OrderProcessManager.java
                  OrderProcessManagerState.java
                  PlaceBuyOrderCommand.java
                  UserOrderProcessesCreatedEvent.java
                wallet/
                  AmountReservedEvent.java
                  BalanceAlreadyExistsErrorEvent.java
                  BalanceCreatedEvent.java
                  CreateBalanceCommand.java
                  CreateWalletCommand.java
                  CreditWalletCommand.java
                  ExternalAccountCreatedEvent.java
                  InsufficientFundsErrorEvent.java
                  InvalidAccountCreatedEvent.java
                  InvalidAmountErrorEvent.java
                  InvalidCurrencyErrorEvent.java
                  ReserveAmountCommand.java
                  Wallet.java
                  WalletCreatedEvent.java
                  WalletCreditedEvent.java
                  WalletState.java
              control/
                AkcesAggregateControllerTests.java
              old/
                BalanceCreatedEvent.java
                BuyOrderPlacedEvent.java
                CreateWalletCommand.java
                WalletCreditedEvent.java
              protocol/
                ProtocolTests.java
              schemas/
                AccountCreatedEvent.java
                AccountCreatedEventV2.java
                AccountCreatedEventV3.java
                AccountTypeV1.java
                AccountTypeV2.java
                CreditWalletCommand.java
                JsonSchemaTests.java
                NotCompatibleAccountCreatedEventV4.java
              state/
                RocksDBAggregateStateRepositoryTests.java
              util/
                HostUtilsTests.java
              AggregateServiceApplicationTests.java
              RuntimeConfiguration.java
              RuntimeTests.java
              TestUtils.java
              WalletConfiguration.java
              WalletTests.java
      resources/
        akces-client.properties
        logback-test.xml
        test-application.yaml
  pom.xml
shared/
  src/
    main/
      java/
        org/
          elasticsoftware/
            akces/
              control/
                AggregateServiceCommandType.java
                AggregateServiceDomainEventType.java
                AggregateServiceRecord.java
                AkcesControlRecord.java
              errors/
                AggregateAlreadyExistsErrorEvent.java
                CommandExecutionErrorEvent.java
              gdpr/
                jackson/
                  AkcesGDPRModule.java
                  PIIDataDeserializerModifier.java
                  PIIDataJsonDeserializer.java
                  PIIDataJsonSerializer.java
                  PIIDataSerializerModifier.java
                EncryptingGDPRContext.java
                GDPRAnnotationUtils.java
                GDPRContext.java
                GDPRContextHolder.java
                GDPRContextRepository.java
                GDPRContextRepositoryException.java
                GDPRContextRepositoryFactory.java
                GDPRKeyUtils.java
                InMemoryGDPRContextRepository.java
                InMemoryGDPRContextRepositoryFactory.java
                NoopGDPRContext.java
                RocksDBGDPRContextRepository.java
                RocksDBGDPRContextRepositoryFactory.java
              kafka/
                CustomKafkaConsumerFactory.java
                CustomKafkaProducerFactory.java
                RecordAndMetadata.java
              protocol/
                AggregateStateRecord.java
                CommandRecord.java
                CommandResponseRecord.java
                DomainEventRecord.java
                GDPRKeyRecord.java
                PayloadEncoding.java
                ProtocolRecord.java
              schemas/
                IncompatibleSchemaException.java
                InvalidSchemaVersionException.java
                KafkaSchemaRegistry.java
                PreviousSchemaVersionMissingException.java
                SchemaException.java
                SchemaNotBackwardsCompatibleException.java
                SchemaNotFoundException.java
                SchemaVersionNotFoundException.java
              serialization/
                AkcesControlRecordSerde.java
                BigDecimalSerializer.java
                ProtocolRecordSerde.java
              util/
                EnvironmentPropertiesPrinter.java
                HostUtils.java
                KafkaSender.java
                KafkaUtils.java
    test/
      java/
        org/
          elasticsoftware/
            akces/
              gdpr/
                jackson/
                  AkcesGDPRModuleTests.java
                GDPRContextTests.java
                RocksDBGDPRContextRepositoryTests.java
  pom.xml
pom.xml

================================================================
Files
================================================================

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/Aggregate.java
================
package org.elasticsoftware.akces.aggregate;

import org.elasticsoftware.akces.commands.CommandBus;
import org.elasticsoftware.akces.commands.CommandBusHolder;

public interface Aggregate<S extends AggregateState> {
    default String getName() {
        return getClass().getSimpleName();
    }

    Class<S> getStateClass();

    default CommandBus getCommandBus() {
        return CommandBusHolder.getCommandBus(getClass());
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/AggregateState.java
================
package org.elasticsoftware.akces.aggregate;

import com.fasterxml.jackson.annotation.JsonIgnore;
import jakarta.validation.constraints.NotNull;

public interface AggregateState {
    @JsonIgnore
    @NotNull
    String getAggregateId();

    @JsonIgnore
    @NotNull
    default String getIndexKey() {
        return getAggregateId();
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/AggregateStateType.java
================
package org.elasticsoftware.akces.aggregate;

public record AggregateStateType<C extends AggregateState>(String typeName, int version, Class<C> typeClass,
                                                           boolean generateGDPRKeyOnCreate, boolean indexed,
                                                           String indexName) {
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/CommandHandlerFunction.java
================
package org.elasticsoftware.akces.aggregate;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.commands.CommandBus;
import org.elasticsoftware.akces.commands.CommandBusHolder;
import org.elasticsoftware.akces.events.DomainEvent;

import java.util.List;
import java.util.stream.Stream;

@FunctionalInterface
public interface CommandHandlerFunction<S extends AggregateState, C extends Command, E extends DomainEvent> {
    @NotNull
    Stream<E> apply(@NotNull C command, S state);

    default boolean isCreate() {
        throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override isCreate()");
    }

    default CommandType<C> getCommandType() {
        throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getCommandType()");
    }

    default Aggregate<S> getAggregate() {
        throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getAggregate()");
    }

    default List<DomainEventType<E>> getProducedDomainEventTypes() {
        throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getProducedDomainEventTypes()");
    }

    default List<DomainEventType<E>> getErrorEventTypes() {
        throw new UnsupportedOperationException("When implementing CommandHandlerFunction directly, you must override getErrorEventTypes()");
    }

    default CommandBus getCommandBus() {
        return CommandBusHolder.getCommandBus(getAggregate().getClass());
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/CommandType.java
================
package org.elasticsoftware.akces.aggregate;

import com.fasterxml.jackson.annotation.JsonIgnore;
import org.elasticsoftware.akces.commands.Command;

public record CommandType<C extends Command>(
        String typeName,
        int version,
        @JsonIgnore Class<C> typeClass,
        boolean create,
        boolean external
) implements SchemaType {
    @Override
    public String getSchemaPrefix() {
        return "commands.";
    }

    @Override
    public boolean relaxExternalValidation() {
        return false;
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/DomainEventType.java
================
package org.elasticsoftware.akces.aggregate;

import com.fasterxml.jackson.annotation.JsonIgnore;
import org.elasticsoftware.akces.events.DomainEvent;

public record DomainEventType<T extends DomainEvent>(
        String typeName,
        int version,
        @JsonIgnore Class<T> typeClass,
        boolean create,
        boolean external,
        boolean error
) implements SchemaType {
    @Override
    public String getSchemaPrefix() {
        return "domainevents.";
    }

    @Override
    public boolean relaxExternalValidation() {
        return true;
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/EventHandlerFunction.java
================
package org.elasticsoftware.akces.aggregate;

import jakarta.validation.constraints.NotEmpty;
import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.commands.CommandBus;
import org.elasticsoftware.akces.commands.CommandBusHolder;
import org.elasticsoftware.akces.events.DomainEvent;

import java.util.List;
import java.util.stream.Stream;

@FunctionalInterface
public interface EventHandlerFunction<S extends AggregateState, InputEvent extends DomainEvent, E extends DomainEvent> {
    @NotEmpty
    Stream<E> apply(@NotNull InputEvent event, S state);

    default DomainEventType<InputEvent> getEventType() {
        throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getEventType()");
    }

    default Aggregate<S> getAggregate() {
        throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getAggregate()");
    }

    default boolean isCreate() {
        throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override isCreate()");
    }

    default List<DomainEventType<E>> getProducedDomainEventTypes() {
        throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getProducedDomainEventTypes()");
    }

    default List<DomainEventType<E>> getErrorEventTypes() {
        throw new UnsupportedOperationException("When implementing EventHandlerFunction directly, you must override getErrorEventTypes()");
    }

    default CommandBus getCommandBus() {
        return CommandBusHolder.getCommandBus(getAggregate().getClass());
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/EventSourcingHandlerFunction.java
================
package org.elasticsoftware.akces.aggregate;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.events.DomainEvent;

@FunctionalInterface
public interface EventSourcingHandlerFunction<S extends AggregateState, E extends DomainEvent> {
    @NotNull
    S apply(@NotNull E event, S state);

    default DomainEventType<E> getEventType() {
        throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override getEventType()");
    }

    default Aggregate<S> getAggregate() {
        throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override getAggregate()");
    }

    default boolean isCreate() {
        throw new UnsupportedOperationException("When implementing EventSourcingHandlerFunction directly, you must override isCreate()");
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/aggregate/SchemaType.java
================
package org.elasticsoftware.akces.aggregate;

public interface SchemaType {
    String getSchemaPrefix();

    default String getSchemaName() {
        return getSchemaPrefix() + typeName();
    }

    boolean relaxExternalValidation();

    String typeName();

    int version();

    Class<?> typeClass();

    boolean external();
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/AggregateIdentifier.java
================
package org.elasticsoftware.akces.annotations;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ElementType.FIELD, ElementType.METHOD, ElementType.ANNOTATION_TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface AggregateIdentifier {
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/AggregateInfo.java
================
package org.elasticsoftware.akces.annotations;

import org.springframework.core.annotation.AliasFor;
import org.springframework.stereotype.Component;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target({ElementType.TYPE})
@Component
public @interface AggregateInfo {
    @AliasFor(annotation = Component.class)
    String value();

    int version() default 1;

    boolean generateGDPRKeyOnCreate() default false;

    boolean indexed() default false;

    String indexName() default "UNDEFINED";
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/AggregateStateInfo.java
================
package org.elasticsoftware.akces.annotations;

public @interface AggregateStateInfo {
    String type();

    int version() default 1;
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/CommandHandler.java
================
package org.elasticsoftware.akces.annotations;

import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.events.ErrorEvent;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
public @interface CommandHandler {
    boolean create() default false;

    Class<? extends DomainEvent>[] produces();

    Class<? extends ErrorEvent>[] errors();
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/CommandInfo.java
================
package org.elasticsoftware.akces.annotations;

import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;

@Retention(RetentionPolicy.RUNTIME)
public @interface CommandInfo {
    String type();

    int version() default 1;
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/DomainEventInfo.java
================
package org.elasticsoftware.akces.annotations;

import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;

@Retention(RetentionPolicy.RUNTIME)
public @interface DomainEventInfo {
    String type();

    int version() default 1;
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/EventHandler.java
================
package org.elasticsoftware.akces.annotations;

import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.events.ErrorEvent;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
public @interface EventHandler {
    boolean create() default false;

    Class<? extends DomainEvent>[] produces();

    Class<? extends ErrorEvent>[] errors();
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/EventSourcingHandler.java
================
package org.elasticsoftware.akces.annotations;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
public @interface EventSourcingHandler {
    boolean create() default false;
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/PIIData.java
================
package org.elasticsoftware.akces.annotations;

import com.fasterxml.jackson.annotation.JacksonAnnotation;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@JacksonAnnotation
@Retention(RetentionPolicy.RUNTIME)
@Target({ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER})
public @interface PIIData {
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelEventHandler.java
================
package org.elasticsoftware.akces.annotations;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
public @interface QueryModelEventHandler {
    boolean create() default false;
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelInfo.java
================
package org.elasticsoftware.akces.annotations;

import org.springframework.core.annotation.AliasFor;
import org.springframework.stereotype.Component;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Retention(RetentionPolicy.RUNTIME)
@Target({ElementType.TYPE})
@Component
public @interface QueryModelInfo {
    @AliasFor(annotation = Component.class)
    String value();

    int version() default 1;

    String indexName();
}

================
File: api/src/main/java/org/elasticsoftware/akces/annotations/QueryModelStateInfo.java
================
package org.elasticsoftware.akces.annotations;

public @interface QueryModelStateInfo {
    String type();

    int version() default 1;
}

================
File: api/src/main/java/org/elasticsoftware/akces/commands/Command.java
================
package org.elasticsoftware.akces.commands;

import com.fasterxml.jackson.annotation.JsonIgnore;
import jakarta.validation.constraints.NotNull;

public interface Command {
    @JsonIgnore
    @NotNull
    String getAggregateId();
}

================
File: api/src/main/java/org/elasticsoftware/akces/commands/CommandBus.java
================
package org.elasticsoftware.akces.commands;

public interface CommandBus {
    void send(Command command);
}

================
File: api/src/main/java/org/elasticsoftware/akces/commands/CommandBusHolder.java
================
package org.elasticsoftware.akces.commands;

import org.elasticsoftware.akces.aggregate.Aggregate;

public class CommandBusHolder {
    protected static final ThreadLocal<CommandBus> commandBusThreadLocal = new ThreadLocal<>();

    protected CommandBusHolder() {
    }

    public static CommandBus getCommandBus(Class<? extends Aggregate> aggregateClass) {
        return commandBusThreadLocal.get();
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/events/DomainEvent.java
================
package org.elasticsoftware.akces.events;

import com.fasterxml.jackson.annotation.JsonIgnore;
import jakarta.validation.constraints.NotNull;

public interface DomainEvent {
    @JsonIgnore
    @NotNull
    String getAggregateId();
}

================
File: api/src/main/java/org/elasticsoftware/akces/events/ErrorEvent.java
================
package org.elasticsoftware.akces.events;

public interface ErrorEvent extends DomainEvent {
}

================
File: api/src/main/java/org/elasticsoftware/akces/processmanager/AkcesProcess.java
================
package org.elasticsoftware.akces.processmanager;

import com.fasterxml.jackson.annotation.JsonIgnore;
import jakarta.validation.constraints.NotNull;

public interface AkcesProcess {
    @JsonIgnore
    @NotNull
    String getProcessId();
}

================
File: api/src/main/java/org/elasticsoftware/akces/processmanager/ProcessManager.java
================
package org.elasticsoftware.akces.processmanager;

import org.elasticsoftware.akces.aggregate.Aggregate;
import org.elasticsoftware.akces.aggregate.AggregateState;

public interface ProcessManager<S extends AggregateState, P extends AkcesProcess> extends Aggregate<S> {
}

================
File: api/src/main/java/org/elasticsoftware/akces/processmanager/ProcessManagerState.java
================
package org.elasticsoftware.akces.processmanager;

import org.elasticsoftware.akces.aggregate.AggregateState;

public interface ProcessManagerState<P extends AkcesProcess> extends AggregateState {
    P getAkcesProcess(String processId) throws UnknownAkcesProcessException;

    boolean hasAkcesProcess(String processId);
}

================
File: api/src/main/java/org/elasticsoftware/akces/processmanager/UnknownAkcesProcessException.java
================
package org.elasticsoftware.akces.processmanager;

import org.elasticsoftware.akces.AkcesException;

public class UnknownAkcesProcessException extends AkcesException {
    private final String processId;

    public UnknownAkcesProcessException(String aggregateName, String aggregateId, String processId) {
        super(aggregateName, aggregateId);
        this.processId = processId;
    }

    public String getProcessId() {
        return processId;
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/query/QueryModel.java
================
package org.elasticsoftware.akces.query;

public interface QueryModel<S extends QueryModelState> {
    default String getName() {
        return getClass().getSimpleName();
    }

    Class<S> getStateClass();

    String getIndexName();
}

================
File: api/src/main/java/org/elasticsoftware/akces/query/QueryModelEventHandlerFunction.java
================
package org.elasticsoftware.akces.query;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.events.DomainEvent;

@FunctionalInterface
public interface QueryModelEventHandlerFunction<S extends QueryModelState, E extends DomainEvent> {
    @NotNull
    S apply(@NotNull E event, S state);

    default DomainEventType<E> getEventType() {
        throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getEventType()");
    }

    default QueryModel<S> getQueryModel() {
        throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override getQueryModel()");
    }

    default boolean isCreate() {
        throw new UnsupportedOperationException("When implementing QueryModelEventHandlerFunction directly, you must override isCreate()");
    }
}

================
File: api/src/main/java/org/elasticsoftware/akces/query/QueryModelState.java
================
package org.elasticsoftware.akces.query;

import com.fasterxml.jackson.annotation.JsonIgnore;
import jakarta.validation.constraints.NotNull;

public interface QueryModelState {
    @JsonIgnore
    @NotNull
    String getIndexKey();
}

================
File: api/src/main/java/org/elasticsoftware/akces/query/QueryModelStateType.java
================
package org.elasticsoftware.akces.query;

public record QueryModelStateType<C extends QueryModelState>(String typeName, int version, Class<C> typeClass,
                                                             String indexName) {
}

================
File: api/src/main/java/org/elasticsoftware/akces/AkcesException.java
================
package org.elasticsoftware.akces;

public abstract class AkcesException extends RuntimeException {
    private final String aggregateName;
    private final String aggregateId;

    public AkcesException(String aggregateName, String aggregateId) {
        this.aggregateName = aggregateName;
        this.aggregateId = aggregateId;
    }

    public String getAggregateName() {
        return aggregateName;
    }

    public String getAggregateId() {
        return aggregateId;
    }
}

================
File: api/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.7.21-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-api</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: API</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testng</groupId>
            <artifactId>testng</artifactId>
            <scope>test</scope>
        </dependency>







    </dependencies>

    <build>
        <plugins>
        </plugins>
    </build>
</project>

================
File: client/src/main/java/org/elasticsoftware/akces/client/AkcesClient.java
================
package org.elasticsoftware.akces.client;

import jakarta.annotation.Nonnull;
import jakarta.annotation.Nullable;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.events.DomainEvent;

import java.util.List;
import java.util.concurrent.CompletionStage;

public interface AkcesClient {



















    default CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nonnull Command command) {
        return send(tenantId, null, command);
    }

    CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command);

    default void sendAndForget(@Nonnull String tenantId, @Nonnull Command command) {
        sendAndForget(tenantId, null, command);
    }

    void sendAndForget(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command);
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/AkcesClientAutoConfiguration.java
================
package org.elasticsoftware.akces.client;

import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.kafka.CustomKafkaConsumerFactory;
import org.elasticsoftware.akces.kafka.CustomKafkaProducerFactory;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;
import org.elasticsoftware.akces.util.EnvironmentPropertiesPrinter;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;
import org.springframework.boot.autoconfigure.jackson.Jackson2ObjectMapperBuilderCustomizer;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.ClassPathScanningCandidateComponentProvider;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.PropertySource;
import org.springframework.core.env.Environment;
import org.springframework.core.type.filter.AnnotationTypeFilter;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdmin;
import org.springframework.kafka.core.KafkaAdminOperations;
import org.springframework.kafka.core.ProducerFactory;

import java.math.BigDecimal;
import java.util.List;
import java.util.Map;

@Configuration
@PropertySource("classpath:akces-client.properties")
public class AkcesClientAutoConfiguration {
    private final ProtocolRecordSerde serde = new ProtocolRecordSerde();

    public AkcesClientAutoConfiguration() {
    }

    @Bean(name = "akcesClientControlSerde")
    public AkcesControlRecordSerde controlSerde(ObjectMapper objectMapper) {
        return new AkcesControlRecordSerde(objectMapper);
    }

    @Bean(name = "akcesClientJsonCustomizer")
    public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
        return builder -> {
            builder.modulesToInstall(new AkcesGDPRModule());
            builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        };
    }

    @Bean(name = "akcesClientKafkaAdmin")
    public KafkaAdmin createKafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
        return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
    }

    @Bean(name = "akcesClientSchemaRegistryClient")
    public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
        return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
    }

    @Bean(name = "akcesClientSchemaRegistry")
    public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesClientSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
        return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
    }

    @Bean(name = "akcesClientProducerFactory")
    public ProducerFactory<String, ProtocolRecord> producerFactory(KafkaProperties properties) {
        return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), serde.serializer());
    }

    @Bean(name = "akcesClientControlConsumerFactory")
    public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
                                                                              @Qualifier("akcesClientControlSerde") AkcesControlRecordSerde controlSerde) {
        return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
    }

    @Bean(name = "akcesClientCommandResponseConsumerFactory")
    public ConsumerFactory<String, ProtocolRecord> commandResponseConsumerFactory(KafkaProperties properties) {
        return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
    }

    @Bean(name = "domainEventScanner")
    public ClassPathScanningCandidateComponentProvider domainEventScanner(Environment environment) {
        ClassPathScanningCandidateComponentProvider provider = new ClassPathScanningCandidateComponentProvider(false);
        provider.addIncludeFilter(new AnnotationTypeFilter(DomainEventInfo.class));
        provider.setEnvironment(environment);
        return provider;
    }

    @ConditionalOnMissingBean(EnvironmentPropertiesPrinter.class)
    @Bean(name = "environmentPropertiesPrinter")
    public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
        return new EnvironmentPropertiesPrinter();
    }

    @Bean(name = "akcesClient", initMethod = "start", destroyMethod = "close")
    public AkcesClientController akcesClient(@Qualifier("akcesClientProducerFactory") ProducerFactory<String, ProtocolRecord> producerFactory,
                                             @Qualifier("akcesClientControlConsumerFactory") ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory,
                                             @Qualifier("akcesClientCommandResponseConsumerFactory") ConsumerFactory<String, ProtocolRecord> commandResponseConsumerFactory,
                                             @Qualifier("akcesClientKafkaAdmin") KafkaAdminOperations kafkaAdmin,
                                             @Qualifier("akcesClientSchemaRegistry") KafkaSchemaRegistry schemaRegistry,
                                             ObjectMapper objectMapper,
                                             @Qualifier("domainEventScanner") ClassPathScanningCandidateComponentProvider domainEventScanner,
                                             @Value("${akces.client.domainEventsPackage}") String domainEventsPackage) {
        return new AkcesClientController(producerFactory,
                controlConsumerFactory,
                commandResponseConsumerFactory,
                kafkaAdmin,
                schemaRegistry,
                objectMapper,
                domainEventScanner,
                domainEventsPackage);
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/AkcesClientCommandException.java
================
package org.elasticsoftware.akces.client;

import jakarta.annotation.Nonnull;
import jakarta.annotation.Nullable;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public abstract class AkcesClientCommandException extends RuntimeException {
    private final Class<? extends Command> commandClass;
    private final String commandType;
    private final Integer commandVersion;

    public AkcesClientCommandException(@Nonnull Class<? extends Command> commandClass,
                                       @Nullable CommandInfo commandInfo,
                                       @Nonnull String causeMessage,
                                       @Nonnull Throwable cause) {
        super("Problem " + causeMessage + " Command", cause);
        this.commandClass = commandClass;
        this.commandType = commandInfo != null ? commandInfo.type() : null;
        this.commandVersion = commandInfo != null ? commandInfo.version() : null;
    }

    public AkcesClientCommandException(@Nonnull Class<? extends Command> commandClass,
                                       @Nullable CommandInfo commandInfo,
                                       @Nonnull String message) {
        super(message);
        this.commandClass = commandClass;
        this.commandType = commandInfo != null ? commandInfo.type() : null;
        this.commandVersion = commandInfo != null ? commandInfo.version() : null;
    }

    public Class<? extends Command> getCommandClass() {
        return commandClass;
    }

    public String getCommandType() {
        return commandType;
    }

    public Integer getCommandVersion() {
        return commandVersion;
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/AkcesClientController.java
================
package org.elasticsoftware.akces.client;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;
import io.confluent.kafka.schemaregistry.ParsedSchema;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import jakarta.annotation.Nonnull;
import jakarta.annotation.Nullable;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.InterruptException;
import org.apache.kafka.common.errors.WakeupException;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.control.AggregateServiceCommandType;
import org.elasticsoftware.akces.control.AggregateServiceDomainEventType;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.events.ErrorEvent;
import org.elasticsoftware.akces.gdpr.EncryptingGDPRContext;
import org.elasticsoftware.akces.gdpr.GDPRKeyUtils;
import org.elasticsoftware.akces.protocol.*;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.elasticsoftware.akces.schemas.SchemaException;
import org.elasticsoftware.akces.util.HostUtils;
import org.elasticsoftware.akces.util.KafkaSender;
import org.everit.json.schema.ValidationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.BeansException;
import org.springframework.beans.factory.config.BeanDefinition;
import org.springframework.boot.availability.AvailabilityChangeEvent;
import org.springframework.boot.availability.LivenessState;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;
import org.springframework.context.annotation.ClassPathScanningCandidateComponentProvider;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdminOperations;
import org.springframework.kafka.core.ProducerFactory;

import java.io.IOException;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.*;

import static java.nio.charset.StandardCharsets.UTF_8;
import static java.util.Collections.singletonList;
import static org.elasticsoftware.akces.client.AkcesClientControllerState.*;
import static org.elasticsoftware.akces.gdpr.GDPRContextHolder.resetCurrentGDPRContext;
import static org.elasticsoftware.akces.gdpr.GDPRContextHolder.setCurrentGDPRContext;

public class AkcesClientController extends Thread implements AutoCloseable, AkcesClient, ApplicationContextAware {
    private static final Logger logger = LoggerFactory.getLogger(AkcesClientController.class);
    private static final TopicPartition AKCES_CONTROL_PARTITION = new TopicPartition("Akces-Control", 0);
    private final ProducerFactory<String, ProtocolRecord> producerFactory;
    private final ConsumerFactory<String, AkcesControlRecord> controlRecordConsumerFactory;
    private final ConsumerFactory<String, ProtocolRecord> commandResponseConsumerFactory;
    private final KafkaAdminOperations kafkaAdmin;
    private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
    private final Map<String, AggregateServiceRecord> aggregateServices = new ConcurrentHashMap<>();
    private final BlockingQueue<CommandRequest> commandQueue = new LinkedBlockingQueue<>();
    private final Map<String, PendingCommandResponse> pendingCommandResponseMap = new HashMap<>();
    private final KafkaSchemaRegistry schemaRegistry;
    private final ObjectMapper objectMapper;
    private final Map<Class<? extends Command>, AggregateServiceCommandType> commandTypes = new ConcurrentHashMap<>();
    private final Map<String, TreeMap<Integer, DomainEventType<? extends DomainEvent>>> domainEventClasses = new ConcurrentHashMap<>();
    private final Map<String, Map<Integer, ParsedSchema>> commandSchemas = new ConcurrentHashMap<>();
    private final Map<String, Map<Integer, ParsedSchema>> domainEventSchemas = new ConcurrentHashMap<>();
    private final Map<Class<? extends Command>, ParsedSchema> commandSchemasLookup = new ConcurrentHashMap<>();
    private final ClassPathScanningCandidateComponentProvider domainEventScanner;
    private final CountDownLatch shutdownLatch = new CountDownLatch(1);
    private Integer partitions = null;
    private volatile AkcesClientControllerState processState = INITIALIZING;
    private TopicPartition commandResponsePartition;
    private ApplicationContext applicationContext;

    public AkcesClientController(ProducerFactory<String, ProtocolRecord> producerFactory,
                                 ConsumerFactory<String, AkcesControlRecord> controlRecordConsumerFactory,
                                 ConsumerFactory<String, ProtocolRecord> commandResponseConsumerFactory,
                                 KafkaAdminOperations kafkaAdmin,
                                 KafkaSchemaRegistry schemaRegistry,
                                 ObjectMapper objectMapper,
                                 ClassPathScanningCandidateComponentProvider domainEventScanner,
                                 String basePackage) {
        super("AkcesClientController");
        this.producerFactory = producerFactory;
        this.controlRecordConsumerFactory = controlRecordConsumerFactory;
        this.commandResponseConsumerFactory = commandResponseConsumerFactory;
        this.kafkaAdmin = kafkaAdmin;
        this.schemaRegistry = schemaRegistry;
        this.objectMapper = objectMapper;
        this.domainEventScanner = domainEventScanner;
        loadSupportedDomainEvents(basePackage);

        loadSupportedDomainEvents("org.elasticsoftware.akces.errors");
    }

    @VisibleForTesting
    public TopicPartition getCommandResponsePartition() {
        return commandResponsePartition;
    }

    @Override
    public void run() {
        try (final Consumer<String, AkcesControlRecord> controlConsumer = controlRecordConsumerFactory.createConsumer(
                HostUtils.getHostName() + "-AkcesClientController-Control",
                HostUtils.getHostName() + "-AkcesClientController-Control",
                null);
             final Consumer<String, ProtocolRecord> commandResponseConsumer = commandResponseConsumerFactory.createConsumer(
                     HostUtils.getHostName() + "-AkcesClientController-CommandResponses",
                     HostUtils.getHostName() + "-AkcesClientController-CommandResponses",
                     null);
             final Producer<String, ProtocolRecord> producer = producerFactory.createProducer(HostUtils.getHostName() + "-AkcesClientController")) {

            partitions = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control").partitions().size();

            controlConsumer.assign(singletonList(AKCES_CONTROL_PARTITION));

            controlConsumer.seekToBeginning(singletonList(AKCES_CONTROL_PARTITION));

            int commandResponsePartitions = kafkaAdmin.describeTopics("Akces-CommandResponses").get("Akces-CommandResponses").partitions().size();
            commandResponsePartition = new TopicPartition("Akces-CommandResponses",
                    resolveCommandResponsePartition(HostUtils.getHostName(), commandResponsePartitions));
            commandResponseConsumer.assign(singletonList(commandResponsePartition));

            commandResponseConsumer.seekToEnd(singletonList(commandResponsePartition));
            while (processState != SHUTTING_DOWN) {
                process(controlConsumer, commandResponseConsumer, producer);
            }

            List<CommandRequest> pendingRequests = new ArrayList<>();
            commandQueue.drainTo(pendingRequests);
            for (CommandRequest pendingRequest : pendingRequests) {
                pendingRequest.completableFuture().completeExceptionally(new CommandRefusedException(pendingRequest.command().getClass(), SHUTTING_DOWN));
            }

            applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));

            shutdownLatch.countDown();
        }
    }

    @SuppressWarnings("unchecked")
    private void loadSupportedDomainEvents(String basePackage) {
        for (BeanDefinition beanDefinition : domainEventScanner.findCandidateComponents(basePackage)) {
            try {
                Class<? extends DomainEvent> domainEventClass = (Class<? extends DomainEvent>) Class.forName(beanDefinition.getBeanClassName());
                DomainEventInfo domainEventInfo = domainEventClass.getAnnotation(DomainEventInfo.class);
                TreeMap<Integer, DomainEventType<? extends DomainEvent>> versionMap = domainEventClasses.computeIfAbsent(domainEventInfo.type(), k -> new TreeMap<>());
                versionMap.put(domainEventInfo.version(), new DomainEventType<>(domainEventInfo.type(), domainEventInfo.version(), domainEventClass, false, true, ErrorEvent.class.isAssignableFrom(domainEventClass)));
            } catch (ClassNotFoundException e) {

            } catch (ClassCastException e) {

            }
        }
    }

    private void process(Consumer<String, AkcesControlRecord> controlConsumer,
                         Consumer<String, ProtocolRecord> commandResponseConsumer,
                         Producer<String, ProtocolRecord> producer) {
        if (processState == RUNNING) {
            try {

                processControlRecords(controlConsumer.poll(Duration.ofMillis(10)));

                processCommands(producer);

                processCommandResponses(commandResponseConsumer.poll(Duration.ofMillis(10)));
            } catch (WakeupException | InterruptException e) {

            } catch (KafkaException e) {

                logger.error("Unrecoverable exception in AkcesController", e);

                processState = SHUTTING_DOWN;
            } catch (IOException | RestClientException e) {

                logger.error("Exception while loading Command (JSON)Schemas from SchemaRegistry", e);
                processState = SHUTTING_DOWN;
            }
        } else if (processState == INITIALIZING) {
            try {
                Map<TopicPartition, Long> endOffsets = controlConsumer.endOffsets(singletonList(AKCES_CONTROL_PARTITION));
                ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(10));
                processControlRecords(consumerRecords);

                if (consumerRecords.isEmpty() && endOffsets.getOrDefault(AKCES_CONTROL_PARTITION, 0L) <= controlConsumer.position(AKCES_CONTROL_PARTITION)) {
                    processState = RUNNING;
                }
            } catch (WakeupException | InterruptException e) {

            } catch (KafkaException e) {

                logger.error("Unrecoverable exception in AkcesController", e);

                processState = SHUTTING_DOWN;
            } catch (IOException | RestClientException e) {

                logger.error("Exception while loading Command (JSON)Schemas from SchemaRegistry", e);
                processState = SHUTTING_DOWN;
            }
        }
    }

    private void processControlRecords(ConsumerRecords<String, AkcesControlRecord> consumerRecords) throws RestClientException, IOException {
        if (!consumerRecords.isEmpty()) {
            for (ConsumerRecord<String, AkcesControlRecord> record : consumerRecords) {
                AkcesControlRecord controlRecord = record.value();
                if (controlRecord instanceof AggregateServiceRecord aggregateServiceRecord) {
                    if (!aggregateServices.containsKey(record.key())) {
                        logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
                    }

                    aggregateServices.put(record.key(), aggregateServiceRecord);
                } else {
                    logger.warn("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
                }
            }
        }
    }

    private void processCommands(Producer<String, ProtocolRecord> producer) {
        List<CommandRequest> commandRequests = new ArrayList<>();
        if (commandQueue.drainTo(commandRequests) > 0) {
            Map<ProducerRecord<String,ProtocolRecord>, CommandRequest> commandRecords = new HashMap<>();

            for(CommandRequest commandRequest : commandRequests) {
                try {
                    registerCommand(
                            commandRequest.commandType(),
                            commandRequest.commandVersion(),
                            commandRequest.command().getClass());

                    String topic = resolveTopic(
                            commandRequest.commandType(),
                            commandRequest.commandVersion(),
                            commandRequest.command());

                    CommandRecord commandRecord = new CommandRecord(
                            commandRequest.tenantId(),
                            commandRequest.commandType(),
                            commandRequest.commandVersion(),
                            serialize(commandRequest.command()),
                            PayloadEncoding.JSON,
                            commandRequest.command().getAggregateId(),
                            commandRequest.correlationId() != null ? commandRequest.correlationId() : UUID.randomUUID().toString(), commandResponsePartition.toString());

                    ProducerRecord<String, ProtocolRecord> producerRecord =
                            new ProducerRecord<>(
                                    topic,
                                    resolvePartition(commandRecord.aggregateId()),
                                    commandRecord.aggregateId(),
                                    commandRecord);
                    commandRecords.put(producerRecord, commandRequest);

                    if(commandRequest.completeAfterValidation()) {
                        commandRequest.completableFuture().complete(Collections.emptyList());
                    }
                } catch(AkcesClientCommandException | SchemaException e){
                    commandRequest.completableFuture().completeExceptionally(e);
                }
            }

            producer.beginTransaction();
            for(Map.Entry<ProducerRecord<String,ProtocolRecord>, CommandRequest> entry : commandRecords.entrySet()) {
                final CompletableFuture<List<DomainEvent>> completableFuture = entry.getValue().completableFuture();
                final CommandRecord commandRecord = (CommandRecord) entry.getKey().value();
                final Class<? extends Command> commandClass = entry.getValue().command().getClass();
                if (!completableFuture.isDone()) {
                    KafkaSender.send(producer, entry.getKey(), (metadata, exception) -> {
                        if (exception != null) {
                            completableFuture.completeExceptionally(new CommandSendingFailedException(commandClass, exception));
                        } else {

                            pendingCommandResponseMap.put(commandRecord.id(), new PendingCommandResponse(commandRecord, completableFuture));
                        }
                    });
                } else {

                    KafkaSender.send(producer, entry.getKey());
                }
            }
            producer.commitTransaction();
        }
    }


    private void processCommandResponses(ConsumerRecords<String, ProtocolRecord> consumerRecords) {
        if (!consumerRecords.isEmpty()) {
            for (ConsumerRecord<String, ProtocolRecord> record : consumerRecords) {
                ProtocolRecord protocolRecord = record.value();
                if (protocolRecord instanceof CommandResponseRecord commandResponseRecord) {
                    PendingCommandResponse pendingCommandResponse = pendingCommandResponseMap.remove(commandResponseRecord.commandId());
                    if (pendingCommandResponse == null) {

                        logger.trace("Received CommandResponseRecord for unknown commandId: {}", commandResponseRecord.commandId());
                    } else {

                        try {
                            List<DomainEvent> domainEvents = new ArrayList<>();
                            for (DomainEventRecord domainEventRecord : commandResponseRecord.events()) {
                                domainEvents.add(deserialize(domainEventRecord, commandResponseRecord.encryptionKey()));
                            }

                            pendingCommandResponse.completableFuture().complete(domainEvents);
                        } catch (IOException e) {

                            pendingCommandResponse.completableFuture().completeExceptionally(e);
                        }
                    }
                } else {
                    logger.warn("Received unknown ProtocolRecord type: {}", protocolRecord.getClass().getSimpleName());
                }
            }
        }
    }

    @Override
    public CompletionStage<List<DomainEvent>> send(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command) {

        checkRunning(command);
        CommandInfo commandInfo = command.getClass().getAnnotation(CommandInfo.class);
        if(commandInfo == null) {

            throw new IllegalArgumentException("Command class " + command.getClass().getName() + " is not annotated with @CommandInfo");
        }

        CompletableFuture<List<DomainEvent>> completableFuture = new CompletableFuture<>();
        commandQueue.add(new CommandRequest(
                tenantId,
                correlationId,
                commandInfo.type(),
                commandInfo.version(),
                command,
                completableFuture,
                false));

        return completableFuture;
    }

    @Override
    public void sendAndForget(@Nonnull String tenantId, @Nullable String correlationId, @Nonnull Command command) {

        checkRunning(command);

        CommandInfo commandInfo = command.getClass().getAnnotation(CommandInfo.class);
        if(commandInfo == null) {

            throw new IllegalArgumentException("Command class " + command.getClass().getName() + " is not annotated with @CommandInfo");
        }

        CompletableFuture<List<DomainEvent>> completableFuture = new CompletableFuture<>();
        commandQueue.add(new CommandRequest(
                tenantId,
                correlationId,
                commandInfo.type(),
                commandInfo.version(),
                command,
                completableFuture,
                true));

        try {
            completableFuture.get();
        } catch (InterruptedException | CancellationException e) {

        } catch (ExecutionException e) {

            if(e.getCause() instanceof RuntimeException) {
                throw (RuntimeException) e.getCause();
            } else {
                throw new RuntimeException(e.getCause());
            }
        }
    }

    @Override
    public void close() throws Exception {
        this.processState = SHUTTING_DOWN;

        try {
            if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
                logger.info("AkcesClientController has been shutdown");
            } else {
                logger.warn("AkcesClientController did not shutdown within 10 seconds");
            }
        } catch (InterruptedException e) {

        }
    }

    private AggregateServiceCommandType resolveCommandType(String type, int version) {
        return aggregateServices.values().stream()
                .filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), type, version))
                .findFirst().flatMap(commandServiceRecord -> commandServiceRecord.supportedCommands().stream()
                        .filter(commandType -> commandType.typeName().equals(type) && commandType.version() == version)
                        .findFirst()).orElse(null);

    }

    private AggregateServiceRecord resolveAggregateService(AggregateServiceCommandType commandType) {
        return aggregateServices.values().stream()
                .filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType.typeName(), commandType.version()))
                .findFirst().orElse(null);

    }

    private String resolveTopic(String commandType, int version, Command command) {
        List<AggregateServiceRecord> services = aggregateServices.values().stream()
                .filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType, version))
                .toList();
        if (services.size() == 1) {
            return services.getFirst().commandTopic();
        } else if (services.isEmpty()) {
            throw new UnroutableCommandException(command.getClass());
        } else {

            throw new UnroutableCommandException(command.getClass());
        }
    }

    private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, String commandType, int version) {
        for (AggregateServiceCommandType supportedCommand : supportedCommands) {
            if (supportedCommand.typeName().equals(commandType) &&
                    supportedCommand.version() == version) {
                return true;
            }
        }
        return false;
    }

    @VisibleForTesting
    public Integer resolvePartition(@Nonnull String aggregateId) {
        return Math.abs(hashFunction.hashString(aggregateId, UTF_8).asInt()) % partitions;
    }

    private Integer resolveCommandResponsePartition(String hostname, int partitions) {
        return Math.abs(hashFunction.hashString(hostname, UTF_8).asInt()) % partitions;
    }

    private void registerCommand(String type, int version, Class<? extends Command> commandClass) {

        if (!commandTypes.containsKey(commandClass)) {


            AggregateServiceCommandType commandType = resolveCommandType(type, version);
            if (commandType != null) {

                ParsedSchema schema = schemaRegistry.validate(commandType.toLocalCommandType(commandClass));
                commandSchemas.computeIfAbsent(
                        commandType.typeName(),
                        k -> new ConcurrentHashMap<>()).put(commandType.version(), schema);
                logger.trace("Stored schema: {} v{}", commandType.schemaName(), commandType.version());

                commandSchemasLookup.put(commandClass, schema);

                commandTypes.put(commandClass, commandType);


                for (AggregateServiceDomainEventType domainEventType : resolveAggregateService(commandType).producedEvents()) {
                    processDomainEvent(commandClass, domainEventType);
                }
            } else {

                throw new UnroutableCommandException(commandClass);
            }
        }
    }

    private void processDomainEvent(Class<? extends Command> commandClass, AggregateServiceDomainEventType aggregateServiceDomainEventType) throws SchemaException {

        DomainEventType<? extends DomainEvent> domainEventType =
                domainEventClasses.get(aggregateServiceDomainEventType.typeName()).floorEntry(aggregateServiceDomainEventType.version()).getValue();
        if (domainEventType != null) {

            domainEventSchemas.computeIfAbsent(
                    domainEventType.typeName(),
                    k -> new ConcurrentHashMap<>()).put(
                    domainEventType.version(),
                    schemaRegistry.validate(domainEventType));
            logger.trace("Stored schema for: {} v{}", domainEventType.getSchemaName(), domainEventType.version());
        } else {
            throw new MissingDomainEventException(
                    commandClass,
                    aggregateServiceDomainEventType.typeName(),
                    aggregateServiceDomainEventType.version());
        }
    }

    private byte[] serialize(Command command) {
        try {

            ParsedSchema schema = commandSchemasLookup.get(command.getClass());
            if (schema instanceof JsonSchema jsonSchema) {
                JsonNode jsonNode = objectMapper.valueToTree(command);
                jsonSchema.validate(jsonNode);
                return objectMapper.writeValueAsBytes(jsonNode);
            } else {
                return objectMapper.writeValueAsBytes(command);
            }
        } catch (IOException e) {
            throw new CommandSerializationException(command.getClass(), e);
        } catch (ValidationException e) {
            throw new CommandValidationException(command.getClass(), e);
        }
    }

    private DomainEvent deserialize(DomainEventRecord der, @Nullable byte[] encryptionKey) throws IOException {
        try {
            setCurrentGDPRContext(encryptionKey != null ? new EncryptingGDPRContext(der.aggregateId(), encryptionKey, GDPRKeyUtils.isUUID(der.aggregateId())) : null);

            DomainEventType<? extends DomainEvent> domainEventType = domainEventClasses.get(der.name()).floorEntry(der.version()).getValue();

            return objectMapper.readValue(der.payload(), domainEventType.typeClass());
        } finally {
            resetCurrentGDPRContext();
        }

    }

    private void checkRunning(Command command) {
        if (processState != RUNNING) {
            throw new CommandRefusedException(command.getClass(), processState);
        }
    }

    public boolean isRunning() {
        return processState == RUNNING;
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        this.applicationContext = applicationContext;
    }

    private record CommandRequest(
            @Nonnull String tenantId,
            @Nullable String correlationId,
            @Nonnull String commandType,
            int commandVersion,
            @Nonnull Command command,
            @Nonnull CompletableFuture<List<DomainEvent>> completableFuture,
            boolean completeAfterValidation
    ) {

    }

    private record PendingCommandResponse(@Nonnull CommandRecord commandRecord,
                                          @Nonnull CompletableFuture<List<DomainEvent>> completableFuture) {
    }

}

================
File: client/src/main/java/org/elasticsoftware/akces/client/AkcesClientControllerState.java
================
package org.elasticsoftware.akces.client;

public enum AkcesClientControllerState {
    INITIALIZING,
    RUNNING,
    SHUTTING_DOWN
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/CommandRefusedException.java
================
package org.elasticsoftware.akces.client;

import jakarta.annotation.Nonnull;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class CommandRefusedException extends AkcesClientCommandException {
    private final AkcesClientControllerState state;

    public CommandRefusedException(@Nonnull Class<? extends Command> commandClass, AkcesClientControllerState state) {
        super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Command Refused because AkcesClient is not in RUNNING state");
        this.state = state;
    }

    public AkcesClientControllerState getState() {
        return state;
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/CommandSendingFailedException.java
================
package org.elasticsoftware.akces.client;

import jakarta.annotation.Nonnull;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class CommandSendingFailedException extends AkcesClientCommandException {
    public CommandSendingFailedException(@Nonnull Class<? extends Command> commandClass, @Nonnull Throwable cause) {
        super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Sending", cause);
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/CommandSerializationException.java
================
package org.elasticsoftware.akces.client;

import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class CommandSerializationException extends AkcesClientCommandException {

    public CommandSerializationException(Class<? extends Command> commandClass,
                                         Throwable cause) {
        super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Serializing", cause);
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/CommandServiceApplication.java
================
package org.elasticsoftware.akces.client;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.boot.context.properties.EnableConfigurationProperties;

import java.util.Set;

@SpringBootApplication(exclude = KafkaAutoConfiguration.class)
@EnableConfigurationProperties(KafkaProperties.class)
public class CommandServiceApplication {
    public static void main(String[] args) {
        SpringApplication application = new SpringApplication(CommandServiceApplication.class);
        if (args.length > 0) {

            application.setSources(Set.of(args));
        }
        application.run();
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/CommandValidationException.java
================
package org.elasticsoftware.akces.client;

import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class CommandValidationException extends AkcesClientCommandException {

    public CommandValidationException(Class<? extends Command> commandClass,
                                      Throwable cause) {
        super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Validating", cause);
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/MissingDomainEventException.java
================
package org.elasticsoftware.akces.client;

import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class MissingDomainEventException extends AkcesClientCommandException {
    private final String schemaName;
    private final int version;

    public MissingDomainEventException(Class<? extends Command> commandClass, String typeName, int version) {
        super(commandClass,
                commandClass.getAnnotation(CommandInfo.class),
                "Unable to send Command, missing local version of DomainEvent " + typeName + " v" + version);
        this.schemaName = typeName;
        this.version = version;
    }

    public String getSchemaName() {
        return schemaName;
    }

    public int getVersion() {
        return version;
    }

}

================
File: client/src/main/java/org/elasticsoftware/akces/client/UnknownSchemaException.java
================
package org.elasticsoftware.akces.client;

import jakarta.annotation.Nonnull;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class UnknownSchemaException extends AkcesClientCommandException {
    private final String schemaIdentifier;

    public UnknownSchemaException(@Nonnull Class<? extends Command> commandClass, String schemaIdentifier) {
        super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Unknown Schema " + schemaIdentifier);
        this.schemaIdentifier = schemaIdentifier;
    }

    public String getSchemaIdentifier() {
        return schemaIdentifier;
    }
}

================
File: client/src/main/java/org/elasticsoftware/akces/client/UnroutableCommandException.java
================
package org.elasticsoftware.akces.client;

import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

public class UnroutableCommandException extends AkcesClientCommandException {
    public UnroutableCommandException(Class<? extends Command> commandClass) {
        super(commandClass, commandClass.getAnnotation(CommandInfo.class), "Unable to Route Command, no AggregateService found that handles the Command");
    }
}

================
File: client/src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#

org.elasticsoftware.akces.client.AkcesClientAutoConfiguration

================
File: client/src/main/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: client/src/test/java/org/elasticsoftware/akces/client/commands/CreateAccountCommand.java
================
package org.elasticsoftware.akces.client.commands;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

@CommandInfo(type = "CreateAccount")
public record CreateAccountCommand(
        @AggregateIdentifier @NotNull String userId,
        @NotNull String country,
        @NotNull String firstName,
        @NotNull String lastName,
        @NotNull String email
) implements Command {
    @Override
    @NotNull
    public String getAggregateId() {
        return userId();
    }
}

================
File: client/src/test/java/org/elasticsoftware/akces/client/commands/InvalidCommand.java
================
package org.elasticsoftware.akces.client.commands;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.commands.Command;

public record InvalidCommand(String id) implements Command {
    @Override
    public @NotNull String getAggregateId() {
        return id();
    }
}

================
File: client/src/test/java/org/elasticsoftware/akces/client/commands/UnroutableCommand.java
================
package org.elasticsoftware.akces.client.commands;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

@CommandInfo(type = "Unrouteable")
public record UnroutableCommand(String id) implements Command {
    @Override
    public @NotNull String getAggregateId() {
        return id();
    }
}

================
File: client/src/test/java/org/elasticsoftware/akces/client/events/AccountCreatedEvent.java
================
package org.elasticsoftware.akces.client.events;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.annotations.PIIData;
import org.elasticsoftware.akces.events.DomainEvent;


@DomainEventInfo(type = "AccountCreated")
public record AccountCreatedEvent(
        @AggregateIdentifier @NotNull String userId,
        String country,
        @PIIData String firstName,
        @PIIData String lastName,
        @PIIData String email
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: client/src/test/java/org/elasticsoftware/akces/client/AkcesClientTestConfiguration.java
================
package org.elasticsoftware.akces.client;

import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.annotation.Configuration;

@Configuration
@EnableAutoConfiguration
@EnableConfigurationProperties(KafkaProperties.class)
public class AkcesClientTestConfiguration {

}

================
File: client/src/test/java/org/elasticsoftware/akces/client/AkcesClientTests.java
================
package org.elasticsoftware.akces.client;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.github.victools.jsonschema.generator.*;
import com.github.victools.jsonschema.module.jackson.JacksonModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationOption;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import jakarta.inject.Inject;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringSerializer;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.client.commands.CreateAccountCommand;
import org.elasticsoftware.akces.client.commands.InvalidCommand;
import org.elasticsoftware.akces.client.commands.UnroutableCommand;
import org.elasticsoftware.akces.client.events.AccountCreatedEvent;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.control.AggregateServiceCommandType;
import org.elasticsoftware.akces.control.AggregateServiceDomainEventType;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.gdpr.EncryptingGDPRContext;
import org.elasticsoftware.akces.gdpr.GDPRKeyUtils;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.protocol.*;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.MethodOrderer;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.TestMethodOrder;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.context.ApplicationContextException;
import org.springframework.context.ApplicationContextInitializer;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdmin;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.test.annotation.DirtiesContext;
import org.springframework.test.context.ContextConfiguration;
import org.springframework.test.context.support.TestPropertySourceUtils;
import org.testcontainers.containers.GenericContainer;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.Network;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import javax.crypto.spec.SecretKeySpec;
import java.io.IOException;
import java.math.BigDecimal;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.concurrent.CompletionStage;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;

@SpringBootTest(
        classes = CommandServiceApplication.class,
        properties = "spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration",
        useMainMethod = SpringBootTest.UseMainMethod.ALWAYS)
@ContextConfiguration(initializers = AkcesClientTests.ContextInitializer.class)
@Testcontainers
@DirtiesContext
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
public class AkcesClientTests {
    private static final String CONFLUENT_PLATFORM_VERSION = "7.8.1";

    private static final Network network = Network.newNetwork();

    @Container
    private static final KafkaContainer kafka =
            new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
                    .withKraft()
                    .withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
                    .withNetwork(network)
                    .withNetworkAliases("kafka");

    @Container
    private static final GenericContainer<?> schemaRegistry =
            new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
                    .withNetwork(network)
                    .withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
                    .withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
                    .withExposedPorts(8081)
                    .withNetworkAliases("schema-registry")
                    .dependsOn(kafka);

    @Inject
    KafkaAdmin adminClient;

    @Inject
    SchemaRegistryClient schemaRegistryClient;

    @Inject
    AkcesClientController akcesClient;

    @Inject
    ConsumerFactory<String, ProtocolRecord> consumerFactory;

    @Inject
    ProducerFactory<String, ProtocolRecord> producerFactory;

    @Inject
    ObjectMapper objectMapper;

    public static void prepareKafka(String bootstrapServers) {
        KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
        kafkaAdmin.createOrModifyTopics(
                createCompactedTopic("Akces-Control", 3),
                createTopic("Akces-CommandResponses", 3, 604800000L),
                createCompactedTopic("Akces-GDPRKeys", 3),
                createTopic("Wallet-Commands", 3),
                createTopic("Wallet-DomainEvents", 3),
                createTopic("Account-Commands", 3),
                createTopic("Account-DomainEvents", 3),
                createTopic("OrderProcessManager-Commands", 3),
                createTopic("OrderProcessManager-DomainEvents", 3),
                createCompactedTopic("Wallet-AggregateState", 3),
                createCompactedTopic("Account-AggregateState", 3),
                createCompactedTopic("OrderProcessManager-AggregateState", 3));
    }

    private static NewTopic createTopic(String name, int numPartitions) {
        return createTopic(name, numPartitions, -1L);
    }

    private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
        NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
        return topic.configs(Map.of(
                "cleanup.policy", "delete",
                "max.message.bytes", "20971520",
                "retention.ms", Long.toString(retentionMs),
                "segment.ms", "604800000"));
    }

    private static NewTopic createCompactedTopic(String name, int numPartitions) {
        NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
        return topic.configs(Map.of(
                "cleanup.policy", "compact",
                "max.message.bytes", "20971520",
                "retention.ms", "-1",
                "segment.ms", "604800000",
                "min.cleanable.dirty.ratio", "0.1",
                "delete.retention.ms", "604800000",
                "compression.type", "lz4"));
    }

    public static <C extends Command> void prepareCommandSchemas(String url, List<Class<C>> commandClasses) {
        SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
        Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
        objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
        objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
        try {
            for (Class<C> commandClass : commandClasses) {
                CommandInfo info = commandClass.getAnnotation(CommandInfo.class);
                src.register("commands." + info.type(),
                        new JsonSchema(jsonSchemaGenerator.generateSchema(commandClass), List.of(), Map.of(), info.version()),
                        info.version(),
                        -1);
            }
        } catch (IOException | RestClientException e) {
            throw new ApplicationContextException("Problem populating SchemaRegistry", e);
        }
    }

    public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<D>> domainEventClasses) {
        SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
        Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
        objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
        objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
        try {
            for (Class<D> domainEventClass : domainEventClasses) {
                DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
                src.register("domainevents." + info.type(),
                        new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
                        info.version(),
                        -1);
            }
        } catch (IOException | RestClientException e) {
            throw new ApplicationContextException("Problem populating SchemaRegistry", e);
        }
    }

    public static void prepareExternalServices(String bootstrapServers) {
        AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(new ObjectMapper());
        Map<String, Object> controlProducerProps = Map.of(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers,
                ProducerConfig.ACKS_CONFIG, "all",
                ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true",
                ProducerConfig.LINGER_MS_CONFIG, "0",
                ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1",
                ProducerConfig.RETRIES_CONFIG, "2147483647",
                ProducerConfig.RETRY_BACKOFF_MS_CONFIG, "0",
                ProducerConfig.TRANSACTIONAL_ID_CONFIG, "Test-AkcesControllerProducer",
                ProducerConfig.CLIENT_ID_CONFIG, "Test-AkcesControllerProducer");
        try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
            controlProducer.initTransactions();
            AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
                    "Account",
                    "Account-Commands",
                    "Account-DomainEvents",
                    List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
                    List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
                    List.of());
            controlProducer.beginTransaction();
            for (int partition = 0; partition < 3; partition++) {
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", aggregateServiceRecord));
            }
            controlProducer.commitTransaction();
        }
    }

    @Test
    public void testUnroutableCommandWithSendAndForget() {
        Assertions.assertNotNull(akcesClient);

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        Assertions.assertThrows(UnroutableCommandException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new UnroutableCommand(UUID.randomUUID().toString())));
    }

    @Test
    public void testUnroutableCommandWithSend() {
        Assertions.assertNotNull(akcesClient);

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        CompletionStage<List<DomainEvent>> result =  akcesClient.send("TEST_TENANT", new UnroutableCommand(UUID.randomUUID().toString()));

        ExecutionException executionException = Assertions.assertThrows(ExecutionException.class, () -> result.toCompletableFuture().get());
        Assertions.assertInstanceOf(UnroutableCommandException.class, executionException.getCause());
    }

    @Test
    public void testInvalidCommandWithSendAndForget() {
        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        Assertions.assertThrows(IllegalArgumentException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new InvalidCommand(UUID.randomUUID().toString())));
    }

    @Test
    public void testInvalidCommandWithSend() {
        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        Assertions.assertThrows(IllegalArgumentException.class, () -> akcesClient.send("TEST_TENANT", new InvalidCommand(UUID.randomUUID().toString())));
    }

    @Test
    public void testValidationErrorWithSendAndForget() {

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        Assertions.assertThrows(CommandValidationException.class, () -> akcesClient.sendAndForget("TEST_TENANT", new CreateAccountCommand(UUID.randomUUID().toString(), "NL", "Aike", "Christianen", null)));
    }

    @Test
    public void testValidationErrorWithSend() {

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT", new CreateAccountCommand(UUID.randomUUID().toString(), "NL", "Aike", "Christianen", null));
        ExecutionException executionException = Assertions.assertThrows(ExecutionException.class, () -> result.toCompletableFuture().get());
        Assertions.assertInstanceOf(CommandValidationException.class, executionException.getCause());
    }

    @Test
    public void testSendCommand() throws InterruptedException, JsonProcessingException {

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        String userId = "deb2b2c1-847c-44f3-a2a4-c81bc5ce795d";
        CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT",
                new CreateAccountCommand(userId,
                        "NL",
                        "Aike",
                        "Christianen",
                        "aike.christianen@gmail.com"));


        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {

            TopicPartition commandResponsesPartition = akcesClient.getCommandResponsePartition();
            TopicPartition commandPartition = new TopicPartition("Account-Commands", akcesClient.resolvePartition(userId));
            testConsumer.assign(List.of(commandPartition));
            testConsumer.seekToBeginning(List.of(commandPartition));
            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();

            while (allRecords.isEmpty()) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }
            Assertions.assertEquals(1, allRecords.size());
            CommandRecord cr = (CommandRecord) allRecords.getFirst();

            testProducer.beginTransaction();
            List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", "Aike", "Christianen", "aike.christianen@gmail.com")), PayloadEncoding.JSON, userId, null, 0));
            CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, null);

            testProducer.send(new ProducerRecord<>(commandResponsesPartition.topic(), commandResponsesPartition.partition(), crr.commandId(), crr));
            testProducer.commitTransaction();
        }

        CountDownLatch waitLatch = new CountDownLatch(1);
        result.whenComplete((s, throwable) -> waitLatch.countDown());
        Assertions.assertTrue(waitLatch.await(10, TimeUnit.SECONDS));
        Assertions.assertTrue(result.toCompletableFuture().isDone());
        Assertions.assertFalse(result.toCompletableFuture().isCompletedExceptionally());
        Assertions.assertNotNull(result.toCompletableFuture().getNow(null));
        Assertions.assertEquals(1, result.toCompletableFuture().getNow(null).size());
        AccountCreatedEvent ace = (AccountCreatedEvent) result.toCompletableFuture().getNow(null).getFirst();
        Assertions.assertEquals(userId, ace.userId());
        Assertions.assertEquals("NL", ace.country());
        Assertions.assertEquals("Aike", ace.firstName());
        Assertions.assertEquals("Christianen", ace.lastName());
        Assertions.assertEquals("aike.christianen@gmail.com", ace.email());
    }

    @Test
    public void testSendCommandWithCorrelationId() throws InterruptedException, JsonProcessingException {

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        String correlationId = UUID.randomUUID().toString();
        String userId = "dff9901f-6ccb-486a-9a79-5b7e300925c5";
        CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT",
                correlationId,
                new CreateAccountCommand(
                        userId,
                        "NL",
                        "Aike",
                        "Christianen",
                        "aike.christianen@gmail.com"));


        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {

            TopicPartition commandResponsesPartition = akcesClient.getCommandResponsePartition();
            TopicPartition commandPartition = new TopicPartition("Account-Commands", akcesClient.resolvePartition(userId));
            testConsumer.assign(List.of(commandPartition));
            testConsumer.seekToBeginning(List.of(commandPartition));

            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();

            while (allRecords.isEmpty()) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }
            Assertions.assertEquals(1, allRecords.size());
            CommandRecord cr = (CommandRecord) allRecords.getFirst();
            Assertions.assertEquals(correlationId, cr.correlationId());

            testProducer.beginTransaction();
            List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", "Aike", "Christianen", "aike.christianen@gmail.com")), PayloadEncoding.JSON, userId, null, 0));
            CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, null);

            testProducer.send(new ProducerRecord<>(commandResponsesPartition.topic(), commandResponsesPartition.partition(), crr.commandId(), crr));
            testProducer.commitTransaction();
        }

        CountDownLatch waitLatch = new CountDownLatch(1);
        result.whenComplete((s, throwable) -> waitLatch.countDown());
        Assertions.assertTrue(waitLatch.await(10, TimeUnit.SECONDS));
        Assertions.assertTrue(result.toCompletableFuture().isDone());
        Assertions.assertFalse(result.toCompletableFuture().isCompletedExceptionally());
        Assertions.assertNotNull(result.toCompletableFuture().getNow(null));
        Assertions.assertEquals(1, result.toCompletableFuture().getNow(null).size());
        AccountCreatedEvent ace = (AccountCreatedEvent) result.toCompletableFuture().getNow(null).getFirst();
        Assertions.assertEquals(userId, ace.userId());
        Assertions.assertEquals("NL", ace.country());
        Assertions.assertEquals("Aike", ace.firstName());
        Assertions.assertEquals("Christianen", ace.lastName());
        Assertions.assertEquals("aike.christianen@gmail.com", ace.email());
    }

    @Test
    public void testGDPRDecryption() throws InterruptedException, JsonProcessingException {

        while (!akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        String userId = "4da5341a-1f69-4995-b5dc-ecd492e940a0";
        CompletionStage<List<DomainEvent>> result = akcesClient.send("TEST_TENANT",
                new CreateAccountCommand(userId,
                        "NL",
                        "Aike",
                        "Christianen",
                        "aike.christianen@gmail.com"));


        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {

            TopicPartition commandResponsesPartition = akcesClient.getCommandResponsePartition();
            TopicPartition commandPartition = new TopicPartition("Account-Commands", akcesClient.resolvePartition(userId));
            testConsumer.assign(List.of(commandPartition));
            testConsumer.seekToBeginning(List.of(commandPartition));

            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();

            while (allRecords.isEmpty()) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }
            Assertions.assertEquals(1, allRecords.size());
            CommandRecord cr = (CommandRecord) allRecords.getFirst();

            SecretKeySpec secretKeySpec = GDPRKeyUtils.createKey();
            EncryptingGDPRContext gdprContext = new EncryptingGDPRContext(userId, secretKeySpec.getEncoded(), GDPRKeyUtils.isUUID(userId));
            String encryptedFirstName = gdprContext.encrypt("Aike");
            String encryptedLastName = gdprContext.encrypt("Christianen");
            String encryptedEmail = gdprContext.encrypt("aike.christianen@gmail.com");

            testProducer.beginTransaction();
            List<DomainEventRecord> events = List.of(new DomainEventRecord("TEST_TENANT", "AccountCreated", 1, objectMapper.writeValueAsBytes(new AccountCreatedEvent(userId, "NL", encryptedFirstName, encryptedLastName, encryptedEmail)), PayloadEncoding.JSON, userId, null, 0));
            CommandResponseRecord crr = new CommandResponseRecord("TEST_TENANT", userId, cr.correlationId(), cr.id(), events, secretKeySpec.getEncoded());

            testProducer.send(new ProducerRecord<>(commandResponsesPartition.topic(), commandResponsesPartition.partition(), crr.commandId(), crr));
            testProducer.commitTransaction();
        }

        CountDownLatch waitLatch = new CountDownLatch(1);
        result.whenComplete((s, throwable) -> waitLatch.countDown());
        Assertions.assertTrue(waitLatch.await(10, TimeUnit.SECONDS));
        Assertions.assertTrue(result.toCompletableFuture().isDone());
        Assertions.assertFalse(result.toCompletableFuture().isCompletedExceptionally());
        Assertions.assertNotNull(result.toCompletableFuture().getNow(null));
        Assertions.assertEquals(1, result.toCompletableFuture().getNow(null).size());
        AccountCreatedEvent ace = (AccountCreatedEvent) result.toCompletableFuture().getNow(null).getFirst();
        Assertions.assertEquals(userId, ace.userId());
        Assertions.assertEquals("NL", ace.country());
        Assertions.assertEquals("Aike", ace.firstName());
        Assertions.assertEquals("Christianen", ace.lastName());
        Assertions.assertEquals("aike.christianen@gmail.com", ace.email());
    }

    public static class ContextInitializer
            implements ApplicationContextInitializer<ConfigurableApplicationContext> {

        @Override
        public void initialize(ConfigurableApplicationContext applicationContext) {

            prepareKafka(kafka.getBootstrapServers());
            prepareCommandSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), List.of(CreateAccountCommand.class));
            prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), List.of(AccountCreatedEvent.class));
            prepareExternalServices(kafka.getBootstrapServers());

            TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
                    applicationContext,
                    "spring.kafka.enabled=true",
                    "spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
                    "akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)
            );
        }
    }
}

================
File: client/src/test/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
akces.client.domainEventsPackage=org.elasticsoftware.akces.client.events
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: client/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.client" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: client/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.7.21-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Client</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-client</artifactId>
    <packaging>jar</packaging>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
    </dependencies>

</project>

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/beans/QueryModelBeanFactoryPostProcessor.java
================
package org.elasticsoftware.akces.query.models.beans;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.annotations.QueryModelEventHandler;
import org.elasticsoftware.akces.annotations.QueryModelInfo;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.query.QueryModelState;
import org.elasticsoftware.akces.query.models.QueryModelRuntimeFactory;
import org.springframework.beans.BeansException;
import org.springframework.beans.factory.aot.BeanFactoryInitializationAotContribution;
import org.springframework.beans.factory.aot.BeanFactoryInitializationAotProcessor;
import org.springframework.beans.factory.aot.BeanRegistrationExcludeFilter;
import org.springframework.beans.factory.config.BeanDefinition;
import org.springframework.beans.factory.config.BeanFactoryPostProcessor;
import org.springframework.beans.factory.config.ConfigurableListableBeanFactory;
import org.springframework.beans.factory.support.BeanDefinitionBuilder;
import org.springframework.beans.factory.support.BeanDefinitionRegistry;
import org.springframework.beans.factory.support.RegisteredBean;
import org.springframework.context.ApplicationContextException;

import java.lang.reflect.Method;
import java.util.Arrays;
import java.util.List;

public class QueryModelBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
    @Override
    public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
        if (beanFactory instanceof BeanDefinitionRegistry bdr) {
            Arrays.asList(beanFactory.getBeanNamesForAnnotation(QueryModelInfo.class)).forEach(beanName -> {
                BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
                try {
                    Class<?> queryModelClass = Class.forName(bd.getBeanClassName());
                    List<Method> queryModelEventHandlers = Arrays.stream(queryModelClass.getMethods())
                            .filter(method -> method.isAnnotationPresent(QueryModelEventHandler.class))
                            .toList();
                    queryModelEventHandlers.forEach(eventHandlerMethod -> processQueryModelEventHandler(beanName, eventHandlerMethod, bdr));
                } catch (ClassNotFoundException e) {
                    throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
                }

                bdr.registerBeanDefinition(beanName + "QueryModelRuntime",
                        BeanDefinitionBuilder.genericBeanDefinition(QueryModelRuntimeFactory.class)
                                .addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
                                .addConstructorArgReference("akcesQueryModelSchemaRegistry")
                                .addConstructorArgReference(beanName)
                                .getBeanDefinition());
            });
        } else {
            throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
        }
    }

    private void processQueryModelEventHandler(String aggregateBeanName, Method queryModelEventHandlerMethod, BeanDefinitionRegistry bdr) {
        QueryModelEventHandler queryModelEventHandler = queryModelEventHandlerMethod.getAnnotation(QueryModelEventHandler.class);
        if (queryModelEventHandlerMethod.getParameterCount() == 2 &&
                DomainEvent.class.isAssignableFrom(queryModelEventHandlerMethod.getParameterTypes()[0]) &&
                QueryModelState.class.isAssignableFrom(queryModelEventHandlerMethod.getParameterTypes()[1]) &&
                QueryModelState.class.isAssignableFrom(queryModelEventHandlerMethod.getReturnType())) {
            DomainEventInfo eventInfo = queryModelEventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);

            String beanName = aggregateBeanName + "_qmeh_" + queryModelEventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
            bdr.registerBeanDefinition(beanName,
                    BeanDefinitionBuilder.genericBeanDefinition(QueryModelEventHandlerFunctionAdapter.class)
                            .addConstructorArgReference(aggregateBeanName)
                            .addConstructorArgValue(queryModelEventHandlerMethod.getName())
                            .addConstructorArgValue(queryModelEventHandlerMethod.getParameterTypes()[0])
                            .addConstructorArgValue(queryModelEventHandlerMethod.getParameterTypes()[1])
                            .addConstructorArgValue(queryModelEventHandler.create())
                            .addConstructorArgValue(eventInfo.type())
                            .addConstructorArgValue(eventInfo.version())
                            .setInitMethodName("init")
                            .getBeanDefinition());
        } else {
            throw new ApplicationContextException("Invalid QueryModelEventHandler method signature: " + queryModelEventHandlerMethod);
        }
    }

    @Override
    public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
        return null;
    }

    @Override
    public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {
        return false;
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/beans/QueryModelEventHandlerFunctionAdapter.java
================
package org.elasticsoftware.akces.query.models.beans;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akces.query.QueryModelEventHandlerFunction;
import org.elasticsoftware.akces.query.QueryModelState;

import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;

public class QueryModelEventHandlerFunctionAdapter<S extends QueryModelState, E extends DomainEvent>
        implements QueryModelEventHandlerFunction<S, E> {
    private final QueryModel<S> queryModel;
    private final String adapterMethodName;
    private final Class<E> domainEventClass;
    private final Class<S> stateClass;
    private final boolean create;
    private final DomainEventType<E> domainEventType;
    private Method adapterMethod;

    public QueryModelEventHandlerFunctionAdapter(QueryModel<S> queryModel,
                                                 String adapterMethodName,
                                                 Class<E> domainEventClass,
                                                 Class<S> stateClass,
                                                 boolean create,
                                                 String typeName,
                                                 int version) {
        this.queryModel = queryModel;
        this.adapterMethodName = adapterMethodName;
        this.domainEventClass = domainEventClass;
        this.stateClass = stateClass;
        this.create = create;
        this.domainEventType = new DomainEventType<>(
                typeName,
                version,
                domainEventClass,
                create,
                true,
                false);
    }

    @SuppressWarnings("unused")
    public void init() {
        try {
            adapterMethod = queryModel.getClass().getMethod(adapterMethodName, domainEventClass, stateClass);
        } catch (NoSuchMethodException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public @NotNull S apply(@NotNull E event, S state) {
        try {
            return (S) adapterMethod.invoke(queryModel, event, state);
        } catch (IllegalAccessException e) {
            throw new RuntimeException(e);
        } catch (InvocationTargetException e) {
            if (e.getCause() != null) {
                if (e.getCause() instanceof RuntimeException) {
                    throw (RuntimeException) e.getCause();
                } else {
                    throw new RuntimeException(e.getCause());
                }
            } else {
                throw new RuntimeException(e);
            }
        }
    }

    @Override
    public DomainEventType<E> getEventType() {
        return domainEventType;
    }

    @Override
    public QueryModel<S> getQueryModel() {
        return queryModel;
    }

    @Override
    public boolean isCreate() {
        return create;
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelAutoConfiguration.java
================
package org.elasticsoftware.akces.query.models;

import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.elasticsoftware.akces.gdpr.GDPRContextRepositoryFactory;
import org.elasticsoftware.akces.gdpr.RocksDBGDPRContextRepositoryFactory;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.kafka.CustomKafkaConsumerFactory;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.query.models.beans.QueryModelBeanFactoryPostProcessor;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;
import org.elasticsoftware.akces.util.EnvironmentPropertiesPrinter;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.autoconfigure.condition.ConditionalOnBean;
import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;
import org.springframework.boot.autoconfigure.jackson.Jackson2ObjectMapperBuilderCustomizer;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Conditional;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.PropertySource;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdmin;

import java.math.BigDecimal;
import java.util.List;
import java.util.Map;

@Configuration
@PropertySource("classpath:akces-querymodel.properties")
public class AkcesQueryModelAutoConfiguration {
    private final ProtocolRecordSerde serde = new ProtocolRecordSerde();

    @Conditional(QueryModelImplementationPresentCondition.class)
    @Bean(name = "queryModelBeanFactoryPostProcessor")
    public static QueryModelBeanFactoryPostProcessor queryModelBeanFactoryPostProcessor() {
        return new QueryModelBeanFactoryPostProcessor();
    }

    @Bean(name = "akcesQueryModelJsonCustomizer")
    public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
        return builder -> {
            builder.modulesToInstall(new AkcesGDPRModule());
            builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        };
    }

    @ConditionalOnBean(QueryModelBeanFactoryPostProcessor.class)
    @Bean(name = "akcesQueryModelSchemaRegistryClient")
    public SchemaRegistryClient createSchemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
        return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
    }

    @ConditionalOnBean(QueryModelBeanFactoryPostProcessor.class)
    @Bean(name = "akcesQueryModelSchemaRegistry")
    public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("akcesQueryModelSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
        return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
    }

    @ConditionalOnBean(QueryModelBeanFactoryPostProcessor.class)
    @Bean(name = "akcesQueryModelConsumerFactory")
    public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
        return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
    }

    @ConditionalOnBean(QueryModelBeanFactoryPostProcessor.class)
    @Bean(name = "akcesQueryModelGDPRContextRepositoryFactory")
    public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
        return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);

    }

    @ConditionalOnBean(QueryModelBeanFactoryPostProcessor.class)
    @Bean(name = "akcesQueryModelKafkaAdmin")
    public KafkaAdmin kafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
        return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
    }

    @ConditionalOnBean(QueryModelBeanFactoryPostProcessor.class)
    @Bean(name = "ackesQueryModelController", initMethod = "start", destroyMethod = "close")
    public AkcesQueryModelController queryModelRuntimes(@Qualifier("akcesQueryModelKafkaAdmin") KafkaAdmin kafkaAdmin,
                                                        @Qualifier("akcesQueryModelConsumerFactory") ConsumerFactory<String, ProtocolRecord> consumerFactory,
                                                        @Qualifier("akcesQueryModelGDPRContextRepositoryFactory") GDPRContextRepositoryFactory gdprContextRepositoryFactory) {
        return new AkcesQueryModelController(kafkaAdmin, consumerFactory, gdprContextRepositoryFactory);
    }

    @ConditionalOnMissingBean(EnvironmentPropertiesPrinter.class)
    @Bean(name = "environmentPropertiesPrinter")
    public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
        return new EnvironmentPropertiesPrinter();
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelController.java
================
package org.elasticsoftware.akces.query.models;

import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;
import jakarta.annotation.Nullable;
import org.apache.kafka.clients.admin.TopicDescription;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.InterruptException;
import org.apache.kafka.common.errors.WakeupException;
import org.elasticsoftware.akces.gdpr.GDPRContext;
import org.elasticsoftware.akces.gdpr.GDPRContextHolder;
import org.elasticsoftware.akces.gdpr.GDPRContextRepository;
import org.elasticsoftware.akces.gdpr.GDPRContextRepositoryFactory;
import org.elasticsoftware.akces.protocol.DomainEventRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akces.query.QueryModelState;
import org.elasticsoftware.akces.util.HostUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.BeansException;
import org.springframework.boot.availability.AvailabilityChangeEvent;
import org.springframework.boot.availability.LivenessState;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdminOperations;

import java.io.IOException;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.*;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import static java.nio.charset.StandardCharsets.UTF_8;
import static org.elasticsoftware.akces.query.models.AkcesQueryModelControllerState.*;
import static org.elasticsoftware.akces.util.KafkaUtils.getIndexTopicName;

@SuppressWarnings({"rawtypes", "unchecked"})
public class AkcesQueryModelController extends Thread implements AutoCloseable, ApplicationContextAware, QueryModels {
    private static final Logger logger = LoggerFactory.getLogger(AkcesQueryModelController.class);
    private final Map<Class<? extends QueryModel>, QueryModelRuntime> enabledRuntimes = new ConcurrentHashMap<>();
    private final Map<Class<? extends QueryModel>, QueryModelRuntime> disabledRuntimes = new ConcurrentHashMap<>();
    private final KafkaAdminOperations kafkaAdmin;
    private final ConsumerFactory<String, ProtocolRecord> consumerFactory;
    private final GDPRContextRepositoryFactory gdprContextRepositoryFactory;
    private final Map<TopicPartition, GDPRContextRepository> gdprContextRepositories = new HashMap<>();
    private final BlockingQueue<HydrationRequest<?>> commandQueue = new LinkedBlockingQueue<>();
    private final Map<TopicPartition, HydrationExecution<?>> hydrationExecutions = new HashMap<>();
    private final CountDownLatch shutdownLatch = new CountDownLatch(1);
    private volatile AkcesQueryModelControllerState processState = INITIALIZING;
    private final Set<TopicPartition> gdprKeyPartitions = new HashSet<>();
    private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
    private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
    private int totalPartitions;
    private ApplicationContext applicationContext;
    private final Cache<String, CachedQueryModelState<?>> queryModelStateCache = Caffeine.newBuilder()
            .maximumSize(1000)
            .build();

    public AkcesQueryModelController(KafkaAdminOperations kafkaAdmin,
                                     ConsumerFactory<String, ProtocolRecord> consumerFactory,
                                     GDPRContextRepositoryFactory gdprContextRepositoryFactory) {
        super("AkcesQueryModelController");
        this.kafkaAdmin = kafkaAdmin;
        this.consumerFactory = consumerFactory;
        this.gdprContextRepositoryFactory = gdprContextRepositoryFactory;
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        this.applicationContext = applicationContext;
        this.enabledRuntimes.putAll(
                applicationContext.getBeansOfType(QueryModelRuntime.class).values().stream()
                        .collect(Collectors.toMap(runtime -> ((QueryModelRuntime<?>) runtime).getQueryModelClass(), runtime -> runtime)));
    }


    @SuppressWarnings("unchecked")
    private <S extends QueryModelState> QueryModelRuntime<S> getEnabledRuntime(Class<? extends QueryModel<S>> modelClass) {
        return (QueryModelRuntime<S>) this.enabledRuntimes.get(modelClass);
    }

    private <S extends QueryModelState> boolean isRuntimeDisabled(Class<? extends QueryModel<S>> modelClass) {
        return this.disabledRuntimes.containsKey(modelClass);
    }

    @Override
    public <S extends QueryModelState> CompletionStage<S> getHydratedState(Class<? extends QueryModel<S>> modelClass, String id) {
        QueryModelRuntime<S> runtime = getEnabledRuntime(modelClass);
        if (runtime != null) {
            CachedQueryModelState<S> cachedQueryModelState = (CachedQueryModelState<S>) queryModelStateCache.getIfPresent(runtime.getName()+"-"+id);
            S currentState = cachedQueryModelState != null ? cachedQueryModelState.state() : null;
            Long currentOffset = cachedQueryModelState != null ? cachedQueryModelState.offset() : null;
            CompletableFuture<S> completableFuture = new CompletableFuture<>();
            commandQueue.add(new HydrationRequest<>(runtime, completableFuture, id, currentState, currentOffset));
            return completableFuture;
        } else if (isRuntimeDisabled(modelClass)) {

            return CompletableFuture.failedFuture(new QueryModelExecutionDisabledException(modelClass));
        } else {

            return CompletableFuture.failedFuture(new QueryModelNotFoundException(modelClass));
        }
    }

    @Override
    public void run() {
        try (final Consumer<String, ProtocolRecord> indexConsumer = consumerFactory.createConsumer(
                HostUtils.getHostName() + "-AkcesQueryModelController",
                HostUtils.getHostName() + "-AkcesQueryModelController",
                null)) {

            while (processState != SHUTTING_DOWN) {
                process(indexConsumer);
            }
            logger.info("AkcesQueryModelController is shutting down");

            List<HydrationRequest<?>> pendingRequests = new ArrayList<>();
            commandQueue.drainTo(pendingRequests);
            pendingRequests.forEach(request -> request.completableFuture.completeExceptionally(
                    new QueryModelExecutionCancelledException(request.runtime().getQueryModelClass())));

            Iterator<HydrationExecution<?>> iterator = hydrationExecutions.values().iterator();
            while (iterator.hasNext()) {
                HydrationExecution<?> execution = iterator.next();
                execution.completableFuture.completeExceptionally(
                        new QueryModelExecutionCancelledException(execution.runtime().getQueryModelClass()));
                iterator.remove();
            }

            for (GDPRContextRepository gdprContextRepository : gdprContextRepositories.values()) {
                try {
                    gdprContextRepository.close();
                } catch (IOException e) {

                }
            }

            applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));
        }
        shutdownLatch.countDown();
    }

    private void process(Consumer<String, ProtocolRecord> indexConsumer) {
        if (processState == RUNNING) {
            try {
                Map<TopicPartition, HydrationExecution<?>> newExecutions = processHydrationRequests(indexConsumer);

                hydrationExecutions.putAll(newExecutions);
                indexConsumer.assign(Stream.concat(hydrationExecutions.keySet().stream(), gdprKeyPartitions.stream()).toList());

                if(!newExecutions.isEmpty()){
                    logger.info("Processing {} new HydrationExecutions", newExecutions.size());

                    indexConsumer.endOffsets(newExecutions.keySet()).forEach((partition, endOffset) -> {
                        hydrationExecutions.computeIfPresent(partition, (topicPartition, hydrationExecution) -> hydrationExecution.withEndOffset(endOffset));
                    });
                }

                newExecutions.forEach((partition, execution) -> {
                    if (execution.currentOffset() != null) {
                        indexConsumer.seek(partition, execution.currentOffset());
                    } else if(hydrationExecutions.get(partition).endOffset() > 0) {

                        indexConsumer.seek(partition, 0);
                    } else {

                        indexConsumer.seekToBeginning(List.of(partition));
                    }
                });
                if(!hydrationExecutions.isEmpty()) {
                    logger.info("Processing HydrationExecutions {}", hydrationExecutions);
                }
                ConsumerRecords<String, ProtocolRecord> consumerRecords = indexConsumer.poll(Duration.ofMillis(10));
                if(!consumerRecords.isEmpty()) {
                    logger.info("Processing {}", consumerRecords.partitions());

                    if(!gdprKeyPartitions.isEmpty()) {
                        List<TopicPartition> gdprKeyPartitions = consumerRecords.partitions().stream()
                                .filter(topicPartition -> topicPartition.topic().equals("Akces-GDPRKeys")).toList();
                        logger.info("Processing {} GDPRKeyPartitions", gdprKeyPartitions.size());
                        for (TopicPartition gdprKeyPartition : gdprKeyPartitions) {
                            gdprContextRepositories.get(gdprKeyPartition).process(consumerRecords.records(gdprKeyPartition));
                        }
                    }
                    if(!hydrationExecutions.isEmpty()) {
                        List<TopicPartition> indexPartitions = consumerRecords.partitions().stream()
                                .filter(partition -> !partition.topic().equals("Akces-GDPRKeys")).toList();
                        logger.info("Processing {} indexPartitions", indexPartitions.size());
                        for (TopicPartition partition : indexPartitions) {
                            hydrationExecutions.computeIfPresent(partition,
                                    (topicPartition, hydrationExecution) ->
                                            processHydrationExecution(
                                                    hydrationExecution.runtime().shouldHandlePIIData() ? getGDPRContextRepository(hydrationExecution.id()) : null,
                                                    hydrationExecution,
                                                    consumerRecords.records(partition)));
                        }
                    }
                }

                Iterator<HydrationExecution<?>> itr = hydrationExecutions.values().iterator();
                while (itr.hasNext()) {
                    HydrationExecution<?> execution = itr.next();
                    if (execution.endOffset() <= indexConsumer.position(execution.indexPartition())) {
                        logger.info(
                                "HydrationExecution on index {} with id {} and runtime {} is complete: indexPartition {} endOffset {} consumerPosition {}",
                                execution.runtime().getIndexName(),
                                execution.id(),
                                execution.runtime().getName(),
                                execution.indexPartition(),
                                execution.endOffset(),
                                indexConsumer.position(execution.indexPartition()));

                        execution.complete();
                        itr.remove();
                        queryModelStateCache.put(
                                execution.runtime().getName()+"-"+execution.id(),
                                new CachedQueryModelState<>(
                                        execution.currentState(),
                                        indexConsumer.position(execution.indexPartition())));
                    }
                }
            } catch (WakeupException | InterruptException e) {

            } catch (KafkaException e) {

                logger.error("Unrecoverable exception in AkcesQueryModelController while {}", processState, e);

                processState = SHUTTING_DOWN;
            }
        } else if (processState == LOADING_GDPR_KEYS) {

            try {
                ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = indexConsumer.poll(Duration.ofMillis(10));

                for (TopicPartition gdprKeyPartition : gdprKeyPartitions) {
                   gdprContextRepositories.get(gdprKeyPartition).process(gdprKeyRecords.records(gdprKeyPartition));

                    initializedEndOffsets.computeIfPresent(gdprKeyPartition, (partition, endOffset) -> {
                        if (endOffset <= indexConsumer.position(gdprKeyPartition)) {
                            return null;
                        } else {
                            return endOffset;
                        }
                    });
                }

                if (gdprKeyRecords.isEmpty() && initializedEndOffsets.isEmpty()) {
                    processState = RUNNING;
                }
            } catch (WakeupException | InterruptException e) {

            } catch (KafkaException e) {

                logger.error("Unrecoverable exception in AkcesQueryModelController while {}", processState, e);

                processState = SHUTTING_DOWN;
            }
        } else if (processState == INITIALIZING) {
            try {
                Iterator<QueryModelRuntime> iterator = enabledRuntimes.values().iterator();
                while (iterator.hasNext()) {
                    QueryModelRuntime queryModelRuntime = iterator.next();
                    try {
                        queryModelRuntime.validateDomainEventSchemas();
                        logger.info("Enabling {} QueryModelRuntime", queryModelRuntime.getName());
                    } catch (org.elasticsoftware.akces.schemas.SchemaException e) {
                        logger.error(
                                "SchemaException while validating DomainEventSchemas for QueryModel {}. Disabling QueryModel",
                                queryModelRuntime.getName(),
                                e);
                        iterator.remove();

                        disabledRuntimes.put(queryModelRuntime.getQueryModelClass(), queryModelRuntime);
                    }
                }
                if(enabledRuntimes.isEmpty() && !disabledRuntimes.isEmpty()) {
                    logger.error("No QueryModelRuntimes enabled. This is an error. Shutting down");
                    processState = SHUTTING_DOWN;
                } else if (enabledRuntimes.values().stream().anyMatch(QueryModelRuntime::shouldHandlePIIData)) {

                    logger.info("Loading GDPR keys");

                    TopicDescription controlTopicDescription = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control");
                    totalPartitions = controlTopicDescription.partitions().size();

                    for (int i = 0; i < totalPartitions; i++) {
                        gdprKeyPartitions.add(new TopicPartition("Akces-GDPRKeys", i));
                    }

                    gdprKeyPartitions.forEach(partition -> {
                        gdprContextRepositories.put(partition, gdprContextRepositoryFactory.create("AkcesQueryModelController",partition.partition()));
                    });

                    indexConsumer.assign(gdprKeyPartitions);

                    gdprKeyPartitions.forEach(partition -> {
                        indexConsumer.seek(partition, gdprContextRepositories.get(partition).getOffset() + 1);
                    });

                    initializedEndOffsets = indexConsumer.endOffsets(gdprKeyPartitions);
                    processState = LOADING_GDPR_KEYS;
                } else {

                    processState = RUNNING;
                }
            } catch (WakeupException | InterruptException e) {

            } catch (KafkaException e) {

                logger.error("Unrecoverable exception in AkcesQueryModelController while {}", processState, e);

                processState = SHUTTING_DOWN;
            }
        }
    }

    private GDPRContextRepository getGDPRContextRepository(String id) {
        Integer partition = Math.abs(hashFunction.hashString(id, UTF_8).asInt()) % totalPartitions;
        return gdprContextRepositories.get(new TopicPartition("Akces-GDPRKeys", partition));
    }

    private Map<TopicPartition, HydrationExecution<?>> processHydrationRequests(Consumer<String, ProtocolRecord> indexConsumer) {
        Map<TopicPartition, HydrationExecution<?>> newExecutions = new HashMap<>();

        try {
            HydrationRequest request = commandQueue.poll(100, TimeUnit.MILLISECONDS);
            while (request != null) {
                logger.info("Processing HydrationRequest on index {} with id {} and runtime {}", request.runtime().getIndexName(), request.id(), request.runtime().getName());

                QueryModelRuntime runtime = request.runtime();
                String topicName = getIndexTopicName(runtime.getIndexName(), request.id());
                if (!indexConsumer.partitionsFor(topicName).isEmpty()) {
                    TopicPartition indexPartition = new TopicPartition(topicName, 0);
                    newExecutions.put(indexPartition, new HydrationExecution<>(runtime, request.completableFuture(), request.id(), request.currentState(), request.currentOffset(), indexPartition, null));
                } else {
                    logger.warn("KafkaTopic {} not found for HydrationRequest on index {} with id {}", topicName, request.runtime().getIndexName(), request.id());
                    request.completableFuture().completeExceptionally(new QueryModelIdNotFoundException(request.runtime().getQueryModelClass(), request.id()));
                }
                request = commandQueue.poll();
            }
            return newExecutions;
        } catch (InterruptedException e) {
            logger.warn("Interrupted while processing HydrationRequests", e);

            Thread.currentThread().interrupt();
        }
        return newExecutions;
    }

    private <S extends QueryModelState> HydrationExecution<S> processHydrationExecution(@Nullable GDPRContextRepository gdprContextRepository,
                                                                                        HydrationExecution<S> execution,
                                                                                        List<ConsumerRecord<String, ProtocolRecord>> records) {
        try {
            if(gdprContextRepository != null) {
                GDPRContext gdprContext = gdprContextRepository.get(execution.id());
                logger.info("Setting GDPRContext {} for aggregateId {}", gdprContext.getClass().getSimpleName(), execution.id());
                GDPRContextHolder.setCurrentGDPRContext(gdprContext);
            }
            logger.info(
                    "Processing {} records HydrationExecution on index {} with id {} and runtime {}",
                    records.size(),
                    execution.runtime().getIndexName(),
                    execution.id(),
                    execution.runtime().getName());
            return execution.withCurrentState(execution.runtime().apply(
                    records.stream().map(record -> (DomainEventRecord) record.value())
                            .toList(), execution.currentState()));
        } catch (IOException e) {
            logger.error("Exception while processing HydrationExecution", e);
            execution.completableFuture.completeExceptionally(
                    new QueryModelExecutionException(
                            "Exception while processing HydrationExecution",
                            execution.runtime().getQueryModelClass(),
                            e));
            return null;
        } finally {
            GDPRContextHolder.resetCurrentGDPRContext();
        }
    }

    @Override
    public void close() throws Exception {
        processState = SHUTTING_DOWN;

        try {
            if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
                logger.info("AkcesQueryModelController has been shutdown");
            } else {
                logger.warn("AkcesQueryModelController did not shutdown within 10 seconds");
            }
        } catch (InterruptedException e) {

        }
    }

    public boolean isRunning() {
        return processState == RUNNING;
    }

    private record HydrationRequest<S extends QueryModelState>(QueryModelRuntime<S> runtime,
                                                               CompletableFuture<S> completableFuture, String id,
                                                               S currentState, Long currentOffset) {
    }

    private record HydrationExecution<S extends QueryModelState>(QueryModelRuntime<S> runtime,
                                                                 CompletableFuture<S> completableFuture, String id,
                                                                 S currentState, Long currentOffset,
                                                                 TopicPartition indexPartition, Long endOffset) {
        HydrationExecution<S> withEndOffset(Long endOffset) {
            return new HydrationExecution<>(runtime, completableFuture, id, currentState, currentOffset, indexPartition, endOffset);
        }

        HydrationExecution<S> withCurrentState(S currentState) {
            return new HydrationExecution<>(runtime, completableFuture, id, currentState, currentOffset, indexPartition, endOffset);
        }

        void complete() {
            if(currentState != null) {
                completableFuture.complete(currentState);
            } else {

                completableFuture.completeExceptionally(new QueryModelIdNotFoundException(runtime.getQueryModelClass(), id));
            }
        }
    }

    private record CachedQueryModelState<S extends QueryModelState>(S state, Long offset) {
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/AkcesQueryModelControllerState.java
================
package org.elasticsoftware.akces.query.models;

public enum AkcesQueryModelControllerState {
    INITIALIZING,
    LOADING_GDPR_KEYS,
    RUNNING,
    SHUTTING_DOWN
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/KafkaQueryModelRuntime.java
================
package org.elasticsoftware.akces.query.models;

import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.gdpr.GDPRAnnotationUtils;
import org.elasticsoftware.akces.protocol.DomainEventRecord;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akces.query.QueryModelEventHandlerFunction;
import org.elasticsoftware.akces.query.QueryModelState;
import org.elasticsoftware.akces.query.QueryModelStateType;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class KafkaQueryModelRuntime<S extends QueryModelState> implements QueryModelRuntime<S> {
    private static final Logger logger = LoggerFactory.getLogger(KafkaQueryModelRuntime.class);
    private final KafkaSchemaRegistry schemaRegistry;
    private final ObjectMapper objectMapper;
    private final Map<Class<? extends DomainEvent>, JsonSchema> domainEventSchemas = new HashMap<>();
    private final QueryModelStateType<?> type;
    private final Class<? extends QueryModel<S>> queryModelClass;
    private final Map<Class<?>, DomainEventType<?>> domainEvents;
    private final QueryModelEventHandlerFunction<S, DomainEvent> createStateHandler;
    private final Map<DomainEventType<?>, QueryModelEventHandlerFunction<S, DomainEvent>> queryModelEventHandlers;
    private final boolean shouldHandlePIIData;

    private KafkaQueryModelRuntime(KafkaSchemaRegistry schemaRegistry,
                                   ObjectMapper objectMapper,
                                   QueryModelStateType<S> type,
                                   Class<? extends QueryModel<S>> queryModelClass,
                                   QueryModelEventHandlerFunction<S, DomainEvent> createStateHandler,
                                   Map<Class<?>, DomainEventType<?>> domainEvents,
                                   Map<DomainEventType<?>, QueryModelEventHandlerFunction<S, DomainEvent>> queryModelEventHandlers, boolean shouldHandlePIIData) {
        this.schemaRegistry = schemaRegistry;
        this.objectMapper = objectMapper;
        this.type = type;
        this.queryModelClass = queryModelClass;
        this.domainEvents = domainEvents;
        this.createStateHandler = createStateHandler;
        this.queryModelEventHandlers = queryModelEventHandlers;
        this.shouldHandlePIIData = shouldHandlePIIData;
    }

    @Override
    public String getName() {
        return type.typeName();
    }

    @Override
    public String getIndexName() {
        return type.indexName();
    }

    @Override
    public Class<? extends QueryModel<S>> getQueryModelClass() {
        return queryModelClass;
    }

    @Override
    public S apply(List<DomainEventRecord> eventRecords, S currentState) throws IOException {
        S state = currentState;
        for (DomainEventRecord eventRecord : eventRecords) {

            DomainEventType<?> domainEventType = getDomainEventType(eventRecord);

            if (domainEventType != null) {
                if (state == null && createStateHandler != null && createStateHandler.getEventType().equals(domainEventType)) {
                    state = createStateHandler.apply(materialize(domainEventType, eventRecord), null);
                } else if (state != null) {

                    QueryModelEventHandlerFunction<S, DomainEvent> eventHandler = queryModelEventHandlers.get(domainEventType);
                    if (eventHandler != null) {
                        state = eventHandler.apply(materialize(domainEventType, eventRecord), state);
                    }
                }
            }
        }
        return state;
    }

    @Override
    public void validateDomainEventSchemas() {
        for (DomainEventType<?> domainEventType : domainEvents.values()) {
            schemaRegistry.validate(domainEventType);
        }
    }

    @Override
    public boolean shouldHandlePIIData() {
        return shouldHandlePIIData;
    }

    private DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
        return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
    }

    private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {

        return domainEvents.entrySet().stream()
                .filter(entry -> entry.getValue().external())
                .filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
                .filter(entry -> entry.getValue().version() <= eventRecord.version())
                .max(Comparator.comparingInt(entry -> entry.getValue().version()))
                .map(Map.Entry::getValue).orElse(null);
    }

    public static class Builder<S extends QueryModelState> {
        private final Map<Class<?>, DomainEventType<?>> domainEvents = new HashMap<>();
        private final Map<DomainEventType<?>, QueryModelEventHandlerFunction<S, DomainEvent>> queryModelEventHandlers = new HashMap<>();
        private KafkaSchemaRegistry schemaRegistry;
        private ObjectMapper objectMapper;
        private QueryModelStateType<S> stateType;
        private Class<? extends QueryModel<S>> queryModelClass;
        private QueryModelEventHandlerFunction<S, DomainEvent> createStateHandler;

        public Builder<S> setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
            this.schemaRegistry = schemaRegistry;
            return this;
        }

        public Builder<S> setObjectMapper(ObjectMapper objectMapper) {
            this.objectMapper = objectMapper;
            return this;
        }

        public Builder<S> setStateType(QueryModelStateType<S> stateType) {
            this.stateType = stateType;
            return this;
        }

        public Builder<S> setQueryModelClass(Class<? extends QueryModel<S>> queryModelClass) {
            this.queryModelClass = queryModelClass;
            return this;
        }

        public Builder<S> setCreateHandler(QueryModelEventHandlerFunction<S, DomainEvent> createStateHandler) {
            this.createStateHandler = createStateHandler;
            return this;
        }

        public Builder<S> addDomainEvent(DomainEventType<?> domainEvent) {
            this.domainEvents.put(domainEvent.typeClass(), domainEvent);
            return this;
        }

        public Builder<S> addQueryModelEventHandler(DomainEventType<?> eventType,
                                                 QueryModelEventHandlerFunction<S, DomainEvent> eventSourcingHandler) {
            this.queryModelEventHandlers.put(eventType, eventSourcingHandler);
            return this;
        }

        public KafkaQueryModelRuntime<S> build() {
            final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
                    .anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);
            return new KafkaQueryModelRuntime<>(
                    schemaRegistry,
                    objectMapper,
                    stateType,
                    queryModelClass,
                    createStateHandler,
                    domainEvents,
                    queryModelEventHandlers,
                    shouldHandlePIIData);
        }
    }

    @Override
    public String toString() {
        return "KafkaQueryModelRuntime{"+getName()+"}";
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionCancelledException.java
================
package org.elasticsoftware.akces.query.models;

public class QueryModelExecutionCancelledException extends QueryModelExecutionException {
    public QueryModelExecutionCancelledException(Class<?> modelClass) {
        super("Query model execution cancelled", modelClass);
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionDisabledException.java
================
package org.elasticsoftware.akces.query.models;

public class QueryModelExecutionDisabledException extends QueryModelExecutionException {

    public QueryModelExecutionDisabledException(Class<?> modelClass) {
        super("Query model execution is disabled due to Schema validation errors", modelClass);
    }

}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelExecutionException.java
================
package org.elasticsoftware.akces.query.models;

public class QueryModelExecutionException extends RuntimeException {
    private final Class<?> modelClass;

    public QueryModelExecutionException(String message, Class<?> modelClass) {
        super(message);
        this.modelClass = modelClass;
    }

    public QueryModelExecutionException(String message, Class<?> modelClass, Throwable cause) {
        super(message, cause);
        this.modelClass = modelClass;
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelIdNotFoundException.java
================
package org.elasticsoftware.akces.query.models;

public class QueryModelIdNotFoundException extends QueryModelExecutionException {
    private final String modelId;

    public QueryModelIdNotFoundException(Class<?> modelClass, String modelId) {
        super("Id "+modelId+ "doesn't exist for QueryModel: "+modelClass.getSimpleName(), modelClass);
        this.modelId = modelId;
    }

    public String getModelId() {
        return modelId;
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelImplementationPresentCondition.java
================
package org.elasticsoftware.akces.query.models;

import org.elasticsoftware.akces.annotations.QueryModelInfo;
import org.springframework.boot.autoconfigure.condition.ConditionOutcome;
import org.springframework.boot.autoconfigure.condition.SpringBootCondition;
import org.springframework.context.annotation.ConditionContext;
import org.springframework.core.type.AnnotatedTypeMetadata;

import java.util.Optional;

public class QueryModelImplementationPresentCondition extends SpringBootCondition {
    @Override
    public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) {
        boolean match = Optional.ofNullable(context.getBeanFactory())
                .map(beanFactory -> beanFactory.getBeanNamesForAnnotation(QueryModelInfo.class))
                .filter(beanNames -> beanNames.length > 0).isPresent();
        return match ? ConditionOutcome.match() : ConditionOutcome.noMatch("No QueryModel beans found");
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelNotFoundException.java
================
package org.elasticsoftware.akces.query.models;

public class QueryModelNotFoundException extends QueryModelExecutionException {
    public QueryModelNotFoundException(Class<?> modelClass) {
        super("Query model not found", modelClass);
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelRuntime.java
================
package org.elasticsoftware.akces.query.models;

import org.elasticsoftware.akces.protocol.DomainEventRecord;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akces.query.QueryModelState;

import java.io.IOException;
import java.util.List;

public interface QueryModelRuntime<S extends QueryModelState> {
    String getName();

    String getIndexName();

    Class<? extends QueryModel<S>> getQueryModelClass();

    S apply(List<DomainEventRecord> eventRecords, S currentState) throws IOException;

    void validateDomainEventSchemas();

    boolean shouldHandlePIIData();
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModelRuntimeFactory.java
================
package org.elasticsoftware.akces.query.models;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.annotations.QueryModelInfo;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akces.query.QueryModelEventHandlerFunction;
import org.elasticsoftware.akces.query.QueryModelState;
import org.elasticsoftware.akces.query.QueryModelStateType;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.springframework.beans.BeansException;
import org.springframework.beans.factory.FactoryBean;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;

public class QueryModelRuntimeFactory<S extends QueryModelState> implements FactoryBean<QueryModelRuntime<S>>, ApplicationContextAware {
    private ApplicationContext applicationContext;
    private final ObjectMapper objectMapper;
    private final KafkaSchemaRegistry schemaRegistry;
    private final QueryModel<S> queryModel;

    public QueryModelRuntimeFactory(ObjectMapper objectMapper,
                                    KafkaSchemaRegistry schemaRegistry,
                                    QueryModel<S> queryModel) {
        this.objectMapper = objectMapper;
        this.schemaRegistry = schemaRegistry;
        this.queryModel = queryModel;
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        this.applicationContext = applicationContext;
    }

    @Override
    public QueryModelRuntime<S> getObject() throws Exception {
        return createRuntime(queryModel);
    }

    @Override
    public Class<?> getObjectType() {
        return QueryModelRuntime.class;
    }

    private QueryModelRuntime<S> createRuntime(QueryModel<S> queryModel) {
        KafkaQueryModelRuntime.Builder<S> runtimeBuilder = new KafkaQueryModelRuntime.Builder<>();

        QueryModelInfo queryModelInfo = queryModel.getClass().getAnnotation(QueryModelInfo.class);

        if (queryModelInfo != null) {
            runtimeBuilder.setStateType(new QueryModelStateType<S>(
                    queryModelInfo.value(),
                    queryModelInfo.version(),
                    queryModel.getStateClass(),
                    queryModelInfo.indexName()
            ));
        } else {
            throw new IllegalStateException("Class implementing Aggregate must be annotated with @AggregateInfo");
        }
        runtimeBuilder.setQueryModelClass((Class<? extends QueryModel<S>>) queryModel.getClass());
        runtimeBuilder.setObjectMapper(objectMapper);

        applicationContext.getBeansOfType(QueryModelEventHandlerFunction.class).values().stream()
                .filter(adapter -> adapter.getQueryModel().equals(queryModel))
                .forEach(adapter -> {
                    DomainEventType<?> type = adapter.getEventType();
                    if (adapter.isCreate()) {
                        runtimeBuilder.setCreateHandler(adapter);
                        runtimeBuilder.addDomainEvent(type);
                    } else {
                        runtimeBuilder.addQueryModelEventHandler(type, adapter);
                        runtimeBuilder.addDomainEvent(type);
                    }
                });

        return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();
    }
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/models/QueryModels.java
================
package org.elasticsoftware.akces.query.models;

import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akces.query.QueryModelState;

import java.util.concurrent.CompletionStage;

public interface QueryModels {
    <S extends QueryModelState> CompletionStage<S> getHydratedState(Class<? extends QueryModel<S>> modelClass, String id);
}

================
File: query-support/src/main/java/org/elasticsoftware/akces/query/QueryServiceApplication.java
================
package org.elasticsoftware.akces.query;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.boot.context.properties.EnableConfigurationProperties;

import java.util.Set;

@SpringBootApplication(exclude = KafkaAutoConfiguration.class)
@EnableConfigurationProperties(KafkaProperties.class)
public class QueryServiceApplication {
    public static void main(String[] args) {
        SpringApplication application = new SpringApplication(QueryServiceApplication.class);
        if (args.length > 0) {

            application.setSources(Set.of(args));
        }
        application.run();
    }
}

================
File: query-support/src/main/resources/META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#

org.elasticsoftware.akces.query.models.AkcesQueryModelAutoConfiguration

================
File: query-support/src/main/resources/akces-querymodel.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor

================
File: query-support/src/test/java/org/elasticsoftware/akces/query/models/account/AccountQueryModel.java
================
package org.elasticsoftware.akces.query.models.account;

import org.elasticsoftware.akces.annotations.QueryModelEventHandler;
import org.elasticsoftware.akces.annotations.QueryModelInfo;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;


@QueryModelInfo(value = "AccountQueryModel", version = 1, indexName = "Users")
public class AccountQueryModel implements QueryModel<AccountQueryModelState> {
    @Override
    public String getName() {
        return "AccountQueryModel";
    }

    @Override
    public Class<AccountQueryModelState> getStateClass() {
        return AccountQueryModelState.class;
    }

    @Override
    public String getIndexName() {
        return "Users";
    }

    @QueryModelEventHandler(create = true)
    public AccountQueryModelState create(AccountCreatedEvent event, AccountQueryModelState isNull) {
        return new AccountQueryModelState(event.userId(), event.country(), event.firstName(), event.lastName(), event.email());
    }
}

================
File: query-support/src/test/java/org/elasticsoftware/akces/query/models/account/AccountQueryModelState.java
================
package org.elasticsoftware.akces.query.models.account;

import org.elasticsoftware.akces.annotations.QueryModelStateInfo;
import org.elasticsoftware.akces.query.QueryModelState;

@QueryModelStateInfo(type = "Account")
public record AccountQueryModelState(
        String accountId,
        String country,
        String firstName,
        String lastName,
        String email
) implements QueryModelState {
    @Override
    public String getIndexKey() {
        return accountId();
    }
}

================
File: query-support/src/test/java/org/elasticsoftware/akces/query/models/wallet/WalletQueryModel.java
================
package org.elasticsoftware.akces.query.models.wallet;

import org.elasticsoftware.akces.annotations.QueryModelEventHandler;
import org.elasticsoftware.akces.annotations.QueryModelInfo;
import org.elasticsoftware.akces.query.QueryModel;
import org.elasticsoftware.akcestest.aggregate.wallet.BalanceCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.WalletCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.WalletCreditedEvent;

import java.math.BigDecimal;
import java.util.ArrayList;
import java.util.List;

@QueryModelInfo(value = "WalletQueryModel", version = 1, indexName = "Users")
public class WalletQueryModel implements QueryModel<WalletQueryModelState> {
    @Override
    public String getName() {
        return "WalletQueryModel";
    }

    @Override
    public Class<WalletQueryModelState> getStateClass() {
        return WalletQueryModelState.class;
    }

    @Override
    public String getIndexName() {
        return "Users";
    }

    @QueryModelEventHandler(create = true)
    public WalletQueryModelState create(WalletCreatedEvent event, WalletQueryModelState isNull) {
        return new WalletQueryModelState(event.id(), List.of());
    }

    @QueryModelEventHandler(create = false)
    public WalletQueryModelState createBalance(BalanceCreatedEvent event, WalletQueryModelState currentState) {
        WalletQueryModelState.Balance balance = new WalletQueryModelState.Balance(event.currency(), BigDecimal.ZERO);
        List<WalletQueryModelState.Balance> balances = new ArrayList<>(currentState.balances());
        balances.add(balance);
        return new WalletQueryModelState(currentState.walletId(), balances);
    }

    @QueryModelEventHandler(create = false)
    public WalletQueryModelState creditWallet(WalletCreditedEvent event, WalletQueryModelState currentState) {
        return new WalletQueryModelState(
                currentState.walletId(),
                currentState.balances().stream().map(balance -> {
                    if (balance.currency().equals(event.currency())) {
                        return new WalletQueryModelState.Balance(
                                balance.currency(),
                                balance.amount().add(event.amount()),
                                balance.reservedAmount()
                        );
                    } else {
                        return balance;
                    }
                }).toList());
    }
}

================
File: query-support/src/test/java/org/elasticsoftware/akces/query/models/wallet/WalletQueryModelState.java
================
package org.elasticsoftware.akces.query.models.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.QueryModelStateInfo;
import org.elasticsoftware.akces.query.QueryModelState;

import java.math.BigDecimal;
import java.util.List;

@QueryModelStateInfo(type = "Wallet")
public record WalletQueryModelState(String walletId, List<Balance> balances) implements QueryModelState {
    @Override
    public String getIndexKey() {
        return walletId();
    }

    public record Balance(String currency, BigDecimal amount, BigDecimal reservedAmount) {
        public Balance(@NotNull String currency) {
            this(currency, BigDecimal.ZERO, BigDecimal.ZERO);
        }

        public Balance(@NotNull String currency, @NotNull BigDecimal amount) {
            this(currency, amount, BigDecimal.ZERO);
        }

        public BigDecimal getAvailableAmount() {
            return amount.subtract(reservedAmount);
        }
    }
}

================
File: query-support/src/test/java/org/elasticsoftware/akces/query/models/QueryModelRuntimeTests.java
================
package org.elasticsoftware.akces.query.models;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.github.victools.jsonschema.generator.*;
import com.github.victools.jsonschema.module.jackson.JacksonModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationOption;
import io.confluent.kafka.schemaregistry.ParsedSchema;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import jakarta.inject.Inject;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.elasticsoftware.akces.AggregateServiceApplication;
import org.elasticsoftware.akces.AkcesAggregateController;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.client.AkcesClientController;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.query.QueryModelEventHandlerFunction;
import org.elasticsoftware.akces.query.models.account.AccountQueryModel;
import org.elasticsoftware.akces.query.models.account.AccountQueryModelState;
import org.elasticsoftware.akces.query.models.wallet.WalletQueryModel;
import org.elasticsoftware.akces.query.models.wallet.WalletQueryModelState;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.account.CreateAccountCommand;
import org.elasticsoftware.akcestest.aggregate.wallet.*;
import org.junit.jupiter.api.*;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextException;
import org.springframework.context.ApplicationContextInitializer;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdmin;
import org.springframework.test.annotation.DirtiesContext;
import org.springframework.test.context.ContextConfiguration;
import org.springframework.test.context.support.TestPropertySourceUtils;
import org.testcontainers.containers.GenericContainer;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.Network;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import java.io.File;
import java.io.IOException;
import java.math.BigDecimal;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Comparator;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import static org.assertj.core.api.Assertions.assertThat;
import static org.junit.jupiter.api.Assertions.*;

@Testcontainers
@SpringBootTest(
        classes = AggregateServiceApplication.class,
        args = "org.elasticsoftware.akces.query.models.QueryModelTestConfiguration",
        useMainMethod = SpringBootTest.UseMainMethod.ALWAYS)
@ContextConfiguration(initializers = QueryModelRuntimeTests.ContextInitializer.class)
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
@DirtiesContext
public class QueryModelRuntimeTests {
    private static final String CONFLUENT_PLATFORM_VERSION = "7.8.1";

    private static final Network network = Network.newNetwork();

    @Container
    private static final KafkaContainer kafka =
            new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
                    .withKraft()
                    .withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
                    .withNetwork(network)
                    .withNetworkAliases("kafka");

    @Container
    private static final GenericContainer<?> schemaRegistry =
            new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
                    .withNetwork(network)
                    .withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
                    .withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
                    .withExposedPorts(8081)
                    .withNetworkAliases("schema-registry")
                    .dependsOn(kafka);

    @Inject
    @Qualifier("akcesQueryModelKafkaAdmin")
    KafkaAdmin adminClient;

    @Inject
    @Qualifier("akcesQueryModelSchemaRegistryClient")
    SchemaRegistryClient schemaRegistryClient;

    @Inject
    @Qualifier("akcesQueryModelConsumerFactory")
    ConsumerFactory<String, ProtocolRecord> consumerFactory;

    @Inject
    @Qualifier("WalletAkcesController")
    AkcesAggregateController walletController;

    @Inject
    @Qualifier("AccountAkcesController")
    AkcesAggregateController accountController;

    @Inject
    @Qualifier("OrderProcessManagerAkcesController")
    AkcesAggregateController orderProcessManagerController;

    @Inject
    AkcesClientController akcesClientController;

    @Inject
    AkcesQueryModelController akcesQueryModelController;

    @Inject
    ObjectMapper objectMapper;

    @Inject
    ApplicationContext applicationContext;

    public static void prepareKafka(String bootstrapServers) {
        KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
        kafkaAdmin.createOrModifyTopics(
                createCompactedTopic("Akces-Control", 3),
                createTopic("Akces-CommandResponses", 3, 604800000L),
                createCompactedTopic("Akces-GDPRKeys", 3),
                createTopic("Wallet-Commands", 3),
                createTopic("Wallet-DomainEvents", 3),
                createTopic("Account-Commands", 3),
                createTopic("Account-DomainEvents", 3),
                createTopic("OrderProcessManager-Commands", 3),
                createTopic("OrderProcessManager-DomainEvents", 3),
                createCompactedTopic("Wallet-AggregateState", 3),
                createCompactedTopic("Account-AggregateState", 3),
                createCompactedTopic("OrderProcessManager-AggregateState", 3));
    }

    public static <D extends DomainEvent> void prepareDomainEventSchemas(String url, List<Class<?>> domainEventClasses) {
        SchemaRegistryClient src = new CachedSchemaRegistryClient(url, 100);
        Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
        objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
        objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
        try {
            for (Class<?> domainEventClass : domainEventClasses) {
                DomainEventInfo info = domainEventClass.getAnnotation(DomainEventInfo.class);
                src.register("domainevents." + info.type(),
                        new JsonSchema(jsonSchemaGenerator.generateSchema(domainEventClass), List.of(), Map.of(), info.version()),
                        info.version(),
                        -1);
            }
        } catch (IOException | RestClientException e) {
            throw new ApplicationContextException("Problem populating SchemaRegistry", e);
        }
    }

    private static NewTopic createTopic(String name, int numPartitions) {
        return createTopic(name, numPartitions, -1L);
    }

    private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
        NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
        return topic.configs(Map.of(
                "cleanup.policy", "delete",
                "max.message.bytes", "20971520",
                "retention.ms", Long.toString(retentionMs),
                "segment.ms", "604800000"));
    }

    private static NewTopic createCompactedTopic(String name, int numPartitions) {
        NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
        return topic.configs(Map.of(
                "cleanup.policy", "compact",
                "max.message.bytes", "20971520",
                "retention.ms", "-1",
                "segment.ms", "604800000",
                "min.cleanable.dirty.ratio", "0.1",
                "delete.retention.ms", "604800000",
                "compression.type", "lz4"));
    }

    @BeforeAll
    @AfterAll
    public static void cleanUp() throws IOException {

        if (Files.exists(Paths.get("/tmp/akces"))) {

            Files.walk(Paths.get("/tmp/akces"))
                    .sorted(Comparator.reverseOrder())
                    .map(Path::toFile)
                    .forEach(File::delete);
        }
    }

    @Test
    @Order(1)
    void testContextLoads() {
        assertNotNull(adminClient);
        assertNotNull(schemaRegistryClient);
        assertNotNull(consumerFactory);
        assertNotNull(objectMapper);
        assertNotNull(walletController);
        assertNotNull(accountController);
        assertNotNull(orderProcessManagerController);
        assertNotNull(akcesClientController);
        assertNotNull(akcesQueryModelController);

        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }
    }

    @Test
    @Order(2)
    public void testFindBeans() {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }
        assertNotNull(applicationContext);
        assertEquals(4, applicationContext.getBeansOfType(QueryModelEventHandlerFunction.class).size());
        assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_create_WalletCreated_1"));
        assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_createBalance_BalanceCreated_1"));
        assertNotNull(applicationContext.getBean("AccountQueryModel_qmeh_create_AccountCreated_1"));
        assertNotNull(applicationContext.getBean("WalletQueryModel_qmeh_creditWallet_WalletCredited_1"));
        assertEquals(2, applicationContext.getBeansOfType(QueryModelRuntimeFactory.class).size());
        assertNotNull(applicationContext.getBean("WalletQueryModelQueryModelRuntime"));
        assertNotNull(applicationContext.getBean("AccountQueryModelQueryModelRuntime"));
        assertEquals(2, applicationContext.getBeansOfType(QueryModelRuntime.class).size());
    }

    @Test
    @Order(3)
    public void testWithUnknownId() throws InterruptedException, TimeoutException {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }

        CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(
                WalletQueryModel.class, "unknown-id").toCompletableFuture();
        assertNotNull(stateFuture);
        ExecutionException exception = assertThrows(ExecutionException.class, stateFuture::get);
        assertInstanceOf(QueryModelIdNotFoundException.class, exception.getCause());
    }

    @Test
    @Order(4)
    public void testWithUnknownIdAgainAndInsureNotCreated() throws InterruptedException, TimeoutException {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }

        CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, "unknown-id")
                .toCompletableFuture();
        assertNotNull(stateFuture);
        ExecutionException exception = assertThrows(ExecutionException.class, stateFuture::get);
        assertInstanceOf(QueryModelIdNotFoundException.class, exception.getCause());
    }

    @Test
    @Order(5)
    public void testRegisteredSchemas() throws RestClientException, IOException {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }


        List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas("domainevents.WalletCreated", false, false);
        assertFalse(registeredSchemas.isEmpty());
        assertEquals(1, schemaRegistryClient.getVersion("domainevents.WalletCreated", registeredSchemas.getFirst()));

        registeredSchemas = schemaRegistryClient.getSchemas("domainevents.AccountCreated", false, false);
        assertFalse(registeredSchemas.isEmpty());
        assertEquals(1, schemaRegistryClient.getVersion("domainevents.AccountCreated", registeredSchemas.getFirst()));
    }

    @Test
    @Order(6)
    public void testCreateAndQueryWalletQueryModel() throws ExecutionException, InterruptedException, TimeoutException {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "a28d41c4-9f9c-4708-b142-6b83768ee8f3";
        List<DomainEvent> result;

        CreateWalletCommand createWalletCommand = new CreateWalletCommand(userId, "BTC");
        result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createWalletCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));
        Assertions.assertNotNull(result);
        Assertions.assertEquals(2, result.size());
        assertInstanceOf(WalletCreatedEvent.class, result.getFirst());
        assertInstanceOf(BalanceCreatedEvent.class, result.getLast());

        CompletableFuture<WalletQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                .toCompletableFuture();

        assertNotNull(stateFuture);
        WalletQueryModelState state = assertDoesNotThrow(() -> stateFuture.get(10, TimeUnit.SECONDS));
        assertNotNull(state);
    }

    @Test
    @Order(7)
    public void testCreateAndQueryWalletQueryModelWithMultipleConcurrentRequests() throws ExecutionException, InterruptedException, TimeoutException {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }

        List<String> userIds = List.of(
                "0dfc7c15-2e97-43c0-81f3-52f19e177909",
                "69084006-819f-47bf-8dad-026915e70eef",
                "1deccc0d-75ce-4fdd-900d-779a088b5449",
                "1c28cd3c-9d67-4756-aa40-8d8bb60870a0",
                "21d39df6-29d2-465f-bef1-e32206a95519",
                "6873a740-a15c-4164-ad8d-101006deea22",
                "fae4298d-041a-4ad3-bef0-1903361c1408",
                "9eb4ebd4-3897-460d-a756-0615e6eb8c7e",
                "278254d9-47ee-4014-b275-1119d74d4af2",
                "fbfd596f-0a84-413c-9f98-206bc6f0be0a"
        );

        List<CompletableFuture<List<DomainEvent>>> futures = userIds.stream()
                .map(userId -> akcesClientController.send("TEST_TENANT", new CreateWalletCommand(userId, "BTC"))
                        .toCompletableFuture())
                .toList();

        CompletableFuture<Void> allOf = CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
        allOf.get(10, TimeUnit.SECONDS);

        for (CompletableFuture<List<DomainEvent>> future : futures) {
            List<DomainEvent> result = future.get();
            assertNotNull(result);
            assertEquals(2, result.size());
            assertInstanceOf(WalletCreatedEvent.class, result.get(0));
            assertInstanceOf(BalanceCreatedEvent.class, result.get(1));
        }

        List<CompletableFuture<WalletQueryModelState>> stateFutures = userIds.stream()
                .map(userId -> akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                        .toCompletableFuture())
                .toList();

        CompletableFuture<Void> allOfStates = CompletableFuture.allOf(stateFutures.toArray(new CompletableFuture[0]));
        allOfStates.get(10, TimeUnit.SECONDS);

        for (CompletableFuture<WalletQueryModelState> stateFuture : stateFutures) {
            WalletQueryModelState state = stateFuture.get();
            assertNotNull(state);
        }
    }

    @Test @Order(8)
    public void testAccountQueryModel() {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "7ee48409-f381-4bb1-8115-0a7bbfd30e9c";
        List<DomainEvent> result;

        CreateAccountCommand createAccountCommand = new CreateAccountCommand(userId, "US", "John", "Doe", "john.doe@example.com");
        result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createAccountCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(AccountCreatedEvent.class, result.getFirst());

        CompletableFuture<AccountQueryModelState> stateFuture = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
                .toCompletableFuture();

        assertNotNull(stateFuture);
        AccountQueryModelState state = assertDoesNotThrow(() -> stateFuture.get(10, TimeUnit.SECONDS));
        assertNotNull(state);
        assertThat(state.country()).isEqualTo("US");
        assertThat(state.firstName()).isEqualTo("John");
        assertThat(state.lastName()).isEqualTo("Doe");
        assertThat(state.email()).isEqualTo("john.doe@example.com");
    }

    @Test @Order(8)
    public void testAccountQueryModelUsingCache() {
        while (!walletController.isRunning() ||
                !accountController.isRunning() ||
                !orderProcessManagerController.isRunning() ||
                !akcesClientController.isRunning() ||
                !akcesQueryModelController.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "4117b11f-3dde-4b71-b80c-fa20a12d9add";
        List<DomainEvent> result;

        CreateAccountCommand createAccountCommand = new CreateAccountCommand(userId, "US", "John", "Doe", "john.doe@example.com");
        result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", createAccountCommand).toCompletableFuture().get(10, TimeUnit.SECONDS));

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(AccountCreatedEvent.class, result.getFirst());

        CompletableFuture<AccountQueryModelState> stateFuture1 = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
                .toCompletableFuture();

        assertNotNull(stateFuture1);
        AccountQueryModelState state1 = assertDoesNotThrow(() -> stateFuture1.get(10, TimeUnit.SECONDS));
        assertNotNull(state1);
        assertThat(state1.country()).isEqualTo("US");
        assertThat(state1.firstName()).isEqualTo("John");
        assertThat(state1.lastName()).isEqualTo("Doe");
        assertThat(state1.email()).isEqualTo("john.doe@example.com");

        CompletableFuture<AccountQueryModelState> stateFuture2 = akcesQueryModelController.getHydratedState(AccountQueryModel.class, userId)
                .toCompletableFuture();

        assertNotNull(stateFuture2);
        AccountQueryModelState state2 = assertDoesNotThrow(() -> stateFuture2.get(10, TimeUnit.SECONDS));
        assertNotNull(state2);
        assertThat(state2.country()).isEqualTo("US");
        assertThat(state2.firstName()).isEqualTo("John");
        assertThat(state2.lastName()).isEqualTo("Doe");
        assertThat(state2.email()).isEqualTo("john.doe@example.com");


        CompletableFuture<WalletQueryModelState> walletStateFuture = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                .toCompletableFuture();
        assertNotNull(walletStateFuture);
        WalletQueryModelState walletState = assertDoesNotThrow(() -> walletStateFuture.get(10, TimeUnit.SECONDS));
        assertNotNull(walletState);
        assertEquals(1, walletState.balances().size());
        assertEquals("EUR", walletState.balances().getFirst().currency());


        CompletableFuture<WalletQueryModelState> walletStateFuture2 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                .toCompletableFuture();
        assertNotNull(walletStateFuture2);
        WalletQueryModelState walletState2 = assertDoesNotThrow(() -> walletStateFuture2.get(10, TimeUnit.SECONDS));
        assertNotNull(walletState2);
        assertEquals(1, walletState2.balances().size());
        assertEquals("EUR", walletState2.balances().getFirst().currency());

        result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreateBalanceCommand(userId, "BTC"))
                .toCompletableFuture().get(10, TimeUnit.SECONDS));

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(BalanceCreatedEvent.class, result.getFirst());
        assertEquals("BTC", ((BalanceCreatedEvent) result.getFirst()).currency());

        CompletableFuture<WalletQueryModelState> walletStateFuture3 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                .toCompletableFuture();
        assertNotNull(walletStateFuture3);
        WalletQueryModelState walletState3 = assertDoesNotThrow(() -> walletStateFuture3.get(10, TimeUnit.SECONDS));
        assertNotNull(walletState3);
        assertEquals(2, walletState3.balances().size());
        assertEquals("EUR", walletState3.balances().getFirst().currency());
        assertEquals("BTC", walletState3.balances().getLast().currency());


        result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreateBalanceCommand(userId, "ETH"))
                .toCompletableFuture().get(10, TimeUnit.SECONDS));

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(BalanceCreatedEvent.class, result.get(0));

        CompletableFuture<WalletQueryModelState> walletStateFuture4 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                .toCompletableFuture();
        assertNotNull(walletStateFuture4);
        WalletQueryModelState walletState4 = assertDoesNotThrow(() -> walletStateFuture4.get(10, TimeUnit.SECONDS));
        assertNotNull(walletState4);
        assertEquals(3, walletState4.balances().size());
        assertEquals("EUR", walletState4.balances().get(0).currency());
        assertEquals("BTC", walletState4.balances().get(1).currency());
        assertEquals("ETH", walletState4.balances().get(2).currency());


        result = assertDoesNotThrow(() -> akcesClientController.send("TEST_TENANT", new CreditWalletCommand(userId, "EUR", new BigDecimal("1000.00")))
                .toCompletableFuture().get(10, TimeUnit.SECONDS));

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(WalletCreditedEvent.class, result.get(0));
        assertEquals("EUR", ((WalletCreditedEvent) result.get(0)).currency());
        assertEquals(new BigDecimal("1000.00"), ((WalletCreditedEvent) result.get(0)).amount());

        CompletableFuture<WalletQueryModelState> walletStateFuture5 = akcesQueryModelController.getHydratedState(WalletQueryModel.class, userId)
                .toCompletableFuture();
        assertNotNull(walletStateFuture5);
        WalletQueryModelState walletState5 = assertDoesNotThrow(() -> walletStateFuture5.get(10, TimeUnit.SECONDS));
        assertNotNull(walletState5);
        assertEquals(3, walletState5.balances().size());
        assertEquals("EUR", walletState5.balances().getFirst().currency());
        assertEquals(new BigDecimal("1000.00"), walletState5.balances().getFirst().amount());


    }

    public static class ContextInitializer
            implements ApplicationContextInitializer<ConfigurableApplicationContext> {

        @Override
        public void initialize(ConfigurableApplicationContext applicationContext) {

            prepareKafka(kafka.getBootstrapServers());
            prepareDomainEventSchemas("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081),
                    List.of(
                            WalletCreatedEvent.class,
                            WalletCreditedEvent.class,
                            BalanceCreatedEvent.class,
                            AccountCreatedEvent.class
                    ));
            try {
                prepareAggregateServiceRecords(kafka.getBootstrapServers());
            } catch (IOException e) {
                throw new RuntimeException(e);
            }
            TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
                    applicationContext,
                    "akces.rocksdb.baseDir=/tmp/akces",
                    "spring.kafka.enabled=true",
                    "spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
                    "akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)
            );
        }
    }

    public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
        Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
        builder.modulesToInstall(new AkcesGDPRModule());
        builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        ObjectMapper objectMapper = builder.build();
        AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
        Map<String, Object> controlProducerProps = Map.of(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers,
                ProducerConfig.ACKS_CONFIG, "all",
                ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true",
                ProducerConfig.LINGER_MS_CONFIG, "0",
                ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1",
                ProducerConfig.RETRIES_CONFIG, "2147483647",
                ProducerConfig.RETRY_BACKOFF_MS_CONFIG, "0",
                ProducerConfig.TRANSACTIONAL_ID_CONFIG, "Test-AkcesControllerProducer",
                ProducerConfig.CLIENT_ID_CONFIG, "Test-AkcesControllerProducer");
        try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
            controlProducer.initTransactions();
            AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
            AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
            AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
            controlProducer.beginTransaction();
            for (int partition = 0; partition < 3; partition++) {
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
            }
            controlProducer.commitTransaction();
        }
    }
}

================
File: query-support/src/test/java/org/elasticsoftware/akces/query/models/QueryModelTestConfiguration.java
================
package org.elasticsoftware.akces.query.models;

import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;

@Configuration
@ComponentScan(basePackages = {
        "org.elasticsoftware.akcestest.aggregate",
        "org.elasticsoftware.akces.query.models.wallet",
        "org.elasticsoftware.akces.query.models.account"
})
public class QueryModelTestConfiguration {
}

================
File: query-support/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.query.models" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.gdpr" level="info" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: query-support/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.7.21-SNAPSHOT</version>
        <relativePath>../pom.xml</relativePath>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Query Support</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-query-support</artifactId>
    <packaging>jar</packaging>

    <properties>

    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>

        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-runtime</artifactId>
            <classifier>tests</classifier>
            <type>test-jar</type>
            <version>${project.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-client</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-runtime</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
    </dependencies>

</project>

================
File: runtime/src/main/java/org/elasticsoftware/akces/aggregate/AggregateRuntime.java
================
package org.elasticsoftware.akces.aggregate;

import org.apache.kafka.common.errors.SerializationException;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.CommandRecord;
import org.elasticsoftware.akces.protocol.DomainEventRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.schemas.SchemaException;

import java.io.IOException;
import java.util.Collection;
import java.util.function.BiConsumer;
import java.util.function.Consumer;
import java.util.function.Supplier;

public interface AggregateRuntime {

    String getName();

    Class<? extends Aggregate> getAggregateClass();

    void handleCommandRecord(CommandRecord commandRecord,
                             Consumer<ProtocolRecord> protocolRecordConsumer,
                             BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                             Supplier<AggregateStateRecord> stateRecordSupplier) throws IOException;

    void handleExternalDomainEventRecord(DomainEventRecord eventRecord,
                                         Consumer<ProtocolRecord> protocolRecordConsumer,
                                         BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                                         Supplier<AggregateStateRecord> stateRecordSupplier) throws IOException;

    Collection<DomainEventType<?>> getAllDomainEventTypes();

    Collection<DomainEventType<?>> getProducedDomainEventTypes();

    Collection<DomainEventType<?>> getExternalDomainEventTypes();

    Collection<CommandType<?>> getAllCommandTypes();

    Collection<CommandType<?>> getLocalCommandTypes();

    Collection<CommandType<?>> getExternalCommandTypes();

    CommandType<?> getLocalCommandType(String type, int version);

    void registerAndValidate(DomainEventType<?> domainEventType, boolean forceRegisterOnIncompatible) throws SchemaException;

    default void registerAndValidate(DomainEventType<?> domainEventType) throws SchemaException {
        registerAndValidate(domainEventType, false);
    }

    void registerAndValidate(CommandType<?> commandType,  boolean forceRegisterOnIncompatible) throws SchemaException;

    default void registerAndValidate(CommandType<?> commandType) throws SchemaException {
        registerAndValidate(commandType, false);
    }

    Command materialize(CommandType<?> commandType, CommandRecord commandRecord) throws IOException;

    byte[] serialize(Command command) throws SerializationException;

    boolean shouldGenerateGPRKey(CommandRecord commandRecord);

    boolean shouldGenerateGPRKey(DomainEventRecord eventRecord);

    boolean shouldHandlePIIData();
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/aggregate/AggregateRuntimeBase.java
================
package org.elasticsoftware.akces.aggregate;

import jakarta.annotation.Nullable;
import org.apache.kafka.common.errors.SerializationException;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.errors.AggregateAlreadyExistsErrorEvent;
import org.elasticsoftware.akces.errors.CommandExecutionErrorEvent;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.events.ErrorEvent;
import org.elasticsoftware.akces.kafka.KafkaAggregateRuntime;
import org.elasticsoftware.akces.protocol.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.*;
import java.util.function.BiConsumer;
import java.util.function.Consumer;
import java.util.function.Supplier;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import static java.util.Collections.emptyList;

public abstract class AggregateRuntimeBase implements AggregateRuntime {
    protected static final Logger log = LoggerFactory.getLogger(KafkaAggregateRuntime.class);
    private final AggregateStateType<?> type;
    private final Class<? extends Aggregate> aggregateClass;
    private final CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler;
    private final EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler;
    private final EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler;
    private final Map<Class<?>, DomainEventType<?>> domainEvents;
    private final Map<String, List<CommandType<?>>> commandTypes;
    private final Map<CommandType<?>, CommandHandlerFunction<AggregateState, Command, DomainEvent>> commandHandlers;
    private final Map<DomainEventType<?>, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent>> eventHandlers;
    private final Map<DomainEventType<?>, EventSourcingHandlerFunction<AggregateState, DomainEvent>> eventSourcingHandlers;
    private final boolean generateGDPRKeyOnCreate;
    private final boolean shouldHandlePIIData;

    public AggregateRuntimeBase(AggregateStateType<?> type,
                                Class<? extends Aggregate> aggregateClass,
                                CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler,
                                EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler,
                                EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler,
                                Map<Class<?>, DomainEventType<?>> domainEvents,
                                Map<String, List<CommandType<?>>> commandTypes,
                                Map<CommandType<?>, CommandHandlerFunction<AggregateState, Command, DomainEvent>> commandHandlers,
                                Map<DomainEventType<?>, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent>> eventHandlers,
                                Map<DomainEventType<?>, EventSourcingHandlerFunction<AggregateState, DomainEvent>> eventSourcingHandlers,
                                boolean generateGDPRKeyOnCreate,
                                boolean shouldHandlePIIData) {
        this.type = type;
        this.aggregateClass = aggregateClass;
        this.commandCreateHandler = commandCreateHandler;
        this.eventCreateHandler = eventCreateHandler;
        this.createStateHandler = createStateHandler;
        this.domainEvents = domainEvents;
        this.commandTypes = commandTypes;
        this.commandHandlers = commandHandlers;
        this.eventHandlers = eventHandlers;
        this.eventSourcingHandlers = eventSourcingHandlers;
        this.generateGDPRKeyOnCreate = generateGDPRKeyOnCreate;
        this.shouldHandlePIIData = shouldHandlePIIData;
    }

    @Override
    public String getName() {
        return type.typeName();
    }

    @Override
    public Class<? extends Aggregate> getAggregateClass() {
        return aggregateClass;
    }

    @Override
    public void handleCommandRecord(CommandRecord commandRecord,
                                    Consumer<ProtocolRecord> protocolRecordConsumer,
                                    BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                                    Supplier<AggregateStateRecord> stateRecordSupplier) throws IOException {

        CommandType<?> commandType = getCommandType(commandRecord);
        try {
            if (commandType.create()) {

                if (stateRecordSupplier.get() != null) {
                    log.warn("Command {} wants to create a {} Aggregate with id {}, but the state already exists. Generating a AggregateAlreadyExistsError",
                            commandRecord.name(),
                            getName(),
                            commandRecord.aggregateId());
                    aggregateAlreadyExists(commandRecord, protocolRecordConsumer);
                } else {
                    handleCreateCommand(commandType, commandRecord, protocolRecordConsumer, domainEventIndexer);
                }
            } else {

                if (commandHandlers.containsKey(commandType)) {
                    handleCommand(commandType, commandRecord, protocolRecordConsumer, domainEventIndexer, stateRecordSupplier);
                } else {
                    commandExecutionError(commandRecord, protocolRecordConsumer, "No handler found for command " + commandRecord.name());
                }
            }
        } catch (Throwable t) {

            log.error("Exception while handling command, sending CommandExecutionError", t);
            commandExecutionError(commandRecord, protocolRecordConsumer, t);
        }
    }

    private void aggregateAlreadyExists(ProtocolRecord commandOrDomainEventRecord,
                                        Consumer<ProtocolRecord> protocolRecordConsumer) {
        AggregateAlreadyExistsErrorEvent errorEvent = new AggregateAlreadyExistsErrorEvent(commandOrDomainEventRecord.aggregateId(), this.getName());
        DomainEventType<?> type = getDomainEventType(AggregateAlreadyExistsErrorEvent.class);
        DomainEventRecord eventRecord = new DomainEventRecord(
                commandOrDomainEventRecord.tenantId(),
                type.typeName(),
                type.version(),
                serialize(errorEvent),
                getEncoding(type),
                errorEvent.getAggregateId(),
                commandOrDomainEventRecord.correlationId(),
                -1L);
        protocolRecordConsumer.accept(eventRecord);
    }

    private void commandExecutionError(CommandRecord commandRecord,
                                       Consumer<ProtocolRecord> protocolRecordConsumer,
                                       Throwable exception) {
        commandExecutionError(commandRecord, protocolRecordConsumer, exception.getMessage());
    }

    private void commandExecutionError(CommandRecord commandRecord,
                                       Consumer<ProtocolRecord> protocolRecordConsumer,
                                       String errorDescription) {
        CommandExecutionErrorEvent errorEvent = new CommandExecutionErrorEvent(
                commandRecord.aggregateId(),
                this.getName(),
                commandRecord.name(),
                errorDescription);
        DomainEventType<?> type = getDomainEventType(AggregateAlreadyExistsErrorEvent.class);
        DomainEventRecord eventRecord = new DomainEventRecord(
                commandRecord.tenantId(),
                type.typeName(),
                type.version(),
                serialize(errorEvent),
                getEncoding(type),
                errorEvent.getAggregateId(),
                commandRecord.correlationId(),
                -1L);
        protocolRecordConsumer.accept(eventRecord);
    }

    private CommandType<?> getCommandType(CommandRecord commandRecord) {
        CommandType<?> commandType = commandTypes.getOrDefault(commandRecord.name(), emptyList()).stream()
                .filter(ct -> ct.version() == commandRecord.version())
                .findFirst().orElseThrow(RuntimeException::new);
        return commandType;
    }

    private void indexDomainEventIfRequired(DomainEventRecord domainEventRecord,
                                            AggregateState state,
                                            BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                                            boolean createIndex) {
        if (type.indexed()) {
            domainEventIndexer.accept(domainEventRecord, new IndexParams(type.indexName(), state.getIndexKey(), createIndex));
        }
    }

    private void handleCreateCommand(CommandType<?> commandType,
                                     CommandRecord commandRecord,
                                     Consumer<ProtocolRecord> protocolRecordConsumer,
                                     BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer) throws IOException {

        Command command = materialize(commandType, commandRecord);

        Stream<DomainEvent> domainEvents = commandCreateHandler.apply(command, null);

        Iterator<DomainEvent> itr = domainEvents.iterator();
        DomainEvent domainEvent = itr.next();

        AggregateState state = createStateHandler.apply(domainEvent, null);

        AggregateStateRecord stateRecord = new AggregateStateRecord(
                commandRecord.tenantId(),
                type.typeName(),
                type.version(),
                serialize(state),
                getEncoding(type),
                state.getAggregateId(),
                commandRecord.correlationId(),
                1L);
        protocolRecordConsumer.accept(stateRecord);

        DomainEventType<?> type = getDomainEventType(domainEvent.getClass());
        DomainEventRecord eventRecord = new DomainEventRecord(
                commandRecord.tenantId(),
                type.typeName(),
                type.version(),
                serialize(domainEvent),
                getEncoding(type),
                domainEvent.getAggregateId(),
                commandRecord.correlationId(),
                stateRecord.generation());
        protocolRecordConsumer.accept(eventRecord);
        indexDomainEventIfRequired(eventRecord, state, domainEventIndexer, true);

        AggregateStateRecord currentStateRecord = stateRecord;
        while (itr.hasNext()) {
            DomainEvent nextDomainEvent = itr.next();
            currentStateRecord = processDomainEvent(
                    commandRecord.correlationId(),
                    protocolRecordConsumer,
                    domainEventIndexer,
                    currentStateRecord,
                    nextDomainEvent);
        }
    }

    private void handleCommand(CommandType<?> commandType,
                               CommandRecord commandRecord,
                               Consumer<ProtocolRecord> protocolRecordConsumer,
                               BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                               Supplier<AggregateStateRecord> stateRecordSupplier) throws IOException {

        Command command = materialize(commandType, commandRecord);
        AggregateStateRecord currentStateRecord = stateRecordSupplier.get();
        AggregateState currentState = materialize(currentStateRecord);
        Stream<DomainEvent> domainEvents = commandHandlers.get(commandType).apply(command, currentState);
        for (DomainEvent domainEvent : domainEvents.toList())
            currentStateRecord = processDomainEvent(commandRecord.correlationId(),
                    protocolRecordConsumer,
                    domainEventIndexer,
                    currentStateRecord,
                    domainEvent);
    }

    private void handleCreateEvent(DomainEventType<?> eventType,
                                   DomainEventRecord domainEventRecord,
                                   Consumer<ProtocolRecord> protocolRecordConsumer,
                                   BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer) throws IOException {

        DomainEvent externalEvent = materialize(eventType, domainEventRecord);

        Stream<DomainEvent> domainEvents = eventCreateHandler.apply(externalEvent, null);

        Iterator<DomainEvent> itr = domainEvents.iterator();
        DomainEvent domainEvent = itr.next();

        AggregateState state = createStateHandler.apply(domainEvent, null);

        AggregateStateRecord stateRecord = new AggregateStateRecord(
                domainEventRecord.tenantId(),
                type.typeName(),
                type.version(),
                serialize(state),
                getEncoding(type),
                state.getAggregateId(),
                domainEventRecord.correlationId(),
                1L);
        protocolRecordConsumer.accept(stateRecord);

        DomainEventType<?> type = getDomainEventType(domainEvent.getClass());
        DomainEventRecord eventRecord = new DomainEventRecord(
                domainEventRecord.tenantId(),
                type.typeName(),
                type.version(),
                serialize(domainEvent),
                getEncoding(type),
                domainEvent.getAggregateId(),
                domainEventRecord.correlationId(),
                stateRecord.generation());
        protocolRecordConsumer.accept(eventRecord);
        indexDomainEventIfRequired(eventRecord, state, domainEventIndexer, true);

        AggregateStateRecord currentStateRecord = stateRecord;
        while (itr.hasNext()) {
            DomainEvent nextDomainEvent = itr.next();
            currentStateRecord = processDomainEvent(
                    domainEventRecord.correlationId(),
                    protocolRecordConsumer,
                    domainEventIndexer,
                    currentStateRecord,
                    nextDomainEvent);
        }
    }

    private void handleEvent(DomainEventType<?> eventType,
                             DomainEventRecord domainEventRecord,
                             Consumer<ProtocolRecord> protocolRecordConsumer,
                             BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                             Supplier<AggregateStateRecord> stateRecordSupplier) throws IOException {

        DomainEvent externalEvent = materialize(eventType, domainEventRecord);
        AggregateStateRecord currentStateRecord = stateRecordSupplier.get();
        AggregateState currentState = materialize(currentStateRecord);
        Stream<DomainEvent> domainEvents = eventHandlers.get(eventType).apply(externalEvent, currentState);
        for (DomainEvent domainEvent : domainEvents.toList())
            currentStateRecord = processDomainEvent(
                    domainEventRecord.correlationId(),
                    protocolRecordConsumer,
                    domainEventIndexer,
                    currentStateRecord,
                    domainEvent);

    }

    private AggregateStateRecord processDomainEvent(String correlationId,
                                                    Consumer<ProtocolRecord> protocolRecordConsumer,
                                                    BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                                                    AggregateStateRecord currentStateRecord,
                                                    DomainEvent domainEvent) throws IOException {
        AggregateState currentState = materialize(currentStateRecord);
        DomainEventType<?> domainEventType = getDomainEventType(domainEvent.getClass());

        if (!(domainEvent instanceof ErrorEvent)) {
            AggregateState nextState = eventSourcingHandlers.get(domainEventType).apply(domainEvent, currentState);

            AggregateStateRecord nextStateRecord = new AggregateStateRecord(
                    currentStateRecord.tenantId(),
                    type.typeName(),
                    type.version(),
                    serialize(nextState),
                    getEncoding(type),
                    currentStateRecord.aggregateId(),
                    correlationId,
                    currentStateRecord.generation() + 1L);
            protocolRecordConsumer.accept(nextStateRecord);
            DomainEventRecord eventRecord = new DomainEventRecord(
                    currentStateRecord.tenantId(),
                    domainEventType.typeName(),
                    domainEventType.version(),
                    serialize(domainEvent),
                    getEncoding(domainEventType),
                    domainEvent.getAggregateId(),
                    correlationId,
                    nextStateRecord.generation());
            protocolRecordConsumer.accept(eventRecord);
            indexDomainEventIfRequired(eventRecord, nextState, domainEventIndexer, false);
            return nextStateRecord;
        } else {

            DomainEventRecord eventRecord = new DomainEventRecord(
                    currentStateRecord.tenantId(),
                    domainEventType.typeName(),
                    domainEventType.version(),
                    serialize(domainEvent),
                    getEncoding(domainEventType),
                    domainEvent.getAggregateId(),
                    correlationId,
                    -1L);
            protocolRecordConsumer.accept(eventRecord);


            return currentStateRecord;
        }
    }

    @Override
    public void handleExternalDomainEventRecord(DomainEventRecord eventRecord,
                                                Consumer<ProtocolRecord> protocolRecordConsumer,
                                                BiConsumer<DomainEventRecord, IndexParams> domainEventIndexer,
                                                Supplier<AggregateStateRecord> stateRecordSupplier) throws IOException {

        DomainEventType<?> domainEventType = getDomainEventType(eventRecord);

        if (domainEventType != null) {
            if (eventCreateHandler != null && eventCreateHandler.getEventType().equals(domainEventType)) {

                if (stateRecordSupplier.get() != null) {

                    log.warn("External DomainEvent {} wants to create a {} Aggregate with id {}, but the state already exists. Generate a AggregateAlreadyExistsError",
                            eventRecord.name(),
                            getName(),
                            eventRecord.aggregateId());
                    aggregateAlreadyExists(eventRecord, protocolRecordConsumer);
                } else {
                    handleCreateEvent(domainEventType, eventRecord, protocolRecordConsumer, domainEventIndexer);
                }
            } else {

                if (eventHandlers.containsKey(domainEventType)) {
                    handleEvent(domainEventType, eventRecord, protocolRecordConsumer, domainEventIndexer, stateRecordSupplier);
                }
            }
        }
    }

    @Nullable
    private DomainEventType<?> getDomainEventType(DomainEventRecord eventRecord) {

        return domainEvents.entrySet().stream()
                .filter(entry -> entry.getValue().external())
                .filter(entry -> entry.getValue().typeName().equals(eventRecord.name()))
                .filter(entry -> entry.getValue().version() <= eventRecord.version())
                .max(Comparator.comparingInt(entry -> entry.getValue().version()))
                .map(Map.Entry::getValue).orElse(null);
    }

    @Override
    public Collection<DomainEventType<?>> getAllDomainEventTypes() {
        return this.domainEvents.values();
    }

    @Override
    public Collection<DomainEventType<?>> getProducedDomainEventTypes() {
        return this.domainEvents.values().stream().filter(domainEventType -> !domainEventType.external()).collect(Collectors.toSet());
    }

    @Override
    public Collection<DomainEventType<?>> getExternalDomainEventTypes() {
        return this.domainEvents.values().stream().filter(DomainEventType::external).collect(Collectors.toSet());
    }

    @Override
    public Collection<CommandType<?>> getAllCommandTypes() {
        return this.commandTypes.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());
    }

    @Override
    public Collection<CommandType<?>> getLocalCommandTypes() {
        return this.commandTypes.values().stream().flatMap(Collection::stream).filter(commandType -> !commandType.external()).collect(Collectors.toSet());
    }

    @Override
    public Collection<CommandType<?>> getExternalCommandTypes() {
        return this.commandTypes.values().stream().flatMap(Collection::stream).filter(CommandType::external).collect(Collectors.toSet());
    }

    private DomainEventType<?> getDomainEventType(Class<?> domainEventClass) {
        return domainEvents.get(domainEventClass);
    }

    @Override
    public CommandType<?> getLocalCommandType(String type, int version) {
        return commandTypes.getOrDefault(type, Collections.emptyList()).stream()
                .filter(commandType -> commandType.version() == version)
                .findFirst()
                .orElseThrow(() -> new IllegalArgumentException("No CommandType found for type " + type + " and version " + version));
    }

    @Override
    public boolean shouldGenerateGPRKey(CommandRecord commandRecord) {
        return getCommandType(commandRecord).create() && generateGDPRKeyOnCreate;
    }

    @Override
    public boolean shouldGenerateGPRKey(DomainEventRecord eventRecord) {
        return Optional.ofNullable(getDomainEventType(eventRecord))
                .map(domainEventType -> domainEventType.create() && generateGDPRKeyOnCreate).orElse(false);
    }

    @Override
    public boolean shouldHandlePIIData() {
        return shouldHandlePIIData || generateGDPRKeyOnCreate;
    }

    protected abstract DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException;

    protected abstract AggregateState materialize(AggregateStateRecord stateRecord) throws IOException;

    protected abstract byte[] serialize(AggregateState state) throws IOException;

    protected abstract byte[] serialize(DomainEvent domainEvent) throws SerializationException;

    protected abstract PayloadEncoding getEncoding(CommandType<?> type);

    protected abstract PayloadEncoding getEncoding(DomainEventType<?> type);

    protected abstract PayloadEncoding getEncoding(AggregateStateType<?> type);

    protected AggregateStateType<?> getAggregateStateType(AggregateStateRecord record) {

        return type;
    }

    protected void addCommand(CommandType<?> commandType) {
        this.commandTypes.computeIfAbsent(commandType.typeName(), typeName -> new ArrayList<>()).add(commandType);
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/aggregate/IndexParams.java
================
package org.elasticsoftware.akces.aggregate;

public record IndexParams(String indexName, String indexKey, boolean createIndex) {
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/beans/AggregateBeanFactoryPostProcessor.java
================
package org.elasticsoftware.akces.beans;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.elasticsoftware.akces.AkcesAggregateController;
import org.elasticsoftware.akces.aggregate.AggregateState;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.annotations.*;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.errors.AggregateAlreadyExistsErrorEvent;
import org.elasticsoftware.akces.errors.CommandExecutionErrorEvent;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.kafka.AggregateRuntimeFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.BeansException;
import org.springframework.beans.factory.aot.BeanFactoryInitializationAotContribution;
import org.springframework.beans.factory.aot.BeanFactoryInitializationAotProcessor;
import org.springframework.beans.factory.aot.BeanRegistrationExcludeFilter;
import org.springframework.beans.factory.config.BeanDefinition;
import org.springframework.beans.factory.config.BeanFactoryPostProcessor;
import org.springframework.beans.factory.config.ConfigurableListableBeanFactory;
import org.springframework.beans.factory.support.BeanDefinitionBuilder;
import org.springframework.beans.factory.support.BeanDefinitionRegistry;
import org.springframework.beans.factory.support.RegisteredBean;
import org.springframework.context.ApplicationContextException;

import java.lang.reflect.Method;
import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.Stream;

public class AggregateBeanFactoryPostProcessor implements BeanFactoryPostProcessor, BeanFactoryInitializationAotProcessor, BeanRegistrationExcludeFilter {
    private static final Logger logger = LoggerFactory.getLogger(AggregateBeanFactoryPostProcessor.class);
    public static final List<DomainEventType<? extends DomainEvent>> COMMAND_HANDLER_CREATE_SYSTEM_ERRORS = List.of(
            new DomainEventType<>("AggregateAlreadyExistsError", 1, AggregateAlreadyExistsErrorEvent.class, false, false, true),
            new DomainEventType<>("CommandExecutionError", 1, CommandExecutionErrorEvent.class, false, false, true)
    );
    public static final List<DomainEventType<? extends DomainEvent>> COMMAND_HANDLER_SYSTEM_ERRORS = List.of(
            new DomainEventType<>("CommandExecutionError", 1, CommandExecutionErrorEvent.class, false, false, true)
    );
    public static final List<DomainEventType<? extends DomainEvent>> EVENT_HANDLER_CREATE_SYSTEM_ERRORS = List.of(
            new DomainEventType<>("AggregateAlreadyExistsError", 1, AggregateAlreadyExistsErrorEvent.class, false, false, true)
    );

    @Override
    public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
        if (beanFactory instanceof BeanDefinitionRegistry bdr) {
            logger.info("Processing Aggregate beans");
            Arrays.asList(beanFactory.getBeanNamesForAnnotation(AggregateInfo.class)).forEach(beanName -> {
                logger.info("Processing Aggregate bean {}", beanName);
                BeanDefinition bd = beanFactory.getBeanDefinition(beanName);
                try {
                    Class<?> aggregateClass = Class.forName(bd.getBeanClassName());
                    List<Method> commandHandlers = Arrays.stream(aggregateClass.getMethods())
                            .filter(method -> method.isAnnotationPresent(CommandHandler.class))
                            .toList();
                    List<Method> eventHandlers = Arrays.stream(aggregateClass.getMethods())
                            .filter(method -> method.isAnnotationPresent(EventHandler.class))
                            .toList();
                    List<Method> eventSourcingHandlers = Arrays.stream(aggregateClass.getMethods())
                            .filter(method -> method.isAnnotationPresent(EventSourcingHandler.class))
                            .toList();
                    commandHandlers.forEach(commandHandlerMethod -> processCommandHandler(beanName, commandHandlerMethod, bdr));
                    eventHandlers.forEach(eventHandlerMethod -> processEventHandler(beanName, eventHandlerMethod, bdr));
                    eventSourcingHandlers.forEach(eventSourcingHandlerMethod -> processEventSourcingHandler(beanName, eventSourcingHandlerMethod, bdr));
                } catch (ClassNotFoundException e) {
                    throw new ApplicationContextException("Unable to load class for bean " + beanName, e);
                }

                bdr.registerBeanDefinition(beanName + "AggregateRuntimeFactory",
                        BeanDefinitionBuilder.genericBeanDefinition(AggregateRuntimeFactory.class)
                                .addConstructorArgReference(beanFactory.getBeanNamesForType(ObjectMapper.class)[0])
                                .addConstructorArgReference("aggregateServiceSchemaRegistry")
                                .addConstructorArgReference(beanName)
                                .getBeanDefinition());


                if (beanFactory.containsBeanDefinition("aggregateServiceConsumerFactory") &&
                        beanFactory.containsBeanDefinition("aggregateServiceProducerFactory") &&
                        beanFactory.containsBeanDefinition("aggregateServiceControlProducerFactory") &&
                        beanFactory.containsBeanDefinition("aggregateServiceAggregateStateRepositoryFactory")) {
                    bdr.registerBeanDefinition(beanName + "AkcesController",
                            BeanDefinitionBuilder.genericBeanDefinition(AkcesAggregateController.class)
                                    .addConstructorArgReference("aggregateServiceConsumerFactory")
                                    .addConstructorArgReference("aggregateServiceProducerFactory")
                                    .addConstructorArgReference("aggregateServiceControlConsumerFactory")
                                    .addConstructorArgReference("aggregateServiceControlProducerFactory")
                                    .addConstructorArgReference("aggregateServiceAggregateStateRepositoryFactory")
                                    .addConstructorArgReference("aggregateServiceGDPRContextRepositoryFactory")
                                    .addConstructorArgReference(beanName + "AggregateRuntimeFactory")

                                    .addConstructorArgReference("aggregateServiceKafkaAdmin")
                                    .setInitMethodName("start")
                                    .setDestroyMethodName("close")
                                    .getBeanDefinition());
                }
            });
        } else {
            throw new ApplicationContextException("BeanFactory is not a BeanDefinitionRegistry");
        }
    }

    private void processEventSourcingHandler(String aggregateBeanName, Method eventSourcingHandlerMethod, BeanDefinitionRegistry bdr) {
        EventSourcingHandler eventSourcingHandler = eventSourcingHandlerMethod.getAnnotation(EventSourcingHandler.class);
        if (eventSourcingHandlerMethod.getParameterCount() == 2 &&
                DomainEvent.class.isAssignableFrom(eventSourcingHandlerMethod.getParameterTypes()[0]) &&
                AggregateState.class.isAssignableFrom(eventSourcingHandlerMethod.getParameterTypes()[1]) &&
                AggregateState.class.isAssignableFrom(eventSourcingHandlerMethod.getReturnType())) {
            DomainEventInfo eventInfo = eventSourcingHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);

            String beanName = aggregateBeanName + "_esh_" + eventSourcingHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
            bdr.registerBeanDefinition(beanName,
                    BeanDefinitionBuilder.genericBeanDefinition(EventSourcingHandlerFunctionAdapter.class)
                            .addConstructorArgReference(aggregateBeanName)
                            .addConstructorArgValue(eventSourcingHandlerMethod.getName())
                            .addConstructorArgValue(eventSourcingHandlerMethod.getParameterTypes()[0])
                            .addConstructorArgValue(eventSourcingHandlerMethod.getParameterTypes()[1])
                            .addConstructorArgValue(eventSourcingHandler.create())
                            .addConstructorArgValue(eventInfo.type())
                            .addConstructorArgValue(eventInfo.version())
                            .setInitMethodName("init")
                            .getBeanDefinition());
        } else {
            throw new ApplicationContextException("Invalid EventSourcingHandler method signature: " + eventSourcingHandlerMethod);
        }
    }

    private void processEventHandler(String aggregateBeanName, Method eventHandlerMethod, BeanDefinitionRegistry bdr) {
        EventHandler eventHandler = eventHandlerMethod.getAnnotation(EventHandler.class);
        if (eventHandlerMethod.getParameterCount() == 2 &&
                DomainEvent.class.isAssignableFrom(eventHandlerMethod.getParameterTypes()[0]) &&
                AggregateState.class.isAssignableFrom(eventHandlerMethod.getParameterTypes()[1]) &&
                Stream.class.isAssignableFrom(eventHandlerMethod.getReturnType())) {
            DomainEventInfo eventInfo = eventHandlerMethod.getParameterTypes()[0].getAnnotation(DomainEventInfo.class);

            String beanName = aggregateBeanName + "_eh_" + eventHandlerMethod.getName() + "_" + eventInfo.type() + "_" + eventInfo.version();
            bdr.registerBeanDefinition(beanName,
                    BeanDefinitionBuilder.genericBeanDefinition(EventHandlerFunctionAdapter.class)
                            .addConstructorArgReference(aggregateBeanName)
                            .addConstructorArgValue(eventHandlerMethod.getName())
                            .addConstructorArgValue(eventHandlerMethod.getParameterTypes()[0])
                            .addConstructorArgValue(eventHandlerMethod.getParameterTypes()[1])
                            .addConstructorArgValue(eventHandler.create())
                            .addConstructorArgValue(generateDomainEventTypes(eventHandler.produces(), eventHandler.create()))
                            .addConstructorArgValue(generateEventHandlerErrorEventTypes(eventHandler.errors(), eventHandler.create()))
                            .addConstructorArgValue(eventInfo.type())
                            .addConstructorArgValue(eventInfo.version())
                            .setInitMethodName("init")
                            .getBeanDefinition());
        } else {
            throw new ApplicationContextException("Invalid EventHandler method signature: " + eventHandlerMethod);
        }
    }

    private void processCommandHandler(String aggregateBeanName, Method commandHandlerMethod, BeanDefinitionRegistry bdr) {

        CommandHandler commandHandler = commandHandlerMethod.getAnnotation(CommandHandler.class);
        if (commandHandlerMethod.getParameterCount() == 2 &&
                Command.class.isAssignableFrom(commandHandlerMethod.getParameterTypes()[0]) &&
                AggregateState.class.isAssignableFrom(commandHandlerMethod.getParameterTypes()[1]) &&
                Stream.class.isAssignableFrom(commandHandlerMethod.getReturnType())) {
            CommandInfo commandInfo = commandHandlerMethod.getParameterTypes()[0].getAnnotation(CommandInfo.class);

            String beanName = aggregateBeanName + "_ch_" + commandHandlerMethod.getName() + "_" + commandInfo.type() + "_" + commandInfo.version();
            bdr.registerBeanDefinition(beanName,
                    BeanDefinitionBuilder.genericBeanDefinition(CommandHandlerFunctionAdapter.class)
                            .addConstructorArgReference(aggregateBeanName)
                            .addConstructorArgValue(commandHandlerMethod.getName())
                            .addConstructorArgValue(commandHandlerMethod.getParameterTypes()[0])
                            .addConstructorArgValue(commandHandlerMethod.getParameterTypes()[1])
                            .addConstructorArgValue(commandHandler.create())
                            .addConstructorArgValue(generateDomainEventTypes(commandHandler.produces(), commandHandler.create()))
                            .addConstructorArgValue(generateCommandHandlerErrorEventTypes(commandHandler.errors(), commandHandler.create()))
                            .addConstructorArgValue(commandInfo.type())
                            .addConstructorArgValue(commandInfo.version())
                            .setInitMethodName("init").getBeanDefinition()
            );
        } else {
            throw new ApplicationContextException("Invalid CommandHandler method signature: " + commandHandlerMethod);
        }
    }

    private List<DomainEventType<?>> generateDomainEventTypes(Class<? extends DomainEvent>[] domainEventClasses,
                                                              boolean isCreate) {
        return Arrays.stream(domainEventClasses).map(eventClass -> {
            DomainEventInfo eventInfo = eventClass.getAnnotation(DomainEventInfo.class);
            return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, isCreate, false, false);
        }).collect(Collectors.toList());
    }

    private List<DomainEventType<?>> generateEventHandlerErrorEventTypes(Class<? extends DomainEvent>[] domainEventClasses, boolean isCreate) {
        Stream<DomainEventType<? extends DomainEvent>> systemErrorEvents = (isCreate) ? EVENT_HANDLER_CREATE_SYSTEM_ERRORS.stream() : Stream.empty();
        return Stream.concat(Arrays.stream(domainEventClasses).map(eventClass -> {
            DomainEventInfo eventInfo = eventClass.getAnnotation(DomainEventInfo.class);
            return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, false, false, true);
        }), systemErrorEvents).collect(Collectors.toList());
    }

    private List<DomainEventType<?>> generateCommandHandlerErrorEventTypes(Class<? extends DomainEvent>[] domainEventClasses, boolean isCreate) {
        Stream<DomainEventType<? extends DomainEvent>> systemErrorEvents = (isCreate) ? COMMAND_HANDLER_CREATE_SYSTEM_ERRORS.stream() : COMMAND_HANDLER_SYSTEM_ERRORS.stream();
        return Stream.concat(Arrays.stream(domainEventClasses).map(eventClass -> {
            DomainEventInfo eventInfo = eventClass.getAnnotation(DomainEventInfo.class);
            return new DomainEventType<>(eventInfo.type(), eventInfo.version(), eventClass, false, false, true);
        }), systemErrorEvents).collect(Collectors.toList());
    }

    @Override
    public BeanFactoryInitializationAotContribution processAheadOfTime(ConfigurableListableBeanFactory beanFactory) {
        logger.info("Processing Aggregate beans for AOT");
        return null;
    }

    @Override
    public boolean isExcludedFromAotProcessing(RegisteredBean registeredBean) {
        return false;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/beans/CommandHandlerFunctionAdapter.java
================
package org.elasticsoftware.akces.beans;

import org.elasticsoftware.akces.aggregate.*;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.events.DomainEvent;

import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.util.List;
import java.util.stream.Stream;

public class CommandHandlerFunctionAdapter<S extends AggregateState, C extends Command, E extends DomainEvent>
        implements CommandHandlerFunction<S, C, E> {
    private final Aggregate<S> aggregate;
    private final String adapterMethodName;
    private final Class<C> commandClass;
    private final Class<S> stateClass;
    private final boolean create;
    private final CommandType<C> commandType;
    private final List<DomainEventType<E>> producedDomainEventTypes;
    private final List<DomainEventType<E>> errorEventTypes;
    private Method adapterMethod;

    public CommandHandlerFunctionAdapter(Aggregate<S> aggregate,
                                         String adapterMethodName,
                                         Class<C> commandClass,
                                         Class<S> stateClass,
                                         boolean create,
                                         List<DomainEventType<E>> producedDomainEventTypes,
                                         List<DomainEventType<E>> errorEventTypes,
                                         String typeName,
                                         int version) {
        this.aggregate = aggregate;
        this.adapterMethodName = adapterMethodName;
        this.commandClass = commandClass;
        this.stateClass = stateClass;
        this.create = create;
        this.producedDomainEventTypes = producedDomainEventTypes;
        this.errorEventTypes = errorEventTypes;
        this.commandType = new CommandType<>(typeName, version, commandClass, create, false);
    }

    @SuppressWarnings("unused")
    public void init() {
        try {
            adapterMethod = aggregate.getClass().getMethod(adapterMethodName, commandClass, stateClass);
        } catch (NoSuchMethodException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public Stream<E> apply(C command, S state) {
        try {
            return (Stream<E>) adapterMethod.invoke(aggregate, command, state);
        } catch (IllegalAccessException e) {
            throw new RuntimeException(e);
        } catch (InvocationTargetException e) {
            if (e.getCause() != null) {
                if (e.getCause() instanceof RuntimeException) {
                    throw (RuntimeException) e.getCause();
                } else {
                    throw new RuntimeException(e.getCause());
                }
            } else {
                throw new RuntimeException(e);
            }
        }
    }

    @Override
    public boolean isCreate() {
        return create;
    }

    @Override
    public CommandType<C> getCommandType() {
        return commandType;
    }

    @Override
    public Aggregate<S> getAggregate() {
        return aggregate;
    }

    @Override
    public List<DomainEventType<E>> getProducedDomainEventTypes() {
        return producedDomainEventTypes;
    }

    @Override
    public List<DomainEventType<E>> getErrorEventTypes() {
        return errorEventTypes;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/beans/DomainEventTypeValueCodeGeneratorDelegate.java
================
package org.elasticsoftware.akces.beans;

import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.springframework.aot.generate.ValueCodeGenerator;
import org.springframework.javapoet.CodeBlock;

public class DomainEventTypeValueCodeGeneratorDelegate implements ValueCodeGenerator.Delegate {
    public DomainEventTypeValueCodeGeneratorDelegate() {
    }

    @Override
    public CodeBlock generateCode(ValueCodeGenerator valueCodeGenerator, Object value) {
        if (value instanceof DomainEventType<?>(
                String typeName, int version, Class<?> typeClass, boolean create, boolean external, boolean error
        )) {
            return CodeBlock.builder()
                    .add("new $T($S, $L, $T.class, $L, $L, $L)",
                            DomainEventType.class,
                            typeName,
                            version,
                            typeClass,
                            create,
                            external,
                            error)
                    .build();
        } else {
            return null;
        }
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/beans/EventHandlerFunctionAdapter.java
================
package org.elasticsoftware.akces.beans;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.Aggregate;
import org.elasticsoftware.akces.aggregate.AggregateState;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.aggregate.EventHandlerFunction;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.events.ErrorEvent;

import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.util.List;
import java.util.stream.Stream;

public class EventHandlerFunctionAdapter<S extends AggregateState, InputEvent extends DomainEvent, E extends DomainEvent> implements EventHandlerFunction<S, InputEvent, E> {
    private final Aggregate<S> aggregate;
    private final String adapterMethodName;
    private final Class<InputEvent> inputEventClass;
    private final Class<S> stateClass;
    private final boolean create;
    private final List<DomainEventType<E>> producedDomainEventTypes;
    private final List<DomainEventType<E>> errorEventTypes;
    private final DomainEventType<InputEvent> domainEventType;
    private Method adapterMethod;

    public EventHandlerFunctionAdapter(Aggregate<S> aggregate,
                                       String adapterMethodName,
                                       Class<InputEvent> inputEventClass,
                                       Class<S> stateClass,
                                       boolean create,
                                       List<DomainEventType<E>> producedDomainEventTypes,
                                       List<DomainEventType<E>> errorEventTypes,
                                       String typeName,
                                       int version) {
        this.aggregate = aggregate;
        this.adapterMethodName = adapterMethodName;
        this.inputEventClass = inputEventClass;
        this.stateClass = stateClass;
        this.create = create;
        this.producedDomainEventTypes = producedDomainEventTypes;
        this.errorEventTypes = errorEventTypes;
        this.domainEventType = new DomainEventType<>(
                typeName,
                version,
                inputEventClass,
                create,
                true,
                ErrorEvent.class.isAssignableFrom(inputEventClass));
    }

    @SuppressWarnings("unused")
    public void init() {
        try {
            adapterMethod = aggregate.getClass().getMethod(adapterMethodName, inputEventClass, stateClass);
        } catch (NoSuchMethodException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public Stream<E> apply(@NotNull InputEvent event, S state) {
        try {
            return (Stream<E>) adapterMethod.invoke(aggregate, event, state);
        } catch (IllegalAccessException e) {
            throw new RuntimeException(e);
        } catch (InvocationTargetException e) {
            if (e.getCause() != null) {
                if (e.getCause() instanceof RuntimeException) {
                    throw (RuntimeException) e.getCause();
                } else {
                    throw new RuntimeException(e.getCause());
                }
            } else {
                throw new RuntimeException(e);
            }
        }
    }

    @Override
    public DomainEventType<InputEvent> getEventType() {
        return domainEventType;
    }

    @Override
    public Aggregate<S> getAggregate() {
        return aggregate;
    }

    @Override
    public boolean isCreate() {
        return create;
    }

    @Override
    public List<DomainEventType<E>> getProducedDomainEventTypes() {
        return producedDomainEventTypes;
    }

    @Override
    public List<DomainEventType<E>> getErrorEventTypes() {
        return errorEventTypes;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/beans/EventSourcingHandlerFunctionAdapter.java
================
package org.elasticsoftware.akces.beans;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.Aggregate;
import org.elasticsoftware.akces.aggregate.AggregateState;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.aggregate.EventSourcingHandlerFunction;
import org.elasticsoftware.akces.events.DomainEvent;

import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;

public class EventSourcingHandlerFunctionAdapter<S extends AggregateState, E extends DomainEvent> implements EventSourcingHandlerFunction<S, E> {
    private final Aggregate<S> aggregate;
    private final String adapterMethodName;
    private final Class<E> domainEventClass;
    private final Class<S> stateClass;
    private final boolean create;
    private final DomainEventType<E> domainEventType;
    private Method adapterMethod;

    public EventSourcingHandlerFunctionAdapter(Aggregate<S> aggregate,
                                               String adapterMethodName,
                                               Class<E> domainEventClass,
                                               Class<S> stateClass,
                                               boolean create,
                                               String typeName,
                                               int version) {
        this.aggregate = aggregate;
        this.adapterMethodName = adapterMethodName;
        this.domainEventClass = domainEventClass;
        this.stateClass = stateClass;
        this.create = create;
        this.domainEventType = new DomainEventType<>(typeName, version, domainEventClass, create, false, false);
    }

    @SuppressWarnings("unused")
    public void init() {
        try {
            adapterMethod = aggregate.getClass().getMethod(adapterMethodName, domainEventClass, stateClass);
        } catch (NoSuchMethodException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public @NotNull S apply(@NotNull E event, S state) {
        try {
            return (S) adapterMethod.invoke(aggregate, event, state);
        } catch (IllegalAccessException e) {
            throw new RuntimeException(e);
        } catch (InvocationTargetException e) {
            if (e.getCause() != null) {
                if (e.getCause() instanceof RuntimeException) {
                    throw (RuntimeException) e.getCause();
                } else {
                    throw new RuntimeException(e.getCause());
                }
            } else {
                throw new RuntimeException(e);
            }
        }
    }

    @Override
    public DomainEventType<E> getEventType() {
        return domainEventType;
    }

    @Override
    public Aggregate<S> getAggregate() {
        return aggregate;
    }

    @Override
    public boolean isCreate() {
        return create;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/control/AkcesRegistry.java
================
package org.elasticsoftware.akces.control;

import jakarta.annotation.Nonnull;
import org.elasticsoftware.akces.aggregate.CommandType;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.commands.Command;

public interface AkcesRegistry {
    CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass);

    String resolveTopic(@Nonnull Class<? extends Command> commandClass);

    String resolveTopic(@Nonnull CommandType<?> commandType);

    String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType);

    Integer resolvePartition(@Nonnull String aggregateId);
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartition.java
================
package org.elasticsoftware.akces.kafka;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.*;
import org.elasticsoftware.akces.aggregate.AggregateRuntime;
import org.elasticsoftware.akces.aggregate.CommandType;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.aggregate.IndexParams;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.commands.CommandBus;
import org.elasticsoftware.akces.control.AkcesRegistry;
import org.elasticsoftware.akces.gdpr.GDPRContextHolder;
import org.elasticsoftware.akces.gdpr.GDPRContextRepository;
import org.elasticsoftware.akces.gdpr.GDPRContextRepositoryFactory;
import org.elasticsoftware.akces.gdpr.GDPRKeyUtils;
import org.elasticsoftware.akces.protocol.*;
import org.elasticsoftware.akces.state.AggregateStateRepository;
import org.elasticsoftware.akces.state.AggregateStateRepositoryFactory;
import org.elasticsoftware.akces.util.HostUtils;
import org.elasticsoftware.akces.util.KafkaSender;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.ProducerFactory;

import java.io.IOException;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.function.BiFunction;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import static java.util.Collections.singletonList;
import static org.elasticsoftware.akces.gdpr.GDPRContextHolder.getCurrentGDPRContext;
import static org.elasticsoftware.akces.kafka.AggregatePartitionState.*;
import static org.elasticsoftware.akces.util.KafkaUtils.getIndexTopicName;

public class AggregatePartition implements Runnable, AutoCloseable, CommandBus {
    private static final Logger logger = LoggerFactory.getLogger(AggregatePartition.class);
    private final ConsumerFactory<String, ProtocolRecord> consumerFactory;
    private final ProducerFactory<String, ProtocolRecord> producerFactory;
    private final AggregateRuntime runtime;
    private final AggregateStateRepository stateRepository;
    private final GDPRContextRepository gdprContextRepository;
    private final Integer id;
    private final TopicPartition commandPartition;
    private final TopicPartition domainEventPartition;
    private final TopicPartition statePartition;
    private final TopicPartition gdprKeyPartition;
    private final Set<TopicPartition> externalEventPartitions = new HashSet<>();
    private final Collection<DomainEventType<?>> externalDomainEventTypes;
    private final AkcesRegistry ackesRegistry;
    private final CountDownLatch shutdownLatch = new CountDownLatch(1);
    private final BiFunction<String, String, Boolean> indexTopicCreator;
    private Consumer<String, ProtocolRecord> consumer;
    private Producer<String, ProtocolRecord> producer;
    private volatile AggregatePartitionState processState;
    private Map<TopicPartition, Long> initializedEndOffsets = Collections.emptyMap();
    private volatile Thread aggregatePartitionThread = null;


    public AggregatePartition(ConsumerFactory<String, ProtocolRecord> consumerFactory,
                              ProducerFactory<String, ProtocolRecord> producerFactory,
                              AggregateRuntime runtime,
                              AggregateStateRepositoryFactory stateRepositoryFactory,
                              GDPRContextRepositoryFactory gdprContextRepositoryFactory,
                              Integer id,
                              TopicPartition commandPartition,
                              TopicPartition domainEventPartition,
                              TopicPartition statePartition,
                              TopicPartition gdprKeyPartition,
                              Collection<DomainEventType<?>> externalDomainEventTypes,
                              AkcesRegistry ackesRegistry,
                              BiFunction<String, String, Boolean> indexTopicCreator) {
        this.gdprKeyPartition = gdprKeyPartition;
        this.ackesRegistry = ackesRegistry;
        this.consumerFactory = consumerFactory;
        this.producerFactory = producerFactory;
        this.runtime = runtime;
        this.indexTopicCreator = indexTopicCreator;
        this.stateRepository = stateRepositoryFactory.create(runtime, id);
        this.id = id;
        this.commandPartition = commandPartition;
        this.domainEventPartition = domainEventPartition;
        this.statePartition = statePartition;
        this.externalDomainEventTypes = externalDomainEventTypes;
        this.processState = INITIALIZING;
        this.gdprContextRepository = gdprContextRepositoryFactory.create(runtime.getName(), id);
    }

    public Integer getId() {
        return id;
    }

    @Override
    public void run() {
        try {

            this.aggregatePartitionThread = Thread.currentThread();

            AggregatePartitionCommandBus.registerCommandBus(this);
            logger.info("Starting AggregatePartition {} of {}Aggregate", id, runtime.getName());
            this.consumer = consumerFactory.createConsumer(
                    runtime.getName() + "Aggregate-partition-" + id,
                    runtime.getName() + "Aggregate-partition-" + id + "-" + HostUtils.getHostName(),
                    null);
            this.producer = producerFactory.createProducer(runtime.getName() + "Aggregate-partition-" + id + "-" + HostUtils.getHostName());

            externalDomainEventTypes.forEach(domainEventType -> {
                String topic = ackesRegistry.resolveTopic(domainEventType);
                externalEventPartitions.add(new TopicPartition(topic, id));
            });

            consumer.assign(Stream.concat(Stream.concat(
                                    Stream.of(commandPartition, domainEventPartition, statePartition), runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty()),
                                    externalEventPartitions.stream())
                    .toList());
            logger.info("Assigned partitions {} for AggregatePartition {} of {}Aggregate", consumer.assignment(), id, runtime.getName());
            while (processState != SHUTTING_DOWN) {
                process();
            }
            logger.info("Shutting down AggregatePartition {} of {}Aggregate", id, runtime.getName());
        } catch (Throwable t) {
            logger.error("Unexpected error in AggregatePartition {} of {}Aggregate", id, runtime.getName(), t);
        } finally {
            try {
                consumer.close(Duration.ofSeconds(5));
                producer.close(Duration.ofSeconds(5));
            } catch (InterruptException e) {

            } catch (KafkaException e) {
                logger.error("Error closing consumer/producer", e);
            }
            try {
                stateRepository.close();
            } catch (IOException e) {
                logger.error("Error closing state repository", e);
            }
            try {
                gdprContextRepository.close();
            } catch (IOException e) {
                logger.error("Error closing gdpr context repository", e);
            }
            AggregatePartitionCommandBus.registerCommandBus(null);
        }
        logger.info("Finished Shutting down AggregatePartition {} of {}Aggregate", id, runtime.getName());
        shutdownLatch.countDown();
    }

    @Override
    public void close() throws InterruptedException {
        processState = SHUTTING_DOWN;

        try {
            if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
                logger.info("AggregatePartition={} has been shutdown", id);
            } else {
                logger.warn("AggregatePartition={} did not shutdown within 10 seconds", id);
            }
        } catch (InterruptedException e) {

        }
    }

    @Override
    public void send(Command command) {

        if (Thread.currentThread() != aggregatePartitionThread) {
            throw new IllegalStateException("send() can only be called from the AggregatePartition thread");
        }

        CommandType<?> commandType = ackesRegistry.resolveType(command.getClass());

        try {
            runtime.registerAndValidate(commandType);
        } catch (Exception e) {
            logger.error("Problem registering command {}", commandType.typeName(), e);


            throw new RuntimeException(e);
        }

        if (commandType != null) {

            String topic = ackesRegistry.resolveTopic(commandType);

            CommandRecord commandRecord = new CommandRecord(
                    null,
                    commandType.typeName(),
                    commandType.version(),
                    runtime.serialize(command),
                    PayloadEncoding.JSON,
                    command.getAggregateId(),
                    null,
                    null);

            Integer partition = ackesRegistry.resolvePartition(command.getAggregateId());
            KafkaSender.send(producer, new ProducerRecord<>(topic, partition, commandRecord.id(), commandRecord));
        }
    }

    private void send(ProtocolRecord protocolRecord) {
        if (protocolRecord instanceof AggregateStateRecord asr) {
            logger.trace("Sending AggregateStateRecord with id {} to {}", asr.aggregateId(), statePartition);

            Future<RecordMetadata> result = KafkaSender.send(producer, new ProducerRecord<>(statePartition.topic(), statePartition.partition(), asr.aggregateId(), asr));

            stateRepository.prepare(asr, result);
        } else if (protocolRecord instanceof DomainEventRecord der) {
            logger.trace("Sending DomainEventRecord {}:{} with id {} to {}", der.name(), der.version(), der.id(), domainEventPartition);
            KafkaSender.send(producer, new ProducerRecord<>(domainEventPartition.topic(), domainEventPartition.partition(), der.id(), der));
        } else if (protocolRecord instanceof GDPRKeyRecord gkr) {
            logger.trace("Sending GDPRKeyRecord with id {} to {}", gkr.aggregateId(), gdprKeyPartition);
            Future<RecordMetadata> result = KafkaSender.send(producer, new ProducerRecord<>(gdprKeyPartition.topic(), gdprKeyPartition.partition(), gkr.aggregateId(), gkr));
            gdprContextRepository.prepare(gkr, result);
        } else if (protocolRecord instanceof CommandRecord cr) {



            throw new IllegalArgumentException("""
                    send(ProtocolRecord) should not be used for CommandRecord type.
                    Use send(commandRecord,commandPartition) instead""");
        }
    }

    private void index(DomainEventRecord der, IndexParams params) {
        // send to the index topic
        String topicName = getIndexTopicName(params.indexName(), params.indexKey());
        if (params.createIndex()) {
            if (consumer.partitionsFor(topicName).isEmpty()) {
                if (indexTopicCreator.apply(params.indexName(), params.indexKey())) {
                    logger.info("Creating DomainEventIndex topic {}", topicName);
                }
            }
        }
        logger.trace("Indexing DomainEventRecord {}:{} with id {} to topic {}", der.name(), der.version(), der.id(), topicName + "-0");

        KafkaSender.send(producer, new ProducerRecord<>(topicName, 0, der.id(), der));
    }

    private void setupGDPRContext(String tenantId, String aggregateId, boolean createIfMissing) {

        if (!gdprContextRepository.exists(aggregateId) && createIfMissing) {
            logger.trace("Generating GDPR key for aggregate {}", aggregateId);

            GDPRKeyRecord gdprKeyRecord = new GDPRKeyRecord(
                    tenantId,
                    aggregateId,
                    GDPRKeyUtils.createKey().getEncoded());

            send(gdprKeyRecord);
        }

        GDPRContextHolder.setCurrentGDPRContext(gdprContextRepository.get(aggregateId));
    }

    private void tearDownGDPRContext() {
        GDPRContextHolder.resetCurrentGDPRContext();
    }

    private void handleCommand(CommandRecord commandRecord) {
        try {
            final List<DomainEventRecord> responseRecords = commandRecord.replyToTopicPartition() != null ? new ArrayList<>() : null;
            java.util.function.Consumer<ProtocolRecord> protocolRecordConsumer = (pr) -> {
                send(pr);
                if (responseRecords != null && pr instanceof DomainEventRecord der) {
                    responseRecords.add(der);
                }
            };
            setupGDPRContext(commandRecord.tenantId(), commandRecord.aggregateId(), runtime.shouldGenerateGPRKey(commandRecord));
            logger.trace("Handling CommandRecord with type {}", commandRecord.name());
            runtime.handleCommandRecord(commandRecord, protocolRecordConsumer, this::index, () -> stateRepository.get(commandRecord.aggregateId()));
            if (responseRecords != null) {
                CommandResponseRecord crr = new CommandResponseRecord(
                        commandRecord.tenantId(),
                        commandRecord.aggregateId(),
                        commandRecord.correlationId(),
                        commandRecord.id(),
                        responseRecords,
                        getCurrentGDPRContext() != null ? getCurrentGDPRContext().getEncryptionKey() : null);
                TopicPartition replyToTopicPartition = PartitionUtils.parseReplyToTopicPartition(commandRecord.replyToTopicPartition());
                logger.trace("Sending CommandResponseRecord with commandId {} to {}", crr.commandId(), replyToTopicPartition);
                KafkaSender.send(producer, new ProducerRecord<>(replyToTopicPartition.topic(), replyToTopicPartition.partition(), crr.commandId(), crr));
            }

        } catch (IOException e) {

            logger.error("Error handling command", e);
        } finally {
            tearDownGDPRContext();
        }
    }

    private void handleExternalEvent(DomainEventRecord eventRecord) {
        try {
            setupGDPRContext(eventRecord.tenantId(), eventRecord.aggregateId(), runtime.shouldGenerateGPRKey(eventRecord));
            logger.trace("Handling DomainEventRecord with type {} as External Event", eventRecord.name());
            runtime.handleExternalDomainEventRecord(eventRecord, this::send, this::index, () -> stateRepository.get(eventRecord.aggregateId()));
        } catch (IOException e) {

            logger.error("Error handling external event", e);
        } finally {
            tearDownGDPRContext();
        }
    }

    private void process() {
        try {
            if (processState == PROCESSING) {
                ConsumerRecords<String, ProtocolRecord> allRecords = consumer.poll(Duration.ofMillis(10));
                if (!allRecords.isEmpty()) {
                    processRecords(allRecords);
                }
            } else if (processState == LOADING_GDPR_KEYS) {
                ConsumerRecords<String, ProtocolRecord> gdprKeyRecords = consumer.poll(Duration.ofMillis(10));
                gdprContextRepository.process(gdprKeyRecords.records(gdprKeyPartition));

                if (gdprKeyRecords.isEmpty() && initializedEndOffsets.getOrDefault(gdprKeyPartition, 0L) <= consumer.position(gdprKeyPartition)) {

                    if (initializedEndOffsets.getOrDefault(statePartition, 0L) == 0L) {


                        commitInitialOffsetsIfNecessary();
                        logger.info("No state found in Kafka for AggregatePartition {} of {}Aggregate", id, runtime.getName());

                        consumer.resume(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());

                        processState = PROCESSING;
                    } else {
                        logger.info("Loading state for AggregatePartition {} of {}Aggregate", id, runtime.getName());

                        consumer.resume(singletonList(statePartition));

                        consumer.pause(singletonList(gdprKeyPartition));
                        processState = LOADING_STATE;
                    }
                }
            } else if (processState == LOADING_STATE) {
                ConsumerRecords<String, ProtocolRecord> stateRecords = consumer.poll(Duration.ofMillis(10));
                stateRepository.process(stateRecords.records(statePartition));

                if (stateRecords.isEmpty() && initializedEndOffsets.getOrDefault(statePartition, 0L) <= consumer.position(statePartition)) {

                    consumer.resume(Stream.concat(
                            Stream.concat(Stream.of(commandPartition, domainEventPartition),runtime.shouldHandlePIIData() ? Stream.of(gdprKeyPartition) : Stream.empty()),
                            externalEventPartitions.stream()).toList());

                    processState = PROCESSING;
                }
            } else if (processState == INITIALIZING) {
                logger.info(
                        "Initializing AggregatePartition {} of {}Aggregate. Will {}",
                        id,
                        runtime.getName(),
                        runtime.shouldHandlePIIData() ? "Handle PII Data" : "Not Handle PII Data");

                long stateRepositoryOffset = stateRepository.getOffset();
                if (stateRepositoryOffset >= 0) {
                    logger.info(
                            "Resuming State from offset {} for AggregatePartition {} of {}Aggregate",
                            stateRepositoryOffset,
                            id,
                            runtime.getName());
                    consumer.seek(statePartition, stateRepository.getOffset() + 1);
                } else {
                    consumer.seekToBeginning(singletonList(statePartition));
                }
                if(runtime.shouldHandlePIIData()) {

                    long gdprKeyRepositoryOffset = gdprContextRepository.getOffset();
                    if (gdprKeyRepositoryOffset >= 0) {
                        logger.info(
                                "Resuming GDPRKeys from offset {} for AggregatePartition {} of {}Aggregate",
                                gdprKeyRepositoryOffset,
                                id,
                                runtime.getName());
                        consumer.seek(gdprKeyPartition, gdprContextRepository.getOffset() + 1);
                    } else {
                        consumer.seekToBeginning(singletonList(gdprKeyPartition));
                    }

                    initializedEndOffsets = consumer.endOffsets(List.of(gdprKeyPartition, statePartition));
                    logger.info("Loading GDPR Keys for AggregatePartition {} of {}Aggregate", id, runtime.getName());

                    consumer.pause(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());
                    processState = LOADING_GDPR_KEYS;
                } else {

                    initializedEndOffsets = consumer.endOffsets(List.of(statePartition));

                    if (initializedEndOffsets.getOrDefault(statePartition, 0L) == 0L) {


                        commitInitialOffsetsIfNecessary();
                        logger.info("No state found in Kafka for AggregatePartition {} of {}Aggregate", id, runtime.getName());

                        consumer.resume(Stream.concat(Stream.of(statePartition, commandPartition, domainEventPartition), externalEventPartitions.stream()).toList());

                        processState = PROCESSING;
                    } else {
                        logger.info("Loading state for AggregatePartition {} of {}Aggregate", id, runtime.getName());

                        consumer.resume(singletonList(statePartition));

                        processState = LOADING_STATE;
                    }
                }
            }
        } catch (WakeupException | InterruptException ignore) {

        } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {

            logger.error("Fatal error during " + processState + " phase, shutting down AggregatePartition " + id + " of " + runtime.getName() + "Aggregate", e);
            processState = SHUTTING_DOWN;
        } catch (KafkaException e) {

            logger.error("Fatal error during " + processState + " phase, shutting down AggregatePartition " + id + " of " + runtime.getName() + "Aggregate", e);
            processState = SHUTTING_DOWN;
        }
    }

    private void commitInitialOffsetsIfNecessary() {

        String autoOffsetResetConfig = (String) Optional.ofNullable(consumerFactory.getConfigurationProperties()
                .get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG)).orElse("latest");
        if ("latest".equals(autoOffsetResetConfig)) {
            List<TopicPartition> topicPartitions = Stream.concat(Stream.of(commandPartition, domainEventPartition, statePartition), externalEventPartitions.stream()).toList();
            final Map<TopicPartition, Long> beginningOffsets = consumer.beginningOffsets(topicPartitions);
            final Map<TopicPartition, OffsetAndMetadata> committedOffsets = consumer.committed(new HashSet<>(topicPartitions));
            final Map<TopicPartition, OffsetAndMetadata> uncommittedTopicPartitions = new HashMap<>();
            committedOffsets.forEach((topicPartition, offsetAndMetadata) -> {
                if (offsetAndMetadata == null) {
                    logger.info("TopicPartition[{}] has no committed offsets, will commit offset {} to avoid " +
                            "skipping records", topicPartition, beginningOffsets.getOrDefault(topicPartition, 0L));
                    uncommittedTopicPartitions.put(topicPartition, new OffsetAndMetadata(beginningOffsets.getOrDefault(topicPartition, 0L)));
                }
            });
            if (!uncommittedTopicPartitions.isEmpty()) {
                producer.beginTransaction();
                producer.sendOffsetsToTransaction(uncommittedTopicPartitions, consumer.groupMetadata());
                producer.commitTransaction();
            }
        }
    }

    private void processRecords(ConsumerRecords<String, ProtocolRecord> allRecords) {
        try {
            if (logger.isTraceEnabled()) {
                logger.trace("Processing {} records in a single transaction", allRecords.count());
                logger.trace("Processing {} gdpr key records", allRecords.records(gdprKeyPartition).size());
                logger.trace("Processing {} command records", allRecords.records(commandPartition).size());
                if (!externalEventPartitions.isEmpty()) {
                    logger.trace("Processing {} external event records", externalEventPartitions.stream()
                            .map(externalEventPartition -> allRecords.records(externalEventPartition).size())
                            .mapToInt(Integer::intValue).sum());
                }
                logger.trace("Processing {} state records", allRecords.records(statePartition).size());
                logger.trace("Processing {} internal event records", allRecords.records(domainEventPartition).size());
            }

            producer.beginTransaction();
            Map<TopicPartition, Long> offsets = new HashMap<>();

            List<ConsumerRecord<String, ProtocolRecord>> gdprKeyRecords = allRecords.records(gdprKeyPartition);
            if (!gdprKeyRecords.isEmpty()) {
                gdprContextRepository.process(gdprKeyRecords);
                offsets.put(gdprKeyPartition, gdprKeyRecords.getLast().offset());
            }

            externalEventPartitions
                    .forEach(externalEventPartition -> allRecords.records(externalEventPartition)
                            .forEach(eventRecord -> {
                                handleExternalEvent((DomainEventRecord) eventRecord.value());
                                offsets.put(externalEventPartition, eventRecord.offset());
                            }));

            allRecords.records(commandPartition)
                    .forEach(commandRecord -> {
                        handleCommand((CommandRecord) commandRecord.value());
                        offsets.put(commandPartition, commandRecord.offset());
                    });

            List<ConsumerRecord<String, ProtocolRecord>> stateRecords = allRecords.records(statePartition);
            if (!stateRecords.isEmpty()) {
                stateRepository.process(stateRecords);
                offsets.put(statePartition, stateRecords.get(stateRecords.size() - 1).offset());
            }

            allRecords.records(domainEventPartition)
                    .forEach(domainEventRecord -> offsets.put(domainEventPartition, domainEventRecord.offset()));

            producer.sendOffsetsToTransaction(offsets.entrySet().stream()
                            .collect(Collectors.toMap(Map.Entry::getKey, e -> new OffsetAndMetadata(e.getValue() + 1))),
                    consumer.groupMetadata());
            producer.commitTransaction();

            stateRepository.commit();

            gdprContextRepository.commit();
        } catch (InvalidProducerEpochException e) {



            producer.abortTransaction();
            rollbackConsumer(allRecords);
            stateRepository.rollback();
            gdprContextRepository.rollback();
        }
    }

    private void rollbackConsumer(ConsumerRecords<String, ProtocolRecord> consumerRecords) {
        consumerRecords.partitions().forEach(topicPartition -> {

            consumerRecords.records(topicPartition).stream().map(ConsumerRecord::offset).min(Long::compareTo)
                    .ifPresent(offset -> consumer.seek(topicPartition, offset));
        });
    }

    public boolean isProcessing() {
        return processState == PROCESSING;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartitionCommandBus.java
================
package org.elasticsoftware.akces.kafka;

import org.elasticsoftware.akces.commands.CommandBusHolder;

class AggregatePartitionCommandBus extends CommandBusHolder {
    static void registerCommandBus(AggregatePartition aggregatePartition) {
        commandBusThreadLocal.set(aggregatePartition);
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregatePartitionState.java
================
package org.elasticsoftware.akces.kafka;

public enum AggregatePartitionState {
    INITIALIZING,
    LOADING_GDPR_KEYS,
    LOADING_STATE,
    PROCESSING,
    SHUTTING_DOWN
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/kafka/AggregateRuntimeFactory.java
================
package org.elasticsoftware.akces.kafka;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.elasticsoftware.akces.aggregate.*;
import org.elasticsoftware.akces.annotations.AggregateInfo;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.springframework.beans.BeansException;
import org.springframework.beans.factory.FactoryBean;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;

public class AggregateRuntimeFactory<S extends AggregateState> implements FactoryBean<AggregateRuntime>, ApplicationContextAware {
    private ApplicationContext applicationContext;
    private final ObjectMapper objectMapper;
    private final KafkaSchemaRegistry schemaRegistry;
    private final Aggregate<S> aggregate;

    public AggregateRuntimeFactory(ObjectMapper objectMapper,
                                   KafkaSchemaRegistry schemaRegistry,
                                   Aggregate<S> aggregate) {
        this.objectMapper = objectMapper;
        this.schemaRegistry = schemaRegistry;
        this.aggregate = aggregate;
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        this.applicationContext = applicationContext;
    }

    @Override
    public AggregateRuntime getObject() throws Exception {
        return createRuntime(aggregate);
    }

    @Override
    public Class<?> getObjectType() {
        return AggregateRuntime.class;
    }

    private KafkaAggregateRuntime createRuntime(Aggregate<S> aggregate) {
        KafkaAggregateRuntime.Builder runtimeBuilder = new KafkaAggregateRuntime.Builder();

        AggregateInfo aggregateInfo = aggregate.getClass().getAnnotation(AggregateInfo.class);

        if (aggregateInfo != null) {
            runtimeBuilder.setStateType(new AggregateStateType<>(
                    aggregateInfo.value(),
                    aggregateInfo.version(),
                    aggregate.getStateClass(),
                    aggregateInfo.generateGDPRKeyOnCreate(),
                    aggregateInfo.indexed(),
                    aggregateInfo.indexName()
            ));
        } else {
            throw new IllegalStateException("Class implementing Aggregate must be annotated with @AggregateInfo");
        }
        runtimeBuilder
                .setAggregateClass(aggregate.getClass())
                .setObjectMapper(objectMapper)
                .setGenerateGDPRKeyOnCreate(aggregateInfo.generateGDPRKeyOnCreate());

        applicationContext.getBeansOfType(CommandHandlerFunction.class).values().stream()

                .filter(adapter -> adapter.getAggregate().equals(aggregate))
                .forEach(adapter -> {
                    CommandType<?> type = adapter.getCommandType();
                    if (adapter.isCreate()) {
                        runtimeBuilder
                                .setCommandCreateHandler(adapter)
                                .addCommand(type);
                    } else {
                        runtimeBuilder
                                .addCommandHandler(type, adapter)
                                .addCommand(type);
                    }
                    for (Object producedDomainEventType : adapter.getProducedDomainEventTypes()) {
                        runtimeBuilder.addDomainEvent((DomainEventType<?>) producedDomainEventType);
                    }
                    for (Object errorEventType : adapter.getErrorEventTypes()) {
                        runtimeBuilder.addDomainEvent((DomainEventType<?>) errorEventType);
                    }
                });

        applicationContext.getBeansOfType(EventHandlerFunction.class).values().stream()

                .filter(adapter -> adapter.getAggregate().equals(aggregate))
                .forEach(adapter -> {
                    DomainEventType<?> type = adapter.getEventType();
                    if (adapter.isCreate()) {
                        runtimeBuilder
                                .setEventCreateHandler(adapter)
                                .addDomainEvent(type);
                    } else {
                        runtimeBuilder
                                .addExternalEventHandler(type, adapter)
                                .addDomainEvent(type);
                    }
                    for (Object producedDomainEventType : adapter.getProducedDomainEventTypes()) {
                        runtimeBuilder.addDomainEvent((DomainEventType<?>) producedDomainEventType);
                    }
                    for (Object errorEventType : adapter.getErrorEventTypes()) {
                        runtimeBuilder.addDomainEvent((DomainEventType<?>) errorEventType);
                    }
                });

        applicationContext.getBeansOfType(EventSourcingHandlerFunction.class).values().stream()
                .filter(adapter -> adapter.getAggregate().equals(aggregate))
                .forEach(adapter -> {
                    DomainEventType<?> type = adapter.getEventType();
                    if (adapter.isCreate()) {
                        runtimeBuilder
                                .setEventSourcingCreateHandler(adapter)
                                .addDomainEvent(type);
                    } else {
                        runtimeBuilder
                                .addEventSourcingHandler(type, adapter)
                                .addDomainEvent(type);
                    }
                });

        return runtimeBuilder.setSchemaRegistry(schemaRegistry).build();
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/kafka/KafkaAggregateRuntime.java
================
package org.elasticsoftware.akces.kafka;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import org.apache.kafka.common.errors.SerializationException;
import org.elasticsoftware.akces.aggregate.*;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.gdpr.GDPRAnnotationUtils;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.CommandRecord;
import org.elasticsoftware.akces.protocol.DomainEventRecord;
import org.elasticsoftware.akces.protocol.PayloadEncoding;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.elasticsoftware.akces.schemas.SchemaException;
import org.everit.json.schema.ValidationException;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class KafkaAggregateRuntime extends AggregateRuntimeBase {
    private final KafkaSchemaRegistry schemaRegistry;
    private final ObjectMapper objectMapper;
    private final Map<Class<? extends DomainEvent>, JsonSchema> domainEventSchemas = new HashMap<>();
    private final Map<Class<? extends Command>, JsonSchema> commandSchemas = new HashMap<>();

    private KafkaAggregateRuntime(KafkaSchemaRegistry schemaRegistry,
                                  ObjectMapper objectMapper,
                                  AggregateStateType<?> stateType,
                                  Class<? extends Aggregate> aggregateClass,
                                  CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler,
                                  EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler,
                                  EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler,
                                  Map<Class<?>, DomainEventType<?>> domainEvents,
                                  Map<String, List<CommandType<?>>> commandTypes,
                                  Map<CommandType<?>, CommandHandlerFunction<AggregateState, Command, DomainEvent>> commandHandlers,
                                  Map<DomainEventType<?>, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent>> eventHandlers,
                                  Map<DomainEventType<?>, EventSourcingHandlerFunction<AggregateState, DomainEvent>> eventSourcingHandlers,
                                  boolean generateGDPRKeyOnCreate,
                                  boolean shouldHandlePIIData) {
        super(stateType,
                aggregateClass,
                commandCreateHandler,
                eventCreateHandler,
                createStateHandler,
                domainEvents,
                commandTypes,
                commandHandlers,
                eventHandlers,
                eventSourcingHandlers,
                generateGDPRKeyOnCreate,
                shouldHandlePIIData);
        this.schemaRegistry = schemaRegistry;
        this.objectMapper = objectMapper;
    }

    @Override
    public void registerAndValidate(DomainEventType<?> domainEventType, boolean forceRegisterOnIncompatible) throws SchemaException {

        JsonSchema localSchema = schemaRegistry.registerAndValidate(domainEventType, forceRegisterOnIncompatible);

        domainEventSchemas.put(domainEventType.typeClass(), localSchema);
    }

    @Override
    public void registerAndValidate(CommandType<?> commandType,  boolean forceRegisterOnIncompatible) throws SchemaException {
        if (!commandSchemas.containsKey(commandType.typeClass())) {
            JsonSchema localSchema = schemaRegistry.registerAndValidate(commandType, forceRegisterOnIncompatible);
            commandSchemas.put(commandType.typeClass(), localSchema);
            if (commandType.external()) addCommand(commandType);
        }
    }

    @Override
    public Command materialize(CommandType<?> type, CommandRecord commandRecord) throws IOException {
        return objectMapper.readValue(commandRecord.payload(), type.typeClass());
    }

    @Override
    protected DomainEvent materialize(DomainEventType<?> domainEventType, DomainEventRecord eventRecord) throws IOException {
        return objectMapper.readValue(eventRecord.payload(), domainEventType.typeClass());
    }

    @Override
    protected AggregateState materialize(AggregateStateRecord stateRecord) throws IOException {
        return objectMapper.readValue(stateRecord.payload(), getAggregateStateType(stateRecord).typeClass());
    }

    @Override
    protected byte[] serialize(AggregateState state) throws IOException {
        return objectMapper.writeValueAsBytes(state);
    }

    @Override
    protected byte[] serialize(DomainEvent domainEvent) throws SerializationException {
        JsonNode jsonNode = objectMapper.convertValue(domainEvent, JsonNode.class);
        try {
            domainEventSchemas.get(domainEvent.getClass()).validate(jsonNode);
            return objectMapper.writeValueAsBytes(jsonNode);
        } catch (ValidationException e) {
            throw new SerializationException("Validation Failed while Serializing DomainEventClass " + domainEvent.getClass().getName(), e);
        } catch (JsonProcessingException e) {
            throw new SerializationException("Serialization Failed while Serializing DomainEventClass " + domainEvent.getClass().getName(), e);
        }
    }

    @Override
    public byte[] serialize(Command command) throws SerializationException {
        JsonNode jsonNode = objectMapper.convertValue(command, JsonNode.class);
        try {
            commandSchemas.get(command.getClass()).validate(jsonNode);
            return objectMapper.writeValueAsBytes(command);
        } catch (ValidationException e) {
            throw new SerializationException("Validation Failed while Serializing CommandClass " + command.getClass().getName(), e);
        } catch (JsonProcessingException e) {
            throw new SerializationException("Serialization Failed while Serializing CommandClass " + command.getClass().getName(), e);
        }

    }

    @Override
    protected PayloadEncoding getEncoding(CommandType<?> type) {
        return PayloadEncoding.JSON;
    }

    @Override
    protected PayloadEncoding getEncoding(DomainEventType<?> type) {
        return PayloadEncoding.JSON;
    }

    @Override
    protected PayloadEncoding getEncoding(AggregateStateType<?> type) {
        return PayloadEncoding.JSON;
    }

    public static class Builder {
        private KafkaSchemaRegistry schemaRegistry;
        private ObjectMapper objectMapper;
        private AggregateStateType<?> stateType;
        private Class<? extends Aggregate> aggregateClass;
        private CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler;
        private EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler;
        private EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler;
        private Map<Class<?>, DomainEventType<?>> domainEvents = new HashMap<>();
        private Map<String, List<CommandType<?>>> commandTypes = new HashMap<>();
        private Map<CommandType<?>, CommandHandlerFunction<AggregateState, Command, DomainEvent>> commandHandlers = new HashMap<>();
        private Map<DomainEventType<?>, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent>> eventHandlers = new HashMap<>();
        private Map<DomainEventType<?>, EventSourcingHandlerFunction<AggregateState, DomainEvent>> eventSourcingHandlers = new HashMap<>();
        private boolean generateGDPRKeyOnCreate = false;

        public Builder setSchemaRegistry(KafkaSchemaRegistry schemaRegistry) {
            this.schemaRegistry = schemaRegistry;
            return this;
        }

        public Builder setObjectMapper(ObjectMapper objectMapper) {
            this.objectMapper = objectMapper;
            return this;
        }

        public Builder setStateType(AggregateStateType<?> stateType) {
            this.stateType = stateType;
            return this;
        }

        public Builder setAggregateClass(Class<? extends Aggregate> aggregateClass) {
            this.aggregateClass = aggregateClass;
            return this;
        }

        public Builder setCommandCreateHandler(CommandHandlerFunction<AggregateState, Command, DomainEvent> commandCreateHandler) {
            this.commandCreateHandler = commandCreateHandler;
            return this;
        }

        public Builder setEventCreateHandler(EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventCreateHandler) {
            this.eventCreateHandler = eventCreateHandler;
            return this;
        }

        public Builder setEventSourcingCreateHandler(EventSourcingHandlerFunction<AggregateState, DomainEvent> createStateHandler) {
            this.createStateHandler = createStateHandler;
            return this;
        }

        public Builder addDomainEvent(DomainEventType<?> domainEvent) {
            this.domainEvents.put(domainEvent.typeClass(), domainEvent);
            return this;
        }

        public Builder addCommand(CommandType<?> commandType) {
            this.commandTypes.computeIfAbsent(commandType.typeName(), s -> new ArrayList<>()).add(commandType);
            return this;
        }

        public Builder addCommandHandler(CommandType<?> commandType, CommandHandlerFunction<AggregateState, Command, DomainEvent> commandHandler) {
            this.commandHandlers.put(commandType, commandHandler);
            return this;
        }

        public Builder addExternalEventHandler(DomainEventType<?> eventType, EventHandlerFunction<AggregateState, DomainEvent, DomainEvent> eventHandler) {
            this.eventHandlers.put(eventType, eventHandler);
            return this;
        }

        public Builder addEventSourcingHandler(DomainEventType<?> eventType, EventSourcingHandlerFunction<AggregateState, DomainEvent> eventSourcingHandler) {
            this.eventSourcingHandlers.put(eventType, eventSourcingHandler);
            return this;
        }

        public Builder setGenerateGDPRKeyOnCreate(boolean generateGDPRKeyOnCreate) {
            this.generateGDPRKeyOnCreate = generateGDPRKeyOnCreate;
            return this;
        }

        public KafkaAggregateRuntime build() {

            final boolean shouldHandlePIIData = domainEvents.values().stream().map(DomainEventType::typeClass)
                    .anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation) ||
                    commandTypes.values().stream().flatMap(List::stream).map(CommandType::typeClass)
                            .anyMatch(GDPRAnnotationUtils::hasPIIDataAnnotation);

            return new KafkaAggregateRuntime(
                    schemaRegistry,
                    objectMapper,
                    stateType,
                    aggregateClass,
                    commandCreateHandler,
                    eventCreateHandler,
                    createStateHandler,
                    domainEvents,
                    commandTypes,
                    commandHandlers,
                    eventHandlers,
                    eventSourcingHandlers,
                    generateGDPRKeyOnCreate,
                    shouldHandlePIIData);
        }
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/kafka/PartitionUtils.java
================
package org.elasticsoftware.akces.kafka;

import org.apache.kafka.common.TopicPartition;
import org.elasticsoftware.akces.aggregate.AggregateRuntime;

import java.util.*;
import java.util.stream.Collectors;

public final class PartitionUtils {
    public static final String COMMANDS_SUFFIX = "-Commands";
    public static final String DOMAINEVENTS_SUFFIX = "-DomainEvents";
    public static final String AGGREGRATESTATE_SUFFIX = "-AggregateState";

    private PartitionUtils() {
    }

    public static Map<Integer, AggregatePartition> toAggregatePartitions(Collection<TopicPartition> topicPartitions) {
        Set<Integer> partitions = topicPartitions.stream().map(TopicPartition::partition).collect(Collectors.toSet());
        Map<Integer, AggregatePartition> aggregatePartitions = new HashMap<>();
        partitions.forEach(partition -> {
            Set<TopicPartition> aggregatePartition = topicPartitions.stream().filter(topicPartition ->
                    topicPartition.partition() == partition &&
                            (topicPartition.topic().endsWith(COMMANDS_SUFFIX) ||
                                    topicPartition.topic().endsWith(DOMAINEVENTS_SUFFIX) ||
                                    topicPartition.topic().endsWith(AGGREGRATESTATE_SUFFIX))).collect(Collectors.toSet());
            if (aggregatePartition.size() == 3) {
                TopicPartition command = aggregatePartition.stream().filter(topicPartition ->
                        topicPartition.topic().endsWith(COMMANDS_SUFFIX)).findFirst().orElseThrow();
                TopicPartition domainEvent = aggregatePartition.stream().filter(topicPartition ->
                        topicPartition.topic().endsWith(DOMAINEVENTS_SUFFIX)).findFirst().orElseThrow();
                TopicPartition aggregateState = aggregatePartition.stream().filter(topicPartition ->
                        topicPartition.topic().endsWith(AGGREGRATESTATE_SUFFIX)).findFirst().orElseThrow();

            } else {
                throw new NoSuchElementException("Partition " + partition + " is incomplete, found " + aggregatePartition);
            }
        });
        return aggregatePartitions;
    }

    public static TopicPartition toCommandTopicPartition(AggregateRuntime aggregate, int partition) {
        return new TopicPartition(aggregate.getName() + COMMANDS_SUFFIX, partition);
    }

    public static TopicPartition toDomainEventTopicPartition(AggregateRuntime aggregate, int partition) {
        return new TopicPartition(aggregate.getName() + DOMAINEVENTS_SUFFIX, partition);
    }

    public static TopicPartition toAggregateStateTopicPartition(AggregateRuntime aggregate, int partition) {
        return new TopicPartition(aggregate.getName() + AGGREGRATESTATE_SUFFIX, partition);
    }

    public static TopicPartition toGDPRKeysTopicPartition(AggregateRuntime aggregate, int partition) {
        return new TopicPartition("Akces-GDPRKeys", partition);
    }

    public static List<TopicPartition> toExternalDomainEventTopicPartitions(AggregateRuntime aggregate, int partition) {
        return aggregate.getExternalDomainEventTypes().stream().map(externalDomainEvent ->
                new TopicPartition(externalDomainEvent.typeName() + DOMAINEVENTS_SUFFIX, partition)).collect(Collectors.toList());
    }

    public static TopicPartition parseReplyToTopicPartition(String replyTo) {

        int lastIndex = replyTo.lastIndexOf('-');
        String topic = replyTo.substring(0, lastIndex);
        int partition = Integer.parseInt(replyTo.substring(lastIndex + 1));
        return new TopicPartition(topic, partition);
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepository.java
================
package org.elasticsoftware.akces.state;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.requests.ProduceResponse;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;

import java.io.Closeable;
import java.util.List;
import java.util.concurrent.Future;

public interface AggregateStateRepository extends Closeable {
    default long getOffset() {
        return ProduceResponse.INVALID_OFFSET;
    }

    void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture);

    void commit();

    void rollback();

    void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords);

    AggregateStateRecord get(String aggregateId);
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepositoryException.java
================
package org.elasticsoftware.akces.state;

public class AggregateStateRepositoryException extends RuntimeException {

    public AggregateStateRepositoryException(String message, Throwable cause) {
        super(message, cause);
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/AggregateStateRepositoryFactory.java
================
package org.elasticsoftware.akces.state;

import org.elasticsoftware.akces.aggregate.AggregateRuntime;

public interface AggregateStateRepositoryFactory {
    AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId);
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/InMemoryAggregateStateRepository.java
================
package org.elasticsoftware.akces.state;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.requests.ProduceResponse;
import org.elasticsoftware.akces.kafka.RecordAndMetadata;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.Future;

public class InMemoryAggregateStateRepository implements AggregateStateRepository {
    private static final Logger log = LoggerFactory.getLogger(InMemoryAggregateStateRepository.class);
    private final Map<String, AggregateStateRecord> stateRecordMap = new HashMap<>();
    private final Map<String, RecordAndMetadata<AggregateStateRecord>> transactionStateRecordMap = new HashMap<>();
    private long offset = -1L;

    @Override
    public void close() {
        stateRecordMap.clear();
        transactionStateRecordMap.clear();
    }

    @Override
    public void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture) {
        transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata(record, recordMetadataFuture));
    }

    @Override
    public void commit() {

        if (!transactionStateRecordMap.isEmpty()) {

            this.offset = transactionStateRecordMap.values().stream()
                    .map(RecordAndMetadata::metadata)
                    .map(recordMetadataFuture -> {
                        try {
                            return recordMetadataFuture.get();
                        } catch (Exception e) {
                            log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
                            return null;
                        }
                    })
                    .map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
                    .max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
            log.trace("Committing {} records and offset {}", transactionStateRecordMap.size(), this.offset);
            transactionStateRecordMap.values().forEach(recordAndMetadata -> stateRecordMap.put(recordAndMetadata.record().aggregateId(), recordAndMetadata.record()));
            transactionStateRecordMap.clear();
        }
    }

    @Override
    public void rollback() {
        transactionStateRecordMap.clear();
    }

    @Override
    public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
        for (ConsumerRecord<String, ProtocolRecord> consumerRecord : consumerRecords) {
            AggregateStateRecord record = (AggregateStateRecord) consumerRecord.value();
            if (record != null) {
                stateRecordMap.put(record.aggregateId(), record);
            } else {
                stateRecordMap.remove(consumerRecord.key());
            }
            this.offset = consumerRecord.offset();
        }
    }

    @Override
    public AggregateStateRecord get(String aggregateId) {

        if (transactionStateRecordMap.containsKey(aggregateId)) {
            return transactionStateRecordMap.get(aggregateId).record();
        } else {
            return stateRecordMap.get(aggregateId);
        }
    }

    @Override
    public long getOffset() {
        return offset;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/InMemoryAggregateStateRepositoryFactory.java
================
package org.elasticsoftware.akces.state;

import org.elasticsoftware.akces.aggregate.AggregateRuntime;

public class InMemoryAggregateStateRepositoryFactory implements AggregateStateRepositoryFactory {
    @Override
    public AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId) {
        return new InMemoryAggregateStateRepository();
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/RocksDBAggregateStateRepository.java
================
package org.elasticsoftware.akces.state;

import com.google.common.base.Charsets;
import com.google.common.primitives.Longs;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.errors.SerializationException;
import org.apache.kafka.common.requests.ProduceResponse;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serializer;
import org.elasticsoftware.akces.kafka.RecordAndMetadata;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.rocksdb.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.file.Files;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.concurrent.Future;

public class RocksDBAggregateStateRepository implements AggregateStateRepository {
    private static final Logger log = LoggerFactory.getLogger(RocksDBAggregateStateRepository.class);

    private static final byte[] OFFSET = new byte[]{0x4f, 0x46, 0x46, 0x53, 0x45, 0x54};
    private final TransactionDB db;
    private final File rocksDBDataDir;
    private final Map<String, RecordAndMetadata<AggregateStateRecord>> transactionStateRecordMap = new HashMap<>();
    private final String topicName;
    private final Serializer<ProtocolRecord> serializer;
    private final Deserializer<ProtocolRecord> deserializer;
    private boolean aggregateIdIsUUID = false;
    private boolean aggregateIdTypeCheckDone = false;
    private long lastOffset = ProduceResponse.INVALID_OFFSET;

    public RocksDBAggregateStateRepository(String baseDir,
                                           String partitionId,
                                           String topicName,
                                           Serializer<ProtocolRecord> serializer,
                                           Deserializer<ProtocolRecord> deserializer) {
        this.topicName = topicName;
        this.serializer = serializer;
        this.deserializer = deserializer;
        RocksDB.loadLibrary();
        final Options options = new Options();
        final TransactionDBOptions transactionDBOptions = new TransactionDBOptions();
        options.setCreateIfMissing(true);


        this.rocksDBDataDir = new File(baseDir, partitionId);
        try {
            Files.createDirectories(this.rocksDBDataDir.getParentFile().toPath());
            Files.createDirectories(this.rocksDBDataDir.getAbsoluteFile().toPath());
            db = TransactionDB.open(options, transactionDBOptions, this.rocksDBDataDir.getAbsolutePath());


            initializeOffset();
            log.info("RocksDB for partition {} initialized in folder {}", partitionId, this.rocksDBDataDir.getAbsolutePath());
        } catch (IOException | RocksDBException e) {
            throw new AggregateStateRepositoryException("Error initializing RocksDB", e);
        }
    }

    @Override
    public void close() {
        try {
            db.syncWal();
        } catch (RocksDBException e) {
            log.error("Error syncing WAL. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
        }
        db.close();
    }

    private void initializeOffset() {
        try {
            byte[] offsetBytes = db.get(OFFSET);
            if (offsetBytes != null) {
                lastOffset = Longs.fromByteArray(offsetBytes);
            }
        } catch (RocksDBException e) {
            throw new AggregateStateRepositoryException("Error initializing offset", e);
        }
    }

    private void updateOffset(long offset) {
        this.lastOffset = offset;
        log.trace("Updated offset to {}", offset);
    }

    @Override
    public long getOffset() {
        return lastOffset;
    }

    @Override
    public void prepare(AggregateStateRecord record, Future<RecordMetadata> recordMetadataFuture) {
        checkAggregateIdType(record.aggregateId());
        transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata(record, recordMetadataFuture));
    }

    @Override
    public void commit() {
        if (!transactionStateRecordMap.isEmpty()) {

            Transaction transaction = db.beginTransaction(new WriteOptions());
            try {
                for (RecordAndMetadata recordAndMetadata : transactionStateRecordMap.values()) {
                    transaction.put(keyBytes(recordAndMetadata.record().aggregateId()), serializer.serialize(topicName, recordAndMetadata.record()));
                }

                long offset = transactionStateRecordMap.values().stream()
                        .map(RecordAndMetadata::metadata)
                        .map(recordMetadataFuture -> {
                            try {
                                return recordMetadataFuture.get();
                            } catch (Exception e) {
                                log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
                                return null;
                            }
                        })
                        .map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
                        .max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
                transaction.put(OFFSET, Longs.toByteArray(offset));
                transaction.commit();
                transaction.close();
                updateOffset(offset);
            } catch (RocksDBException e) {
                throw new AggregateStateRepositoryException("Error committing records", e);
            } finally {
                transactionStateRecordMap.clear();
            }
        }
    }

    @Override
    public void rollback() {
        transactionStateRecordMap.clear();
    }

    @Override
    public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {

        long offset = consumerRecords.stream()
                .map(ConsumerRecord::offset)
                .max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
        if (offset > lastOffset) {
            Transaction transaction = db.beginTransaction(new WriteOptions());
            try {
                for (ConsumerRecord<String, ProtocolRecord> consumerRecord : consumerRecords) {
                    transaction.put(keyBytes(consumerRecord.key()), serializer.serialize(topicName, consumerRecord.value()));
                }
                transaction.put(OFFSET, Longs.toByteArray(offset));
                transaction.commit();
                transaction.close();
                updateOffset(offset);
            } catch (RocksDBException e) {
                throw new AggregateStateRepositoryException("Error processing records", e);
            }
        }
    }

    @Override
    public AggregateStateRecord get(String aggregateId) {
        checkAggregateIdType(aggregateId);

        if (transactionStateRecordMap.containsKey(aggregateId)) {
            return transactionStateRecordMap.get(aggregateId).record();
        } else {
            byte[] keyBytes = keyBytes(aggregateId);
            if (db.keyExists(keyBytes)) {
                try {
                    return (AggregateStateRecord) deserializer.deserialize(topicName, db.get(keyBytes));
                } catch (RocksDBException | SerializationException e) {
                    throw new AggregateStateRepositoryException("Problem reading record with aggregateId " + aggregateId, e);
                }
            } else {
                return null;
            }
        }
    }

    private byte[] keyBytes(String aggregateId) {
        checkAggregateIdType(aggregateId);
        if (aggregateIdIsUUID) {
            UUID aggregateUUID = UUID.fromString(aggregateId);
            return ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array();
        } else
            return aggregateId.getBytes(Charsets.UTF_8);
    }

    private void checkAggregateIdType(String aggregateId) {
        if (!aggregateIdTypeCheckDone) {
            try {
                UUID.fromString(aggregateId);
                aggregateIdIsUUID = true;
            } catch (IllegalArgumentException e) {
                aggregateIdIsUUID = false;
            }
            aggregateIdTypeCheckDone = true;
        }
    }

}

================
File: runtime/src/main/java/org/elasticsoftware/akces/state/RocksDBAggregateStateRepositoryFactory.java
================
package org.elasticsoftware.akces.state;

import org.elasticsoftware.akces.aggregate.AggregateRuntime;
import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;

public class RocksDBAggregateStateRepositoryFactory implements AggregateStateRepositoryFactory {
    private final ProtocolRecordSerde serde;
    private final String baseDir;

    public RocksDBAggregateStateRepositoryFactory(ProtocolRecordSerde serde, String baseDir) {
        this.serde = serde;
        this.baseDir = baseDir;
    }

    @Override
    public AggregateStateRepository create(AggregateRuntime aggregateRuntime, Integer partitionId) {
        return new RocksDBAggregateStateRepository(
                baseDir,
                aggregateRuntime.getName() + "-AggregateState-" + partitionId.toString(),
                aggregateRuntime.getName() + "-AggregateState",
                serde.serializer(),
                serde.deserializer());
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/AggregateServiceApplication.java
================
package org.elasticsoftware.akces;

import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.json.JsonSchemaProvider;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.elasticsoftware.akces.beans.AggregateBeanFactoryPostProcessor;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.gdpr.GDPRContextRepositoryFactory;
import org.elasticsoftware.akces.gdpr.RocksDBGDPRContextRepositoryFactory;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.kafka.CustomKafkaConsumerFactory;
import org.elasticsoftware.akces.kafka.CustomKafkaProducerFactory;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;
import org.elasticsoftware.akces.state.AggregateStateRepositoryFactory;
import org.elasticsoftware.akces.state.RocksDBAggregateStateRepositoryFactory;
import org.elasticsoftware.akces.util.EnvironmentPropertiesPrinter;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.autoconfigure.jackson.Jackson2ObjectMapperBuilderCustomizer;
import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.PropertySource;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdmin;
import org.springframework.kafka.core.ProducerFactory;

import java.math.BigDecimal;
import java.util.List;
import java.util.Map;
import java.util.Set;

@SpringBootApplication(exclude = KafkaAutoConfiguration.class)
@EnableConfigurationProperties(KafkaProperties.class)
@PropertySource("classpath:akces-aggregateservice.properties")
public class AggregateServiceApplication {
    private final ProtocolRecordSerde serde = new ProtocolRecordSerde();

    public static void main(String[] args) {
        SpringApplication application = new SpringApplication(AggregateServiceApplication.class);
        if (args.length > 0) {

            application.setSources(Set.of(args));
        }
        application.run();
    }

    @Bean(name = "aggregateServiceBeanFactoryPostProcessor")
    public static AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
        return new AggregateBeanFactoryPostProcessor();
    }

    @Bean(name = "aggregateServiceJsonCustomizer")
    public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
        return builder -> {
            builder.modulesToInstall(new AkcesGDPRModule());
            builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        };
    }

    @Bean(name = "aggregateServiceControlRecordSerde")
    public AkcesControlRecordSerde akcesControlRecordSerde(ObjectMapper objectMapper) {
        return new AkcesControlRecordSerde(objectMapper);
    }

    @Bean(name = "aggregateServiceKafkaAdmin")
    public KafkaAdmin kafkaAdmin(@Value("${spring.kafka.bootstrap-servers}") String bootstrapServers) {
        return new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
    }

    @Bean(name = "aggregateServiceSchemaRegistryClient")
    public SchemaRegistryClient schemaRegistryClient(@Value("${akces.schemaregistry.url:http://localhost:8081}") String url) {
        return new CachedSchemaRegistryClient(url, 1000, List.of(new JsonSchemaProvider()), null);
    }

    @Bean(name = "aggregateServiceSchemaRegistry")
    public KafkaSchemaRegistry schemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient schemaRegistryClient,
                                              ObjectMapper objectMapper) {
        return new KafkaSchemaRegistry(schemaRegistryClient, objectMapper);
    }

    @Bean(name = "aggregateServiceConsumerFactory")
    public ConsumerFactory<String, ProtocolRecord> consumerFactory(KafkaProperties properties) {
        return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), serde.deserializer());
    }

    @Bean(name = "aggregateServiceProducerFactory")
    public ProducerFactory<String, ProtocolRecord> producerFactory(KafkaProperties properties) {
        return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), serde.serializer());
    }

    @Bean(name = "aggregateServiceControlConsumerFactory")
    public ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory(KafkaProperties properties,
                                                                              @Qualifier("aggregateServiceControlRecordSerde") AkcesControlRecordSerde controlSerde) {
        return new CustomKafkaConsumerFactory<>(properties.buildConsumerProperties(null), new StringDeserializer(), controlSerde.deserializer());
    }

    @Bean(name = "aggregateServiceControlProducerFactory")
    public ProducerFactory<String, AkcesControlRecord> controlProducerFactory(KafkaProperties properties,
                                                                              @Qualifier("aggregateServiceControlRecordSerde") AkcesControlRecordSerde controlSerde) {
        return new CustomKafkaProducerFactory<>(properties.buildProducerProperties(null), new StringSerializer(), controlSerde.serializer());
    }

    @Bean(name = "aggregateServiceAggregateStateRepositoryFactory")
    public AggregateStateRepositoryFactory aggregateStateRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
        return new RocksDBAggregateStateRepositoryFactory(serde, baseDir);
    }

    @Bean(name = "aggregateServiceGDPRContextRepositoryFactory")
    public GDPRContextRepositoryFactory gdprContextRepositoryFactory(@Value("${akces.rocksdb.baseDir:/tmp/akces}") String baseDir) {
        return new RocksDBGDPRContextRepositoryFactory(serde, baseDir);
    }

    @Bean(name = "environmentPropertiesPrinter")
    public EnvironmentPropertiesPrinter environmentPropertiesPrinter() {
        return new EnvironmentPropertiesPrinter();
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/AkcesAggregateController.java
================
package org.elasticsoftware.akces;

import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;
import jakarta.annotation.Nonnull;
import org.apache.kafka.clients.admin.TopicDescription;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.InterruptException;
import org.apache.kafka.common.errors.WakeupException;
import org.elasticsoftware.akces.aggregate.AggregateRuntime;
import org.elasticsoftware.akces.aggregate.CommandType;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.aggregate.SchemaType;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.control.*;
import org.elasticsoftware.akces.gdpr.GDPRContextRepositoryFactory;
import org.elasticsoftware.akces.kafka.AggregatePartition;
import org.elasticsoftware.akces.kafka.PartitionUtils;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.schemas.IncompatibleSchemaException;
import org.elasticsoftware.akces.schemas.SchemaException;
import org.elasticsoftware.akces.state.AggregateStateRepositoryFactory;
import org.elasticsoftware.akces.util.HostUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.BeansException;
import org.springframework.boot.availability.AvailabilityChangeEvent;
import org.springframework.boot.availability.LivenessState;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;
import org.springframework.context.EnvironmentAware;
import org.springframework.core.env.Environment;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdminOperations;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.scheduling.concurrent.CustomizableThreadFactory;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.*;
import java.util.function.BiFunction;
import java.util.stream.IntStream;

import static java.nio.charset.StandardCharsets.UTF_8;
import static org.elasticsoftware.akces.AkcesControllerState.*;
import static org.elasticsoftware.akces.kafka.PartitionUtils.*;
import static org.elasticsoftware.akces.util.KafkaUtils.createCompactedTopic;
import static org.elasticsoftware.akces.util.KafkaUtils.getIndexTopicName;

public class AkcesAggregateController extends Thread implements AutoCloseable, ConsumerRebalanceListener, AkcesRegistry, EnvironmentAware, ApplicationContextAware {
    private static final Logger logger = LoggerFactory.getLogger(AkcesAggregateController.class);
    private final ConsumerFactory<String, ProtocolRecord> consumerFactory;
    private final ProducerFactory<String, ProtocolRecord> producerFactory;
    private final ProducerFactory<String, AkcesControlRecord> controlProducerFactory;
    private final ConsumerFactory<String, AkcesControlRecord> controlRecordConsumerFactory;
    private final AggregateRuntime aggregateRuntime;
    private final KafkaAdminOperations kafkaAdmin;
    private final Map<Integer, AggregatePartition> aggregatePartitions = new HashMap<>();
    private final ExecutorService executorService;
    private final HashFunction hashFunction = Hashing.murmur3_32_fixed();
    private final Map<String, AggregateServiceRecord> aggregateServices = new ConcurrentHashMap<>();
    private final AggregateStateRepositoryFactory aggregateStateRepositoryFactory;
    private final GDPRContextRepositoryFactory gdprContextRepositoryFactory;
    private final List<TopicPartition> partitionsToAssign = new ArrayList<>();
    private final List<TopicPartition> partitionsToRevoke = new ArrayList<>();
    private final CountDownLatch shutdownLatch = new CountDownLatch(1);
    private Integer partitions = null;
    private Short replicationFactor = null;
    private Consumer<String, AkcesControlRecord> controlConsumer;
    private volatile AkcesControllerState processState = INITIALIZING;
    private boolean forceRegisterOnIncompatible = false;
    private ApplicationContext applicationContext;

    public AkcesAggregateController(ConsumerFactory<String, ProtocolRecord> consumerFactory,
                                    ProducerFactory<String, ProtocolRecord> producerFactory,
                                    ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory,
                                    ProducerFactory<String, AkcesControlRecord> controlProducerFactory,
                                    AggregateStateRepositoryFactory aggregateStateRepositoryFactory,
                                    GDPRContextRepositoryFactory gdprContextRepositoryFactory,
                                    AggregateRuntime aggregateRuntime,
                                    KafkaAdminOperations kafkaAdmin) {
        super(aggregateRuntime.getName() + "-AkcesController");
        this.consumerFactory = consumerFactory;
        this.producerFactory = producerFactory;
        this.controlProducerFactory = controlProducerFactory;
        this.controlRecordConsumerFactory = controlConsumerFactory;
        this.aggregateStateRepositoryFactory = aggregateStateRepositoryFactory;
        this.gdprContextRepositoryFactory = gdprContextRepositoryFactory;
        this.aggregateRuntime = aggregateRuntime;
        this.kafkaAdmin = kafkaAdmin;
        this.executorService = Executors.newCachedThreadPool(new CustomizableThreadFactory(aggregateRuntime.getName() + "AggregatePartitionThread-"));
    }

    @Override
    public void run() {

        try {

            controlConsumer =
                    controlRecordConsumerFactory.createConsumer(
                            aggregateRuntime.getName() + "-Akces-Control",
                            aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-Akces-Control",
                            null);
            controlConsumer.subscribe(List.of("Akces-Control"), this);
            while (processState != SHUTTING_DOWN) {
                process();
            }


            logger.info("Closing {} AggregatePartitions", aggregatePartitions.size());
            aggregatePartitions.values().forEach(aggregatePartition -> {
                if (aggregatePartition != null) {
                    try {
                        aggregatePartition.close();
                    } catch (Exception e) {
                        logger.error("Error closing AggregatePartition " + aggregatePartition.getId(), e);
                    }
                }
            });
            try {
                controlConsumer.close(Duration.ofSeconds(5));
            } catch (InterruptException e) {

            } catch (KafkaException e) {
                logger.error("Error closing controlConsumer", e);
            }

            applicationContext.publishEvent(new AvailabilityChangeEvent<>(this, LivenessState.BROKEN));

            shutdownLatch.countDown();
        } catch (Exception e) {
            logger.error("Error in AkcesController", e);
            processState = ERROR;
        }
    }

    private void process() {
        if (processState == RUNNING) {
            try {
                processControlRecords();
            } catch (WakeupException | InterruptException e) {

            } catch (KafkaException e) {

                logger.error("Unrecoverable exception in AkcesController", e);

                processState = SHUTTING_DOWN;
            }
        } else if (processState == INITIALIZING) {

            TopicDescription controlTopicDescription = kafkaAdmin.describeTopics("Akces-Control").get("Akces-Control");
            partitions = controlTopicDescription.partitions().size();
            replicationFactor = (short) controlTopicDescription.partitions().getFirst().replicas().size();

            for (DomainEventType<?> domainEventType : aggregateRuntime.getProducedDomainEventTypes()) {
                try {
                    aggregateRuntime.registerAndValidate(domainEventType);
                } catch (IncompatibleSchemaException e) {


                    if (forceRegisterOnIncompatible) {
                        if (protocolRecordTypeNotYetProduced(domainEventType, PartitionUtils::toDomainEventTopicPartition)) {
                            aggregateRuntime.registerAndValidate(domainEventType, true);
                        } else {
                            logger.warn("Cannot update schema for DomainEvent {} v{} because it has already been produced",
                                    domainEventType.typeName(), domainEventType.version());
                            throw e;
                        }
                    } else {
                        throw e;
                    }
                }
            }

            for (CommandType<?> commandType : aggregateRuntime.getLocalCommandTypes()) {
                try {
                    aggregateRuntime.registerAndValidate(commandType);
                } catch (IncompatibleSchemaException e) {


                    if (forceRegisterOnIncompatible) {
                        if (protocolRecordTypeNotYetProduced(commandType, PartitionUtils::toCommandTopicPartition)) {
                            aggregateRuntime.registerAndValidate(commandType, true);
                        } else {
                            logger.warn("Cannot update schema for Command {} v{} because it has already been produced",
                                    commandType.typeName(), commandType.version());
                            throw e;
                        }
                    } else {
                        throw e;
                    }
                }
            }

            publishControlRecord(partitions);
            processControlRecords();


        } else if (processState == INITIAL_REBALANCING) {

            if (!partitionsToAssign.isEmpty()) {
                try {

                    controlConsumer.seekToBeginning(partitionsToAssign);

                    Map<TopicPartition, Long> initializedEndOffsets = controlConsumer.endOffsets(partitionsToAssign);



                    ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
                    while (!initializedEndOffsets.isEmpty()) {
                        consumerRecords.forEach(record -> {
                            AkcesControlRecord controlRecord = record.value();
                            if (controlRecord instanceof AggregateServiceRecord aggregateServiceRecord) {

                                if (aggregateServices.putIfAbsent(record.key(), aggregateServiceRecord) == null) {
                                    logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
                                }
                            } else {
                                logger.info("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
                            }
                        });

                        if (consumerRecords.isEmpty()) {
                            initializedEndOffsets.entrySet().removeIf(entry -> entry.getValue() <= controlConsumer.position(entry.getKey()));
                        }

                        consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
                    }
                } catch (WakeupException | InterruptException e) {

                } catch (KafkaException e) {

                    logger.error("Unrecoverable exception in AkcesController", e);

                    processState = SHUTTING_DOWN;
                }


                for (DomainEventType<?> domainEventType : aggregateRuntime.getExternalDomainEventTypes()) {
                    try {
                        aggregateRuntime.registerAndValidate(domainEventType);
                    } catch (SchemaException e) {
                        logger.error("Error registering external domain event type: {}:{}", domainEventType.typeName(), domainEventType.version(), e);
                        processState = SHUTTING_DOWN;
                    }
                }

                for (CommandType<?> commandType : aggregateRuntime.getExternalCommandTypes()) {
                    try {
                        aggregateRuntime.registerAndValidate(commandType);
                    } catch (SchemaException e) {
                        logger.error("Error registering external command type: {}:{}", commandType.typeName(), commandType.version(), e);
                        processState = SHUTTING_DOWN;
                    }
                }
            }

            processState = REBALANCING;
        } else if (processState == REBALANCING) {

            for (TopicPartition topicPartition : partitionsToRevoke) {
                AggregatePartition aggregatePartition = aggregatePartitions.remove(topicPartition.partition());
                if (aggregatePartition != null) {
                    logger.info("Stopping AggregatePartition {}", aggregatePartition.getId());
                    try {
                        aggregatePartition.close();
                    } catch (Exception e) {
                        logger.error("Error closing AggregatePartition", e);
                    }
                }
            }
            partitionsToRevoke.clear();

            for (TopicPartition topicPartition : partitionsToAssign) {
                AggregatePartition aggregatePartition = new AggregatePartition(
                        consumerFactory,
                        producerFactory,
                        aggregateRuntime,
                        aggregateStateRepositoryFactory,
                        gdprContextRepositoryFactory,
                        topicPartition.partition(),
                        toCommandTopicPartition(aggregateRuntime, topicPartition.partition()),
                        toDomainEventTopicPartition(aggregateRuntime, topicPartition.partition()),
                        toAggregateStateTopicPartition(aggregateRuntime, topicPartition.partition()),
                        toGDPRKeysTopicPartition(aggregateRuntime, topicPartition.partition()),
                        aggregateRuntime.getExternalDomainEventTypes(),
                        this,
                        this::createIndexTopic);
                aggregatePartitions.put(aggregatePartition.getId(), aggregatePartition);
                logger.info("Starting AggregatePartition {}", aggregatePartition.getId());
                executorService.submit(aggregatePartition);
            }
            partitionsToAssign.clear();

            processState = RUNNING;
        }
    }

    private boolean protocolRecordTypeNotYetProduced(SchemaType schemaType,
                                                     BiFunction<AggregateRuntime, Integer, TopicPartition> createTopicPartition) {
        try (Consumer<String, ProtocolRecord> consumer = consumerFactory.createConsumer(
                aggregateRuntime.getName() + "-Akces-Control-TypeCheck",
                aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-Akces-Control-TypeCheck",
                null)) {
            List<TopicPartition> partitionsList = IntStream.range(0, partitions)
                    .mapToObj(i -> createTopicPartition.apply(aggregateRuntime, i))
                    .toList();
            consumer.assign(partitionsList);
            consumer.seekToBeginning(partitionsList);

            Map<TopicPartition, Long> endOffsets = consumer.endOffsets(partitionsList);
            while (!endOffsets.isEmpty()) {
                try {
                    for (ConsumerRecord<String, ProtocolRecord> record : consumer.poll(Duration.ofMillis(10))) {
                        if (record.value().name().equals(schemaType.typeName()) &&
                                record.value().version() == schemaType.version()) {

                            return false;
                        }
                    }

                    endOffsets.entrySet().removeIf(entry -> entry.getValue() <= consumer.position(entry.getKey()));
                } catch (WakeupException | InterruptException e) {

                } catch (KafkaException e) {

                    logger.error(
                            "KafkaException while checking if ProtocolRecord {} v{} has already been produced",
                            schemaType.typeName(),
                            schemaType.version(),
                            e);

                    return false;
                }
            }
        } catch (KafkaException e) {

            logger.error(
                    "KafkaException while checking if ProtocolRecord {} v{} has already been produced",
                    schemaType.typeName(),
                    schemaType.version(),
                    e);

            return false;
        }

        return true;
    }

    private void processControlRecords() {



        ConsumerRecords<String, AkcesControlRecord> consumerRecords = controlConsumer.poll(Duration.ofMillis(100));
        if (!consumerRecords.isEmpty()) {
            consumerRecords.forEach(record -> {
                AkcesControlRecord controlRecord = record.value();
                if (controlRecord instanceof AggregateServiceRecord aggregateServiceRecord) {
                    if (!aggregateServices.containsKey(record.key())) {
                        logger.info("Discovered service: {}", aggregateServiceRecord.aggregateName());
                    }

                    aggregateServices.put(record.key(), aggregateServiceRecord);
                } else {
                    logger.info("Received unknown AkcesControlRecord type: {}", controlRecord.getClass().getSimpleName());
                }
            });
        }
    }

    private Boolean createIndexTopic(String indexName, String indexKey) {
        try {
            kafkaAdmin.createOrModifyTopics(
                    createCompactedTopic(getIndexTopicName(indexName, indexKey), 1, replicationFactor));
            return true;
        } catch (Exception e) {
            logger.error("Error creating index topic: {}", indexName, e);
            return false;
        }
    }

    private void publishControlRecord(int partitions) {
        String transactionalId = aggregateRuntime.getName() + "-" + HostUtils.getHostName() + "-control";
        try (Producer<String, AkcesControlRecord> controlProducer = controlProducerFactory.createProducer(transactionalId)) {

            AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
                    aggregateRuntime.getName(),
                    aggregateRuntime.getName() + COMMANDS_SUFFIX,
                    aggregateRuntime.getName() + DOMAINEVENTS_SUFFIX,
                    aggregateRuntime.getAllCommandTypes().stream()
                            .map(commandType ->
                                    new AggregateServiceCommandType(
                                            commandType.typeName(),
                                            commandType.version(),
                                            commandType.create(),
                                            "commands." + commandType.typeName())).toList(),
                    aggregateRuntime.getProducedDomainEventTypes().stream().map(domainEventType ->
                            new AggregateServiceDomainEventType(
                                    domainEventType.typeName(),
                                    domainEventType.version(),
                                    domainEventType.create(),
                                    domainEventType.external(),
                                    "domainevents." + domainEventType.typeName())).toList(),
                    aggregateRuntime.getExternalDomainEventTypes().stream().map(externalDomainEventType ->
                            new AggregateServiceDomainEventType(
                                    externalDomainEventType.typeName(),
                                    externalDomainEventType.version(),
                                    externalDomainEventType.create(),
                                    externalDomainEventType.external(),
                                    "domainevents." + externalDomainEventType.typeName())).toList());
            controlProducer.beginTransaction();
            for (int partition = 0; partition < partitions; partition++) {
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, aggregateRuntime.getName(), aggregateServiceRecord));
            }
            controlProducer.commitTransaction();
        } catch (Exception e) {
            logger.error("Error publishing CommandServiceRecord", e);
        }
    }

    @Override
    public void close() throws Exception {
        logger.info("Shutting down AkcesAggregateController");
        this.processState = SHUTTING_DOWN;

        try {
            if (shutdownLatch.await(10, TimeUnit.SECONDS)) {
                logger.info("AkcesAggregateController has been shutdown");
            } else {
                logger.warn("AkcesAggregateController did not shutdown within 10 seconds");
            }
        } catch (InterruptedException e) {

        }
    }

    @Override
    public void onPartitionsRevoked(Collection<TopicPartition> topicPartitions) {

        if (!topicPartitions.isEmpty()) {

            partitionsToRevoke.addAll(topicPartitions);

            if (processState == RUNNING) {
                logger.info("Switching from RUNNING to REBALANCING, revoking partitions: {}",
                        topicPartitions.stream().map(TopicPartition::partition).toList());
                processState = REBALANCING;
            } else if (processState == INITIALIZING) {
                logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, revoking partitions: {}",
                        topicPartitions.stream().map(TopicPartition::partition).toList());
                processState = INITIAL_REBALANCING;
            }
        }
    }

    @Override
    public void onPartitionsAssigned(Collection<TopicPartition> topicPartitions) {
        if (!topicPartitions.isEmpty()) {

            partitionsToAssign.addAll(topicPartitions);

            if (processState == RUNNING) {
                logger.info("Switching from RUNNING to REBALANCING, assigning partitions : {}",
                        topicPartitions.stream().map(TopicPartition::partition).toList());
                processState = REBALANCING;
            } else if (processState == INITIALIZING) {
                logger.info("Switching from INITIALIZING to INITIAL_REBALANCING, assigning partitions : {}",
                        topicPartitions.stream().map(TopicPartition::partition).toList());
                processState = INITIAL_REBALANCING;
            }
        }
    }

    @Override
    @Nonnull
    public CommandType<?> resolveType(@Nonnull Class<? extends Command> commandClass) {

        CommandInfo commandInfo = commandClass.getAnnotation(CommandInfo.class);
        if (commandInfo != null) {
            List<AggregateServiceRecord> services = aggregateServices.values().stream()
                    .filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandInfo))
                    .toList();
            if (services.size() == 1) {
                AggregateServiceRecord aggregateServiceRecord = services.get(0);
                if (aggregateRuntime.getName().equals(aggregateServiceRecord.aggregateName())) {

                    return aggregateRuntime.getLocalCommandType(commandInfo.type(), commandInfo.version());
                } else {
                    return new CommandType<>(commandInfo.type(), commandInfo.version(), commandClass, false, true);
                }
            } else {

                throw new IllegalStateException("Cannot determine where to send command " + commandClass.getName());
            }

        } else {
            throw new IllegalStateException("Command class " + commandClass.getName() + " is not annotated with @CommandInfo");
        }
    }

    private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, CommandInfo commandInfo) {
        for (AggregateServiceCommandType supportedCommand : supportedCommands) {
            if (supportedCommand.typeName().equals(commandInfo.type()) &&
                    supportedCommand.version() == commandInfo.version()) {
                return true;
            }
        }
        return false;
    }

    private boolean supportsCommand(List<AggregateServiceCommandType> supportedCommands, CommandType<?> commandType) {
        for (AggregateServiceCommandType supportedCommand : supportedCommands) {
            if (supportedCommand.typeName().equals(commandType.typeName()) &&
                    supportedCommand.version() == commandType.version()) {
                return true;
            }
        }
        return false;
    }

    private boolean producesDomainEvent(List<AggregateServiceDomainEventType> producedEvents, DomainEventType<?> externalDomainEventType) {
        for (AggregateServiceDomainEventType producedEvent : producedEvents) {
            if (producedEvent.typeName().equals(externalDomainEventType.typeName()) &&
                    producedEvent.version() == externalDomainEventType.version()) {
                return true;
            }
        }
        return false;
    }

    @Override
    @Nonnull
    public String resolveTopic(@Nonnull Class<? extends Command> commandClass) {
        return resolveTopic(resolveType(commandClass));
    }

    @Override
    @Nonnull
    public String resolveTopic(@Nonnull CommandType<?> commandType) {
        List<AggregateServiceRecord> services = aggregateServices.values().stream()
                .filter(commandServiceRecord -> supportsCommand(commandServiceRecord.supportedCommands(), commandType))
                .toList();
        if (services.size() == 1) {
            return services.getFirst().commandTopic();
        } else {
            throw new IllegalStateException("Cannot determine where to send command " + commandType.typeName() + " v" + commandType.version());
        }
    }

    @Override
    public String resolveTopic(@Nonnull DomainEventType<?> externalDomainEventType) {
        List<AggregateServiceRecord> services = aggregateServices.values().stream()
                .filter(commandServiceRecord -> producesDomainEvent(commandServiceRecord.producedEvents(), externalDomainEventType))
                .toList();
        if (services.size() == 1) {
            return services.getFirst().domainEventTopic();
        } else {
            throw new IllegalStateException("Cannot determine which service produces DomainEvent " + externalDomainEventType.typeName() + " v" + externalDomainEventType.version());
        }
    }

    @Override
    @Nonnull
    public Integer resolvePartition(@Nonnull String aggregateId) {
        return Math.abs(hashFunction.hashString(aggregateId, UTF_8).asInt()) % partitions;
    }

    public boolean isRunning() {
        return processState == RUNNING && aggregatePartitions.values().stream().allMatch(AggregatePartition::isProcessing);
    }

    @Override
    public void setEnvironment(Environment environment) {
        this.forceRegisterOnIncompatible = environment.getProperty("akces.aggregate.schemas.forceRegister", Boolean.class, false);
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        this.applicationContext = applicationContext;
    }
}

================
File: runtime/src/main/java/org/elasticsoftware/akces/AkcesControllerState.java
================
package org.elasticsoftware.akces;

public enum AkcesControllerState {
    INITIALIZING,
    INITIAL_REBALANCING,
    REBALANCING,
    RUNNING,
    SHUTTING_DOWN,
    ERROR
}

================
File: runtime/src/main/resources/protobuf/AggregateStateRecord.proto
================
// org.elasticsoftware.akces.protocol.AggregateStateRecord

// Message for org.elasticsoftware.akces.protocol.AggregateStateRecord
message AggregateStateRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional int64 generation = 7;
  optional string tenantId = 8;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: runtime/src/main/resources/protobuf/CommandRecord.proto
================
// org.elasticsoftware.akces.protocol.CommandRecord

// Message for org.elasticsoftware.akces.protocol.CommandRecord
message CommandRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional string tenantId = 7;
  optional string id = 8;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: runtime/src/main/resources/protobuf/DomainEventRecord.proto
================
// org.elasticsoftware.akces.protocol.DomainEventRecord

// Message for org.elasticsoftware.akces.protocol.DomainEventRecord
message DomainEventRecord {
  optional string name = 1;
  optional int32 version = 2;
  optional bytes payload = 3;
  optional PayloadEncoding encoding = 4;
  optional string aggregateId = 5;
  optional string correlationId = 6;
  optional int64 generation = 7;
  optional string tenantId = 8;
  optional string id = 9;
}
// Enum for org.elasticsoftware.akces.protocol.PayloadEncoding
enum PayloadEncoding {
  JSON = 0;
  PROTOBUF = 1;
}

================
File: runtime/src/main/resources/akces-aggregateservice.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
spring.kafka.consumer.isolation-level=read_committed
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.heartbeat-interval=2000
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.properties.max.poll.interval.ms=10000
spring.kafka.consumer.properties.session.timeout.ms=30000
spring.kafka.consumer.properties.partition.assignment.strategy=org.apache.kafka.clients.consumer.CooperativeStickyAssignor
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.producer.acks=all
spring.kafka.producer.retries=2147483647
spring.kafka.producer.properties.linger.ms=0
spring.kafka.producer.properties.retry.backoff.ms=0
spring.kafka.producer.properties.enable.idempotence=true
spring.kafka.producer.properties.max.in.flight.requests.per.connection=1

================
File: runtime/src/test/java/org/elasticsoftware/akces/beans/AotServicesTest.java
================
package org.elasticsoftware.akces.beans;

import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.springframework.aot.generate.ValueCodeGenerator;
import org.springframework.beans.factory.aot.AotServices;

import java.util.List;

public class AotServicesTest {
    @Test
    void testLoadAotServices() {
        AotServices.Loader loader = AotServices.factories();
        List<ValueCodeGenerator.Delegate> additionalDelegates = loader.load(ValueCodeGenerator.Delegate.class).asList();
        Assertions.assertFalse(additionalDelegates.isEmpty());
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akces/beans/MinInsyncReplicasTest.java
================
package org.elasticsoftware.akces.beans;

import org.elasticsoftware.akces.util.KafkaUtils;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;

import static org.elasticsoftware.akces.util.KafkaUtils.calculateQuorum;

public class MinInsyncReplicasTest {
    @Test
    void testScenarios() {

        Assertions.assertEquals(1, calculateQuorum((short) 1));
        Assertions.assertEquals(2, calculateQuorum((short) 2));
        Assertions.assertEquals(2, calculateQuorum((short) 3));
        Assertions.assertEquals(3, calculateQuorum((short) 4));
        Assertions.assertEquals(3, calculateQuorum((short) 5));
        Assertions.assertEquals(4, calculateQuorum((short) 6));
        Assertions.assertEquals(4, calculateQuorum((short) 7));
        Assertions.assertEquals(5, calculateQuorum((short) 8));
        Assertions.assertEquals(5, calculateQuorum((short) 9));
        Assertions.assertEquals(6, calculateQuorum((short) 10));
    }

}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/Account.java
================
package org.elasticsoftware.akcestest.aggregate.account;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.Aggregate;
import org.elasticsoftware.akces.annotations.AggregateInfo;
import org.elasticsoftware.akces.annotations.CommandHandler;
import org.elasticsoftware.akces.annotations.EventSourcingHandler;

import java.util.stream.Stream;

@AggregateInfo(value = "Account", generateGDPRKeyOnCreate = true, indexed = true, indexName = "Users")
@SuppressWarnings("unused")
public final class Account implements Aggregate<AccountState> {
    @Override
    public String getName() {
        return "Account";
    }

    @Override
    public Class<AccountState> getStateClass() {
        return AccountState.class;
    }

    @CommandHandler(create = true, produces = AccountCreatedEvent.class, errors = {})
    public Stream<AccountCreatedEvent> create(CreateAccountCommand cmd, AccountState isNull) {
        return Stream.of(new AccountCreatedEvent(cmd.userId(), cmd.country(), cmd.firstName(), cmd.lastName(), cmd.email()));
    }

    @EventSourcingHandler(create = true)
    @NotNull
    public AccountState create(@NotNull AccountCreatedEvent event, AccountState isNull) {
        return new AccountState(event.userId(), event.country(), event.firstName(), event.lastName(), event.email());
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.account;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.annotations.PIIData;
import org.elasticsoftware.akces.events.DomainEvent;


@DomainEventInfo(type = "AccountCreated")
public record AccountCreatedEvent(
        @AggregateIdentifier @NotNull String userId,
        @NotNull String country,
        @NotNull @PIIData String firstName,
        @NotNull @PIIData String lastName,
        @NotNull @PIIData String email
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/AccountState.java
================
package org.elasticsoftware.akcestest.aggregate.account;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.AggregateState;
import org.elasticsoftware.akces.annotations.AggregateStateInfo;
import org.elasticsoftware.akces.annotations.PIIData;

@AggregateStateInfo(type = "Account", version = 1)
public record AccountState(@NotNull String userId,
                           @NotNull String country,
                           @NotNull @PIIData String firstName,
                           @NotNull @PIIData String lastName,
                           @NotNull @PIIData String email) implements AggregateState {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/account/CreateAccountCommand.java
================
package org.elasticsoftware.akcestest.aggregate.account;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

@CommandInfo(type = "CreateAccount")
public record CreateAccountCommand(
        @AggregateIdentifier @NotNull String userId,
        @NotNull String country,
        @NotNull String firstName,
        @NotNull String lastName,
        @NotNull String email
) implements Command {
    @Override
    @NotNull
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

import java.math.BigDecimal;

@DomainEventInfo(type = "BuyOrderCreated", version = 1)
public record BuyOrderCreatedEvent(
        @NotNull @AggregateIdentifier String userId,
        @NotNull String orderId,
        @NotNull FxMarket market,
        @NotNull BigDecimal quantity,
        @NotNull BigDecimal limitPrice,
        @NotNull String clientReference
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return orderId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderPlacedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

import java.math.BigDecimal;

@DomainEventInfo(type = "BuyOrderPlaced", version = 1)
public record BuyOrderPlacedEvent(
        @NotNull @AggregateIdentifier String userId,
        @NotNull String orderId,
        @NotNull FxMarket market,
        @NotNull BigDecimal quantity,
        @NotNull BigDecimal limitPrice
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return orderId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderProcess.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.InsufficientFundsErrorEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.InvalidCurrencyErrorEvent;

import java.math.BigDecimal;

public record BuyOrderProcess(String orderId, FxMarket market, BigDecimal quantity, BigDecimal limitPrice,
                              String clientReference) implements OrderProcess {
    @Override
    public String getProcessId() {
        return orderId();
    }

    @Override
    public DomainEvent handle(InsufficientFundsErrorEvent error) {
        return new BuyOrderRejectedEvent(error.walletId(), orderId(), clientReference());
    }

    @Override
    public DomainEvent handle(InvalidCurrencyErrorEvent error) {
        return new BuyOrderRejectedEvent(error.walletId(), orderId(), clientReference());
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/BuyOrderRejectedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "BuyOrderRejected", version = 1)
public record BuyOrderRejectedEvent(
        @NotNull @AggregateIdentifier String userId,
        @NotNull String orderId,
        @NotNull String clientReference
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/FxMarket.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

public record FxMarket(String id, String baseCurrency, String quoteCurrency) {
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcess.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import com.fasterxml.jackson.annotation.JsonSubTypes;
import com.fasterxml.jackson.annotation.JsonTypeInfo;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.processmanager.AkcesProcess;
import org.elasticsoftware.akcestest.aggregate.wallet.InsufficientFundsErrorEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.InvalidCurrencyErrorEvent;

import java.math.BigDecimal;

@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
@JsonSubTypes({
        @JsonSubTypes.Type(value = BuyOrderProcess.class, name = "BUY")
})
public sealed interface OrderProcess extends AkcesProcess permits BuyOrderProcess {
    String orderId();

    FxMarket market();

    BigDecimal quantity();

    BigDecimal limitPrice();

    String clientReference();

    DomainEvent handle(InsufficientFundsErrorEvent error);

    DomainEvent handle(InvalidCurrencyErrorEvent error);
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcessManager.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import org.elasticsoftware.akces.aggregate.Aggregate;
import org.elasticsoftware.akces.annotations.AggregateInfo;
import org.elasticsoftware.akces.annotations.CommandHandler;
import org.elasticsoftware.akces.annotations.EventHandler;
import org.elasticsoftware.akces.annotations.EventSourcingHandler;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.AmountReservedEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.InsufficientFundsErrorEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.InvalidCurrencyErrorEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.ReserveAmountCommand;

import java.util.ArrayList;
import java.util.UUID;
import java.util.stream.Stream;

@AggregateInfo(value = "OrderProcessManager", indexed = true, indexName = "Users")
public class OrderProcessManager implements Aggregate<OrderProcessManagerState> {
    @Override
    public String getName() {
        return "OrderProcessManager";
    }

    @Override
    public Class<OrderProcessManagerState> getStateClass() {
        return OrderProcessManagerState.class;
    }

    @EventHandler(create = true, produces = UserOrderProcessesCreatedEvent.class, errors = {})
    public Stream<UserOrderProcessesCreatedEvent> create(AccountCreatedEvent event, OrderProcessManagerState isNull) {
        return Stream.of(new UserOrderProcessesCreatedEvent(event.userId()));
    }

    @EventSourcingHandler(create = true)
    public OrderProcessManagerState create(UserOrderProcessesCreatedEvent event, OrderProcessManagerState isNull) {
        return new OrderProcessManagerState(event.userId());
    }

    @EventSourcingHandler
    public OrderProcessManagerState handle(BuyOrderCreatedEvent event, OrderProcessManagerState state) {
        return new OrderProcessManagerState(state.userId(), new ArrayList<>(state.runningProcesses()) {{
            add(new BuyOrderProcess(
                    event.orderId(),
                    event.market(),
                    event.quantity(),
                    event.limitPrice(),
                    event.clientReference()));
        }});
    }

    @EventSourcingHandler
    public OrderProcessManagerState handle(BuyOrderRejectedEvent event, OrderProcessManagerState state) {
        return new OrderProcessManagerState(state.userId(), new ArrayList<>(state.runningProcesses()) {{
            removeIf(process -> process.orderId().equals(event.orderId()));
        }});
    }

    @EventSourcingHandler
    public OrderProcessManagerState handle(BuyOrderPlacedEvent event, OrderProcessManagerState state) {

        return new OrderProcessManagerState(state.userId(), new ArrayList<>(state.runningProcesses()) {{
            removeIf(process -> process.orderId().equals(event.orderId()));
        }});
    }








    @CommandHandler(produces = BuyOrderCreatedEvent.class, errors = {})
    public Stream<BuyOrderCreatedEvent> placeBuyOrder(PlaceBuyOrderCommand command, OrderProcessManagerState state) {

        String orderId = UUID.randomUUID().toString();

        getCommandBus().send(new ReserveAmountCommand(
                state.userId(),
                command.market().quoteCurrency(),
                command.quantity().multiply(command.limitPrice()),
                orderId));

        return Stream.of(new BuyOrderCreatedEvent(
                state.userId(),
                orderId,
                command.market(),
                command.quantity(),
                command.limitPrice(),
                command.clientReference()));
    }


    @EventHandler(produces = BuyOrderPlacedEvent.class, errors = {})
    public Stream<DomainEvent> handle(AmountReservedEvent event, OrderProcessManagerState state) {

        OrderProcess orderProcess = state.getAkcesProcess(event.referenceId());
        if (orderProcess != null) {

            return Stream.of(new BuyOrderPlacedEvent(state.userId(), orderProcess.orderId(), orderProcess.market(), orderProcess.quantity(), orderProcess.limitPrice()));
        } else {

            return Stream.empty();
        }
    }

    @EventHandler(produces = BuyOrderRejectedEvent.class, errors = {})
    public Stream<DomainEvent> handle(InsufficientFundsErrorEvent errorEvent, OrderProcessManagerState state) {
        return Stream.of(state.getAkcesProcess(errorEvent.referenceId()).handle(errorEvent));
    }

    @EventHandler(produces = BuyOrderRejectedEvent.class, errors = {})
    public Stream<DomainEvent> handle(InvalidCurrencyErrorEvent errorEvent, OrderProcessManagerState state) {
        return Stream.of(state.getAkcesProcess(errorEvent.referenceId()).handle(errorEvent));
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/OrderProcessManagerState.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.processmanager.ProcessManagerState;
import org.elasticsoftware.akces.processmanager.UnknownAkcesProcessException;

import java.util.List;

public record OrderProcessManagerState(
        @NotNull @AggregateIdentifier String userId,
        List<BuyOrderProcess> runningProcesses
) implements ProcessManagerState<OrderProcess> {
    public OrderProcessManagerState(@NotNull String userId) {
        this(userId, List.of());
    }

    @Override
    public String getAggregateId() {
        return userId();
    }

    @Override
    public OrderProcess getAkcesProcess(String processId) {
        return runningProcesses.stream().filter(p -> p.orderId().equals(processId)).findFirst()
                .orElseThrow(() -> new UnknownAkcesProcessException("OrderProcessManager", userId(), processId));
    }

    @Override
    public boolean hasAkcesProcess(String processId) {
        return runningProcesses.stream().anyMatch(p -> p.orderId().equals(processId));
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/PlaceBuyOrderCommand.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

import java.math.BigDecimal;

@CommandInfo(type = "PlaceBuyOrder", version = 1)
public record PlaceBuyOrderCommand(
        @NotNull @AggregateIdentifier String userId,
        @NotNull FxMarket market,
        @NotNull BigDecimal quantity,
        @NotNull BigDecimal limitPrice,
        @NotNull String clientReference
) implements Command {
    @Override
    @NotNull
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/orders/UserOrderProcessesCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.orders;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "UserOrderProcessesCreated", version = 1)
public record UserOrderProcessesCreatedEvent(@NotNull @AggregateIdentifier String userId) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/AmountReservedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

import java.math.BigDecimal;

@DomainEventInfo(type = "AmountReserved", version = 1)
public record AmountReservedEvent(
        @NotNull @AggregateIdentifier String userId,
        @NotNull String currency,
        @NotNull BigDecimal amount,
        @NotNull String referenceId
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/BalanceAlreadyExistsErrorEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.ErrorEvent;

@DomainEventInfo(type = "BalanceAlreadyExistsError")
public record BalanceAlreadyExistsErrorEvent(@AggregateIdentifier @NotNull String walletId,
                                             @NotNull String currency) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return walletId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/BalanceCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "BalanceCreated")
public record BalanceCreatedEvent(
        @AggregateIdentifier @NotNull String id,
        @NotNull String currency
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreateBalanceCommand.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

@CommandInfo(type = "CreateBalance", version = 1)
public record CreateBalanceCommand(
        @NotNull @AggregateIdentifier String id,
        @NotNull String currency
) implements Command {
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreateWalletCommand.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

@CommandInfo(type = "CreateWallet", version = 1)
public record CreateWalletCommand(
        @AggregateIdentifier @NotNull String id,
        @NotNull String currency
) implements Command {
    @NotNull
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/CreditWalletCommand.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

import java.math.BigDecimal;

@CommandInfo(type = "CreditWallet", version = 1)
public record CreditWalletCommand(
        @AggregateIdentifier @NotNull String id,
        @NotNull String currency,
        @NotNull BigDecimal amount
) implements Command {
    @NotNull
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/ExternalAccountCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.events.DomainEvent;


public record ExternalAccountCreatedEvent(
        @AggregateIdentifier @NotNull String userId,
        @NotNull String country,
        @NotNull String firstName,
        @NotNull String lastName,
        @NotNull String email,
        @NotNull String currency
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InsufficientFundsErrorEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.annotation.Nullable;
import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.ErrorEvent;

import javax.annotation.Nonnull;
import java.math.BigDecimal;

@DomainEventInfo(type = "InsufficientFundsError")
public record InsufficientFundsErrorEvent(
        @NotNull @AggregateIdentifier String walletId,
        @NotNull String currency,
        @NotNull BigDecimal availableAmount,
        @NotNull BigDecimal requestedAmount,
        @Nullable String referenceId
) implements ErrorEvent {
    @Override
    public @Nonnull String getAggregateId() {
        return walletId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidAccountCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import org.elasticsoftware.akces.events.DomainEvent;

public record InvalidAccountCreatedEvent(String middleName, String currency) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return null;
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidAmountErrorEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.ErrorEvent;

import javax.annotation.Nonnull;

@DomainEventInfo(type = "InvalidAmountError")
public record InvalidAmountErrorEvent(
        @NotNull @AggregateIdentifier String walletId,
        @NotNull String currency,
        String referenceId
) implements ErrorEvent {
    public InvalidAmountErrorEvent(@NotNull String walletId, @NotNull String currency) {
        this(walletId, currency, null);
    }

    @Override
    public @Nonnull String getAggregateId() {
        return walletId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/InvalidCurrencyErrorEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.ErrorEvent;

import javax.annotation.Nonnull;

@DomainEventInfo(type = "InvalidCurrencyError")
public record InvalidCurrencyErrorEvent(
        @NotNull @AggregateIdentifier String walletId,
        @NotNull String currency,
        String referenceId
) implements ErrorEvent {
    public InvalidCurrencyErrorEvent(@NotNull String walletId, @NotNull String currency) {
        this(walletId, currency, null);
    }

    @Nonnull
    @Override
    public String getAggregateId() {
        return walletId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/ReserveAmountCommand.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

import java.math.BigDecimal;

@CommandInfo(type = "ReserveAmount", version = 1)
public record ReserveAmountCommand(
        @NotNull @AggregateIdentifier String userId,
        @NotNull String currency,
        @NotNull BigDecimal amount,
        @NotNull String referenceId
) implements Command {
    @Override
    @NotNull
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/Wallet.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.Aggregate;
import org.elasticsoftware.akces.annotations.AggregateInfo;
import org.elasticsoftware.akces.annotations.CommandHandler;
import org.elasticsoftware.akces.annotations.EventHandler;
import org.elasticsoftware.akces.annotations.EventSourcingHandler;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;

import java.math.BigDecimal;
import java.util.ArrayList;
import java.util.List;
import java.util.stream.Stream;


@AggregateInfo(value = "Wallet", version = 1, indexed = true, indexName = "Users")
@SuppressWarnings("unused")
public final class Wallet implements Aggregate<WalletState> {
    @Override
    public String getName() {
        return "Wallet";
    }

    @Override
    public Class<WalletState> getStateClass() {
        return WalletState.class;
    }

    @CommandHandler(create = true, produces = WalletCreatedEvent.class, errors = {})
    public @NotNull Stream<DomainEvent> create(@NotNull CreateWalletCommand cmd, WalletState isNull) {
        return Stream.of(new WalletCreatedEvent(cmd.id()), new BalanceCreatedEvent(cmd.id(), cmd.currency()));
    }

    @EventHandler(create = true, produces = WalletCreatedEvent.class, errors = {})
    public @NotNull Stream<DomainEvent> create(@NotNull AccountCreatedEvent event, WalletState isNull) {

        return Stream.of(new WalletCreatedEvent(event.getAggregateId()), new BalanceCreatedEvent(event.getAggregateId(), "EUR"));
    }

    @CommandHandler(produces = BalanceCreatedEvent.class, errors = {BalanceAlreadyExistsErrorEvent.class})
    public @NotNull Stream<DomainEvent> createBalance(@NotNull CreateBalanceCommand cmd, @NotNull WalletState currentState) {
        boolean balanceExists = currentState.balances().stream()
                .anyMatch(balance -> balance.currency().equals(cmd.currency()));
        if (balanceExists) {
            return Stream.of(new BalanceAlreadyExistsErrorEvent(cmd.id(), cmd.currency()));
        }
        return Stream.of(new BalanceCreatedEvent(cmd.id(), cmd.currency()));
    }

    @CommandHandler(produces = WalletCreditedEvent.class, errors = {InvalidCurrencyErrorEvent.class, InvalidAmountErrorEvent.class})
    @NotNull
    public Stream<DomainEvent> credit(@NotNull CreditWalletCommand cmd, @NotNull WalletState currentState) {
        WalletState.Balance balance = currentState.balances().stream().filter(b -> b.currency().equals(cmd.currency())).findFirst().orElse(null);
        if (balance == null) {

            return Stream.of(new InvalidCurrencyErrorEvent(cmd.id(), cmd.currency()));
        }
        if (cmd.amount().compareTo(BigDecimal.ZERO) < 0) {

            return Stream.of(new InvalidAmountErrorEvent(cmd.id(), cmd.currency()));
        }
        return Stream.of(new WalletCreditedEvent(currentState.id(), cmd.currency(), cmd.amount(), balance.amount().add(cmd.amount())));
    }

    @CommandHandler(produces = AmountReservedEvent.class, errors = {InvalidCurrencyErrorEvent.class, InvalidAmountErrorEvent.class, InsufficientFundsErrorEvent.class})
    public Stream<DomainEvent> makeReservation(ReserveAmountCommand command, WalletState state) {
        WalletState.Balance balance = state.balances().stream().filter(b -> b.currency().equals(command.currency())).findFirst().orElse(null);
        if (balance == null) {

            return Stream.of(new InvalidCurrencyErrorEvent(command.userId(), command.currency(), command.referenceId()));
        }
        if (command.amount().compareTo(BigDecimal.ZERO) < 0) {

            return Stream.of(new InvalidAmountErrorEvent(command.userId(), command.currency()));
        }

        if (balance.getAvailableAmount().compareTo(command.amount()) >= 0) {
            return Stream.of(new AmountReservedEvent(command.userId(), command.currency(), command.amount(), command.referenceId()));
        } else {
            return Stream.of(new InsufficientFundsErrorEvent(command.userId(), command.currency(), balance.getAvailableAmount(), command.amount(), command.referenceId()));
        }
    }

    @EventSourcingHandler(create = true)
    public @NotNull WalletState create(@NotNull WalletCreatedEvent event, WalletState isNull) {
        return new WalletState(event.id(), new ArrayList<>());
    }

    @EventSourcingHandler
    public @NotNull WalletState createBalance(@NotNull BalanceCreatedEvent event, WalletState state) {
        List<WalletState.Balance> balances = new ArrayList<>(state.balances());
        balances.add(new WalletState.Balance(event.currency(), BigDecimal.ZERO));
        return new WalletState(state.id(), balances);
    }

    @EventSourcingHandler
    public @NotNull WalletState credit(@NotNull WalletCreditedEvent event, @NotNull WalletState state) {
        return new WalletState(state.id(), state.balances().stream().map(b -> {
            if (b.currency().equals(event.currency())) {
                return new WalletState.Balance(b.currency(), b.amount().add(event.amount()));
            } else {
                return b;
            }
        }).toList());
    }

    @EventSourcingHandler
    public @NotNull WalletState reserveAmount(@NotNull AmountReservedEvent event, @NotNull WalletState state) {
        return new WalletState(state.id(), state.balances().stream().map(b -> {
            if (b.currency().equals(event.currency())) {
                return new WalletState.Balance(b.currency(), b.amount(), b.reservedAmount().add(event.amount()));
            } else {
                return b;
            }
        }).toList());
    }

}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletCreatedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "WalletCreated")
public record WalletCreatedEvent(
        @AggregateIdentifier @NotNull String id
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletCreditedEvent.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

import java.math.BigDecimal;

@DomainEventInfo(type = "WalletCredited")
public record WalletCreditedEvent(
        @AggregateIdentifier @NotNull String id,
        @NotNull String currency,
        @NotNull BigDecimal amount,
        @NotNull BigDecimal balance
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/aggregate/wallet/WalletState.java
================
package org.elasticsoftware.akcestest.aggregate.wallet;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.aggregate.AggregateState;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.AggregateStateInfo;

import java.math.BigDecimal;
import java.util.List;

@AggregateStateInfo(type = "Wallet", version = 1)
public record WalletState(
        @AggregateIdentifier @NotNull String id,
        List<Balance> balances
) implements AggregateState {
    @Override
    public String getAggregateId() {
        return id();
    }

    public record Balance(String currency, BigDecimal amount, BigDecimal reservedAmount) {
        public Balance(@NotNull String currency) {
            this(currency, BigDecimal.ZERO, BigDecimal.ZERO);
        }

        public Balance(@NotNull String currency, @NotNull BigDecimal amount) {
            this(currency, amount, BigDecimal.ZERO);
        }

        public BigDecimal getAvailableAmount() {
            return amount.subtract(reservedAmount);
        }
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/control/AkcesAggregateControllerTests.java
================
package org.elasticsoftware.akcestest.control;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.elasticsoftware.akces.control.AggregateServiceCommandType;
import org.elasticsoftware.akces.control.AggregateServiceDomainEventType;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.junit.jupiter.api.Test;

import java.io.IOException;
import java.util.Collections;
import java.util.List;

import static org.junit.jupiter.api.Assertions.*;


public class AkcesAggregateControllerTests {
    @Test
    public void testSerialization() throws IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        AggregateServiceRecord record = new AggregateServiceRecord(
                "Account",
                "Account-Commands",
                "Account-DomainEvents",
                List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
                List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
                Collections.emptyList());
        assertNotNull(record);

    }

    @Test
    public void testDeserialization() throws JsonProcessingException {
        String serializedRecord = """
                {
                  "aggregateName" : "Account",
                  "commandTopic" : "Account-Commands",
                  "supportedCommands" : [ {
                    "typeName" : "CreateAccount",
                    "version" : 1,
                    "create" : true
                  } ],
                  "producedEvents" : [ {
                    "typeName" : "AccountCreated",
                    "version" : 1,
                    "create" : true,
                    "external" : false
                  } ],
                  "consumedEvents" : [ ]
                }
                """;

        ObjectMapper objectMapper = new ObjectMapper();
        AkcesControlRecord deserialized = objectMapper.readValue(serializedRecord, AggregateServiceRecord.class);
        assertNotNull(deserialized);
        assertTrue(deserialized instanceof AggregateServiceRecord);
        assertEquals("Account", ((AggregateServiceRecord) deserialized).aggregateName());
        assertEquals("Account-Commands", ((AggregateServiceRecord) deserialized).commandTopic());
    }

    @Test
    public void testSerde() {
        AkcesControlRecordSerde serde = new AkcesControlRecordSerde(new ObjectMapper());
        AggregateServiceRecord record = new AggregateServiceRecord(
                "Account",
                "Account-Commands",
                "Account-DomainEvents",
                List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
                List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
                Collections.emptyList());
        byte[] serialized = serde.serializer().serialize("Akces-Control", record);
        assertNotNull(serialized);

        AkcesControlRecord deserialized = serde.deserializer().deserialize("Akces-Control", serialized);
        assertNotNull(deserialized);
        assertTrue(deserialized instanceof AggregateServiceRecord);
        assertEquals("Account", ((AggregateServiceRecord) deserialized).aggregateName());
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/old/BalanceCreatedEvent.java
================
package org.elasticsoftware.akcestest.old;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.events.DomainEvent;

public record BalanceCreatedEvent(@AggregateIdentifier @NotNull String id, String currency) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/old/BuyOrderPlacedEvent.java
================
package org.elasticsoftware.akcestest.old;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akcestest.aggregate.orders.FxMarket;

import java.math.BigDecimal;

public record BuyOrderPlacedEvent(
        @NotNull @AggregateIdentifier String userId,
        String orderId,
        FxMarket market,
        BigDecimal quantity,
        BigDecimal limitPrice
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return orderId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/old/CreateWalletCommand.java
================
package org.elasticsoftware.akcestest.old;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.commands.Command;

public record CreateWalletCommand(
        @AggregateIdentifier String id,
        String currency
) implements Command {
    @NotNull
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/old/WalletCreditedEvent.java
================
package org.elasticsoftware.akcestest.old;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.events.DomainEvent;

import java.math.BigDecimal;

public record WalletCreditedEvent(
        @AggregateIdentifier @NotNull String id,
        String currency,
        BigDecimal amount,
        BigDecimal balance
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/protocol/ProtocolTests.java
================
package org.elasticsoftware.akcestest.protocol;

import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.protobuf.ProtobufMapper;
import com.fasterxml.jackson.dataformat.protobuf.schema.NativeProtobufSchema;
import com.fasterxml.jackson.dataformat.protobuf.schema.ProtobufSchema;
import com.fasterxml.jackson.dataformat.protobuf.schema.ProtobufSchemaLoader;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.CommandRecord;
import org.elasticsoftware.akces.protocol.CommandResponseRecord;
import org.elasticsoftware.akces.protocol.DomainEventRecord;
import org.junit.jupiter.api.Test;

import java.io.IOException;
import java.io.StringReader;
import java.nio.charset.StandardCharsets;
import java.util.UUID;

import static org.elasticsoftware.akces.protocol.PayloadEncoding.JSON;
import static org.junit.jupiter.api.Assertions.assertArrayEquals;
import static org.junit.jupiter.api.Assertions.assertEquals;

public class ProtocolTests {
    @Test
    public void testJacksonProtobuf() throws IOException {
        String protobuf = """



                message DomainEventRecord {
                  optional string name = 1;
                  optional int32 version = 2;
                  optional bytes payload = 3;
                  optional PayloadEncoding encoding = 4;
                  optional string aggregateId = 5;
                  optional string correlationId = 6;
                  optional int64 generation = 7;
                  optional string tenantId = 8;
                  optional string id = 9;
                }

                enum PayloadEncoding {
                  JSON = 0;
                  PROTOBUF = 1;
                  AVRO = 2;
                }
                """;
        DomainEventRecord testRecord = new DomainEventRecord("tenant1", "WalletCreated", 1, "{}".getBytes(StandardCharsets.UTF_8), JSON, "1", UUID.randomUUID().toString(), 1L);





        ProtobufSchema schema = ProtobufSchemaLoader.std.load(new StringReader(protobuf));
        ObjectMapper objectMapper = new ProtobufMapper();
        byte[] serializedTestRecord = objectMapper.writer(schema).writeValueAsBytes(testRecord);

        DomainEventRecord deserializedTestRecord = objectMapper.readerFor(DomainEventRecord.class).with(schema).readValue(serializedTestRecord);

        assertEquals(deserializedTestRecord.name(), testRecord.name());
        assertEquals(deserializedTestRecord.version(), testRecord.version());
        assertArrayEquals(deserializedTestRecord.payload(), testRecord.payload());
        assertEquals(deserializedTestRecord.encoding(), testRecord.encoding());
        assertEquals(deserializedTestRecord.aggregateId(), testRecord.aggregateId());
    }

    @Test
    public void generateDomainEventRecordProtobufSchema() throws JsonMappingException {
        ProtobufMapper mapper = new ProtobufMapper();
        ProtobufSchema schemaWrapper = mapper.generateSchemaFor(DomainEventRecord.class);
        NativeProtobufSchema nativeProtobufSchema = schemaWrapper.getSource();

        String asProtofile = nativeProtobufSchema.toString();

        System.out.println(asProtofile);
    }

    @Test
    public void generateCommandRecordProtobufSchema() throws JsonMappingException {
        ProtobufMapper mapper = new ProtobufMapper();
        ProtobufSchema schemaWrapper = mapper.generateSchemaFor(CommandRecord.class);
        NativeProtobufSchema nativeProtobufSchema = schemaWrapper.getSource();

        String asProtofile = nativeProtobufSchema.toString();

        System.out.println(asProtofile);
    }

    @Test
    public void generateAggregateStateRecordProtobufSchema() throws JsonMappingException {
        ProtobufMapper mapper = new ProtobufMapper();
        ProtobufSchema schemaWrapper = mapper.generateSchemaFor(AggregateStateRecord.class);
        NativeProtobufSchema nativeProtobufSchema = schemaWrapper.getSource();

        String asProtofile = nativeProtobufSchema.toString();

        System.out.println(asProtofile);
    }

    @Test
    public void generateCommandResponseRecordProtobufSchema() throws JsonMappingException {
        ProtobufMapper mapper = new ProtobufMapper();
        ProtobufSchema schemaWrapper = mapper.generateSchemaFor(CommandResponseRecord.class);
        NativeProtobufSchema nativeProtobufSchema = schemaWrapper.getSource();

        String asProtofile = nativeProtobufSchema.toString();

        System.out.println(asProtofile);
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEvent.java
================
package org.elasticsoftware.akcestest.schemas;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "AccountCreatedEvent", version = 1)
public record AccountCreatedEvent(@NotNull String userId, @NotNull String lastName,
                                  @NotNull AccountTypeV1 type) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEventV2.java
================
package org.elasticsoftware.akcestest.schemas;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "AccountCreatedEvent", version = 2)
public record AccountCreatedEventV2(
        @NotNull String userId,
        @NotNull String lastName,
        @NotNull AccountTypeV2 type,
        String firstName,
        String country
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountCreatedEventV3.java
================
package org.elasticsoftware.akcestest.schemas;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "AccountCreatedEvent", version = 3)
public record AccountCreatedEventV3(
        @NotNull String userId,
        @NotNull String lastName,
        @NotNull AccountTypeV2 type,
        String firstName,
        String country,
        String city
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountTypeV1.java
================
package org.elasticsoftware.akcestest.schemas;

public enum AccountTypeV1 {
    FREE, PREMIUM
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/AccountTypeV2.java
================
package org.elasticsoftware.akcestest.schemas;

public enum AccountTypeV2 {
    FREE, PREMIUM, GOLD
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/CreditWalletCommand.java
================
package org.elasticsoftware.akcestest.schemas;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.AggregateIdentifier;
import org.elasticsoftware.akces.annotations.CommandInfo;
import org.elasticsoftware.akces.commands.Command;

import java.math.BigDecimal;

@CommandInfo(type = "CreditWallet", version = 1)
public record CreditWalletCommand(
        @AggregateIdentifier @NotNull String id,
        @NotNull String currency,
        @NotNull BigDecimal amount,
        BigDecimal optionalAmount
) implements Command {
    @NotNull
    @Override
    public String getAggregateId() {
        return id();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/JsonSchemaTests.java
================
package org.elasticsoftware.akcestest.schemas;


import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.github.victools.jsonschema.generator.*;
import com.github.victools.jsonschema.module.jackson.JacksonModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationOption;
import io.confluent.kafka.schemaregistry.CompatibilityLevel;
import io.confluent.kafka.schemaregistry.SimpleParsedSchemaHolder;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.elasticsoftware.akcestest.aggregate.wallet.InvalidAmountErrorEvent;
import org.everit.json.schema.Schema;
import org.everit.json.schema.ValidationException;
import org.jetbrains.annotations.NotNull;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;

import java.io.IOException;
import java.math.BigDecimal;
import java.util.List;
import java.util.UUID;

import static org.junit.jupiter.api.Assertions.assertEquals;

public class JsonSchemaTests {
    private static @NotNull SchemaGenerator createSchemaGenerator() {
        Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
        objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
        objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        return new SchemaGenerator(configBuilder.build());
    }

    @Test
    public void testSchemaCompatibility() throws IOException {
        SchemaGenerator generator = createSchemaGenerator();

        JsonNode schemaV1 = generator.generateSchema(AccountCreatedEvent.class);
        JsonNode schemaV2 = generator.generateSchema(AccountCreatedEventV2.class);

        System.out.println(schemaV1.toString());
        System.out.println(schemaV2.toString());

        JsonSchema schema1 = new JsonSchema(schemaV1);
        JsonSchema schema2 = new JsonSchema(schemaV2);

        assertEquals(schema2.isCompatible(CompatibilityLevel.BACKWARD_TRANSITIVE, List.of(new SimpleParsedSchemaHolder(schema1))).size(), 0);

        schema2.validate(schema2.toJson(new AccountCreatedEvent("1", "Musk", AccountTypeV1.PREMIUM)));

        schema2.validate(schema2.toJson(new AccountCreatedEventV2("1", "Musk", AccountTypeV2.PREMIUM, "Elon", "US")));




    }

    @Test
    public void testNullableString() throws IOException {
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_7, OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS, JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator generator = new SchemaGenerator(config);

        JsonNode schema = generator.generateSchema(InvalidAmountErrorEvent.class);
        JsonSchema jsonSchema = new JsonSchema(schema);

        jsonSchema.validate(jsonSchema.toJson(new InvalidAmountErrorEvent(UUID.randomUUID().toString(), "USD")));
    }

    @Test
    public void testNullForNotNullField() {
        SchemaGenerator generator = createSchemaGenerator();

        JsonNode schema = generator.generateSchema(InvalidAmountErrorEvent.class);
        JsonSchema jsonSchema = new JsonSchema(schema);

        ValidationException exception = Assertions.assertThrows(ValidationException.class, () -> {
            jsonSchema.validate(jsonSchema.toJson(new InvalidAmountErrorEvent(UUID.randomUUID().toString(), null)));
        });

        assertEquals("#/currency", exception.getPointerToViolation());
        assertEquals("#/currency: expected type: String, found: Null", exception.getMessage());

    }

    @Test
    public void testCommandWithBigDecimal() throws IOException {
        Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
        builder.modulesToInstall(new AkcesGDPRModule());
        builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        ObjectMapper objectMapper = builder.build();
        SchemaGenerator generator = createSchemaGenerator();

        JsonNode schema = generator.generateSchema(CreditWalletCommand.class);
        System.out.println(schema);
        JsonSchema jsonSchema = new JsonSchema(schema);

        JsonNode jsonNode = objectMapper.valueToTree(new CreditWalletCommand("de6f87c6-0e4a-4f20-8c06-659fe5bcb7bc", "USD", new BigDecimal("100.00"), null));
        jsonSchema.validate(jsonNode);

        String serialized = objectMapper.writeValueAsString(jsonNode);
        System.out.println(serialized);
    }


    public void testDeepEquals() throws JsonProcessingException {
        String registeredSchemaString = "{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"properties\":{\"funds\":{\"type\":[\"string\",\"null\"]},\"marketId\":{\"type\":[\"string\",\"null\"]},\"orderId\":{\"type\":[\"string\",\"null\"]},\"ownerId\":{\"type\":[\"string\",\"null\"]},\"side\":{\"anyOf\":[{\"type\":\"null\"},{\"type\":\"string\",\"enum\":[\"BUY\",\"SELL\"]}]},\"size\":{\"type\":[\"string\",\"null\"]}},\"additionalProperties\":false}";
        String localSchemaString = "{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"properties\":{\"funds\":{\"type\":[\"string\",\"null\"]},\"marketId\":{\"type\":[\"string\",\"null\"]},\"orderId\":{\"type\":[\"string\",\"null\"]},\"ownerId\":{\"type\":[\"string\",\"null\"]},\"side\":{\"anyOf\":[{\"type\":\"null\"},{\"type\":\"string\",\"enum\":[\"BUY\",\"SELL\"]}]},\"size\":{\"type\":[\"string\",\"null\"]}},\"additionalProperties\":false}";

        Assertions.assertEquals(registeredSchemaString, localSchemaString);

        Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
        builder.modulesToInstall(new AkcesGDPRModule());
        builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        ObjectMapper objectMapper = builder.build();

        JsonSchema localSchema = new JsonSchema(objectMapper.readValue(localSchemaString, Schema.class));
        JsonSchema registeredSchema = new JsonSchema(objectMapper.readValue(registeredSchemaString, Schema.class));

        Assertions.assertTrue(registeredSchema.deepEquals(localSchema));
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/schemas/NotCompatibleAccountCreatedEventV4.java
================
package org.elasticsoftware.akcestest.schemas;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.DomainEvent;

@DomainEventInfo(type = "AccountCreatedEvent", version = 4)
public record NotCompatibleAccountCreatedEventV4(
        @NotNull String userId,
        @NotNull String lastName,
        String firstName,
        String country,
        String city
) implements DomainEvent {
    @Override
    public String getAggregateId() {
        return userId();
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/state/RocksDBAggregateStateRepositoryTests.java
================
package org.elasticsoftware.akcestest.state;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.TopicPartition;
import org.elasticsoftware.akces.protocol.AggregateStateRecord;
import org.elasticsoftware.akces.protocol.PayloadEncoding;
import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;
import org.elasticsoftware.akces.state.RocksDBAggregateStateRepository;
import org.elasticsoftware.akcestest.aggregate.wallet.WalletState;
import org.junit.jupiter.api.AfterAll;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.rocksdb.RocksDBException;

import java.io.File;
import java.io.IOException;
import java.math.BigDecimal;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Comparator;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class RocksDBAggregateStateRepositoryTests {
    private final ProtocolRecordSerde serde = new ProtocolRecordSerde();
    private final ObjectMapper objectMapper = new ObjectMapper();

    private final Future<RecordMetadata> producerResponse = mock(Future.class);

    @AfterAll
    public static void cleanUp() throws IOException {

        Files.walk(Paths.get("/tmp/rocksdb"))
                .sorted(Comparator.reverseOrder())
                .map(Path::toFile)
                .forEach(File::delete);
    }

    @Test
    public void testCreate() throws RocksDBException, IOException {
        try (RocksDBAggregateStateRepository repository =
                     new RocksDBAggregateStateRepository("/tmp/rocksdb",
                             "Wallet-AggregateState-0",
                             "Wallet-AggregateState",
                             serde.serializer(),
                             serde.deserializer())) {
            AggregateStateRecord record = repository.get("1234");
            Assertions.assertNull(record);
        }
    }

    @Test
    public void testSingleUpdate() throws RocksDBException, IOException, ExecutionException, InterruptedException {
        try (RocksDBAggregateStateRepository repository =
                     new RocksDBAggregateStateRepository("/tmp/rocksdb",
                             "Wallet-AggregateState-0",
                             "Wallet-AggregateState",
                             serde.serializer(),
                             serde.deserializer())) {
            String id = "3f61ae34-0945-4d5a-89c6-ee2088a83315";
            WalletState state = new WalletState(id, List.of(new WalletState.Balance("USD", BigDecimal.ZERO)));
            AggregateStateRecord record = new AggregateStateRecord(
                    "AKCES",
                    "Wallet",
                    1,
                    objectMapper.writeValueAsBytes(state),
                    PayloadEncoding.JSON,
                    id,
                    UUID.randomUUID().toString(),
                    1);
            repository.prepare(record, producerResponse);

            when(producerResponse.get()).thenReturn(new RecordMetadata(new TopicPartition("Wallet-AggregateState", 0), 12, 0, System.currentTimeMillis(), 16, 345));
            repository.commit();

            AggregateStateRecord result = repository.get(id);
            Assertions.assertNotNull(result);

            Assertions.assertEquals(12, repository.getOffset());
        }
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/util/HostUtilsTests.java
================
package org.elasticsoftware.akcestest.util;

import org.elasticsoftware.akces.util.HostUtils;
import org.junit.jupiter.api.Test;

public class HostUtilsTests {
    @Test
    public void testGetHostName() {
        String hostName = HostUtils.getHostName();
        System.out.println(hostName);
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/AggregateServiceApplicationTests.java
================
package org.elasticsoftware.akcestest;

import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import jakarta.inject.Inject;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.elasticsoftware.akces.AggregateServiceApplication;
import org.elasticsoftware.akces.AkcesAggregateController;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;
import org.junit.jupiter.api.AfterAll;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.NoSuchBeanDefinitionException;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextInitializer;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.test.annotation.DirtiesContext;
import org.springframework.test.context.ContextConfiguration;
import org.springframework.test.context.support.TestPropertySourceUtils;
import org.testcontainers.containers.GenericContainer;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.Network;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import java.io.File;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.Duration;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;

import static org.elasticsoftware.akcestest.TestUtils.*;

@SpringBootTest(
        classes = AggregateServiceApplication.class,
        args = "org.elasticsoftware.akcestest.aggregate.account",
        useMainMethod = SpringBootTest.UseMainMethod.ALWAYS)
@ContextConfiguration(initializers = AggregateServiceApplicationTests.Initializer.class)
@Testcontainers
@DirtiesContext
public class AggregateServiceApplicationTests {
    private static final String CONFLUENT_PLATFORM_VERSION = "7.8.1";

    private static final Network network = Network.newNetwork();

    @Container
    private static final KafkaContainer kafka =
            new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
                    .withKraft()
                    .withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
                    .withNetwork(network)
                    .withNetworkAliases("kafka");

    @Container
    private static final GenericContainer<?> schemaRegistry =
            new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
                    .withNetwork(network)
                    .withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
                    .withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
                    .withExposedPorts(8081)
                    .withNetworkAliases("schema-registry")
                    .dependsOn(kafka);
    @Inject
    ApplicationContext applicationContext;
    @Inject
    @Qualifier("AccountAkcesController")
    AkcesAggregateController akcesAggregateController;
    @Inject
    @Qualifier("aggregateServiceConsumerFactory")
    ConsumerFactory<String, ProtocolRecord> consumerFactory;
    @Inject
    @Qualifier("aggregateServiceProducerFactory")
    ProducerFactory<String, ProtocolRecord> producerFactory;
    @Inject
    @Qualifier("aggregateServiceControlConsumerFactory")
    ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory;

    @BeforeAll
    @AfterAll
    public static void cleanUp() throws IOException {
        if (Files.exists(Paths.get("/tmp/akces"))) {

            Files.walk(Paths.get("/tmp/akces"))
                    .sorted(Comparator.reverseOrder())
                    .map(Path::toFile)
                    .forEach(File::delete);
        }
    }

    @Test
    public void testAggregateLoading() {

        Assertions.assertNotNull(applicationContext.getBean("AccountAggregateRuntimeFactory"));
        Assertions.assertNotNull(akcesAggregateController);
        Assertions.assertNotNull(consumerFactory);
        Assertions.assertNotNull(producerFactory);
        Assertions.assertNotNull(controlConsumerFactory);
        Assertions.assertThrows(NoSuchBeanDefinitionException.class, () -> applicationContext.getBean("WalletAggregateRuntimeFactory"));
        Assertions.assertThrows(NoSuchBeanDefinitionException.class, () -> applicationContext.getBean("OrderProcessManagerAggregateRuntimeFactory"));

        Consumer<String, AkcesControlRecord> controlConsumer = controlConsumerFactory.createConsumer("Test-AkcesControl", "test-akces-control");

        controlConsumer.subscribe(List.of("Akces-Control"));
        controlConsumer.poll(Duration.ofMillis(1000));
        controlConsumer.seekToBeginning(controlConsumer.assignment());

        ConsumerRecords<String, AkcesControlRecord> controlRecords = new ConsumerRecords<>(Collections.emptyMap());
        while (controlRecords.isEmpty()) {
            controlRecords = controlConsumer.poll(Duration.ofMillis(1000));
        }



        controlConsumer.close();


        while (!akcesAggregateController.isRunning()) {
            Thread.onSpinWait();
        }
    }

    public static class Initializer
            implements ApplicationContextInitializer<ConfigurableApplicationContext> {

        @Override
        public void initialize(ConfigurableApplicationContext applicationContext) {

            prepareKafka(kafka.getBootstrapServers());
            SchemaRegistryClient src = new CachedSchemaRegistryClient("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), 100);
            prepareExternalSchemas(src, List.of(AccountCreatedEvent.class));
            try {
                prepareAggregateServiceRecords(kafka.getBootstrapServers());
            } catch (IOException e) {
                throw new RuntimeException(e);
            }

            TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
                    applicationContext,
                    "akces.rocksdb.baseDir=/tmp/akces",
                    "spring.kafka.enabled=true",
                    "spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
                    "akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)
            );
        }
    }

}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/RuntimeConfiguration.java
================
package org.elasticsoftware.akcestest;

import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;

@Configuration
@ComponentScan(basePackages = {
        "org.elasticsoftware.akcestest.aggregate",
})
public class RuntimeConfiguration {

}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/RuntimeTests.java
================
package org.elasticsoftware.akcestest;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.github.victools.jsonschema.generator.*;
import com.github.victools.jsonschema.module.jackson.JacksonModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationOption;
import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import jakarta.inject.Inject;
import org.apache.kafka.clients.admin.TopicDescription;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;
import org.apache.kafka.common.serialization.StringSerializer;
import org.elasticsoftware.akces.AggregateServiceApplication;
import org.elasticsoftware.akces.AkcesAggregateController;
import org.elasticsoftware.akces.client.AkcesClientController;
import org.elasticsoftware.akces.commands.Command;
import org.elasticsoftware.akces.control.AggregateServiceCommandType;
import org.elasticsoftware.akces.control.AggregateServiceDomainEventType;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.errors.AggregateAlreadyExistsErrorEvent;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.protocol.*;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.account.AccountState;
import org.elasticsoftware.akcestest.aggregate.account.CreateAccountCommand;
import org.elasticsoftware.akcestest.aggregate.orders.BuyOrderCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.orders.FxMarket;
import org.elasticsoftware.akcestest.aggregate.orders.PlaceBuyOrderCommand;
import org.elasticsoftware.akcestest.aggregate.wallet.BalanceCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.CreateWalletCommand;
import org.elasticsoftware.akcestest.aggregate.wallet.CreditWalletCommand;
import org.elasticsoftware.akcestest.aggregate.wallet.WalletCreatedEvent;
import org.junit.jupiter.api.*;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.context.ApplicationContextException;
import org.springframework.context.ApplicationContextInitializer;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;
import org.springframework.kafka.KafkaException;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.KafkaAdmin;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.test.annotation.DirtiesContext;
import org.springframework.test.context.ContextConfiguration;
import org.springframework.test.context.support.TestPropertySourceUtils;
import org.testcontainers.containers.GenericContainer;
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.Network;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;
import org.testcontainers.utility.DockerImageName;

import java.io.File;
import java.io.IOException;
import java.math.BigDecimal;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.stream.IntStream;
import java.util.stream.Stream;

import static org.elasticsoftware.akces.kafka.PartitionUtils.COMMANDS_SUFFIX;
import static org.elasticsoftware.akces.kafka.PartitionUtils.DOMAINEVENTS_SUFFIX;
import static org.elasticsoftware.akcestest.TestUtils.*;
import static org.junit.jupiter.api.Assertions.*;







@SpringBootTest(
        classes = AggregateServiceApplication.class,
        args = "org.elasticsoftware.akcestest.RuntimeConfiguration",
        useMainMethod = SpringBootTest.UseMainMethod.ALWAYS)
@Testcontainers
@ContextConfiguration(initializers = RuntimeTests.DataSourceInitializer.class)
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
@DirtiesContext
public class RuntimeTests {

    private static final String CONFLUENT_PLATFORM_VERSION = "7.8.1";

    private static final Network network = Network.newNetwork();

    @Container
    private static final KafkaContainer kafka =
            new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:" + CONFLUENT_PLATFORM_VERSION))
                    .withKraft()
                    .withEnv("KAFKA_AUTO_CREATE_TOPICS_ENABLE", "false")
                    .withNetwork(network)
                    .withNetworkAliases("kafka");

    @Container
    private static final GenericContainer<?> schemaRegistry =
            new GenericContainer<>(DockerImageName.parse("confluentinc/cp-schema-registry:" + CONFLUENT_PLATFORM_VERSION))
                    .withNetwork(network)
                    .withEnv("SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS", "kafka:9092")
                    .withEnv("SCHEMA_REGISTRY_HOST_NAME", "localhost")
                    .withExposedPorts(8081)
                    .withNetworkAliases("schema-registry")
                    .dependsOn(kafka);

    @Inject
    @Qualifier("aggregateServiceKafkaAdmin")
    KafkaAdmin adminClient;

    @Inject
    @Qualifier("aggregateServiceSchemaRegistryClient")
    SchemaRegistryClient schemaRegistryClient;

    @Inject
    @Qualifier("WalletAkcesController")
    AkcesAggregateController walletAggregateController;

    @Inject
    @Qualifier("AccountAkcesController")
    AkcesAggregateController accountAggregateController;

    @Inject
    @Qualifier("OrderProcessManagerAkcesController")
    AkcesAggregateController orderProcessManagerAggregateController;

    @Inject
    @Qualifier("aggregateServiceConsumerFactory")
    ConsumerFactory<String, ProtocolRecord> consumerFactory;

    @Inject
    @Qualifier("aggregateServiceProducerFactory")
    ProducerFactory<String, ProtocolRecord> producerFactory;

    @Inject
    @Qualifier("aggregateServiceControlConsumerFactory")
    ConsumerFactory<String, AkcesControlRecord> controlConsumerFactory;

    @Inject
    @Qualifier("akcesClient")
    AkcesClientController akcesClient;

    @Inject
    ObjectMapper objectMapper;

    public static void prepareExternalServices(String bootstrapServers) {
        AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(new ObjectMapper());
        Map<String, Object> controlProducerProps = Map.of(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers,
                ProducerConfig.ACKS_CONFIG, "all",
                ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true",
                ProducerConfig.LINGER_MS_CONFIG, "0",
                ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1",
                ProducerConfig.RETRIES_CONFIG, "2147483647",
                ProducerConfig.RETRY_BACKOFF_MS_CONFIG, "0",
                ProducerConfig.TRANSACTIONAL_ID_CONFIG, "Test-AkcesControllerProducer",
                ProducerConfig.CLIENT_ID_CONFIG, "Test-AkcesControllerProducer");
        try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
            controlProducer.initTransactions();
            AggregateServiceRecord aggregateServiceRecord = new AggregateServiceRecord(
                    "Account",
                    "Account" + COMMANDS_SUFFIX,
                    "Account" + DOMAINEVENTS_SUFFIX,
                    List.of(new AggregateServiceCommandType("CreateAccount", 1, true, "commands.CreateAccount")),
                    List.of(new AggregateServiceDomainEventType("AccountCreated", 1, true, false, "domainevents.AccountCreated")),
                    List.of());
            controlProducer.beginTransaction();
            for (int partition = 0; partition < 3; partition++) {
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", aggregateServiceRecord));
            }
            controlProducer.commitTransaction();
        }
    }

    @BeforeAll
    @AfterAll
    public static void cleanUp() throws IOException {

        if (Files.exists(Paths.get("/tmp/akces"))) {
            Files.walk(Paths.get("/tmp/akces"))
                    .sorted(Comparator.reverseOrder())
                    .map(Path::toFile)
                    .filter(File::isDirectory)
                    .forEach(file -> System.out.println(file.getAbsolutePath()));

            Files.walk(Paths.get("/tmp/akces"))
                    .sorted(Comparator.reverseOrder())
                    .map(Path::toFile)
                    .forEach(File::delete);
        }
    }

    public static Stream<TopicPartition> generateTopicPartitions(String topic, int partitions) {
        return IntStream.range(0, partitions)
                .mapToObj(i -> new TopicPartition(topic, i));
    }

    public static <C extends Command> void prepareOldCommandSchemas(SchemaRegistryClient src) {
        Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
        objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
        objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
        try {
            src.register("commands.CreateWallet",
                    new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.CreateWalletCommand.class), List.of(), Map.of(), 1),
                    1,
                    -1);
        } catch (IOException | RestClientException e) {
            throw new ApplicationContextException("Problem populating SchemaRegistry", e);
        }
    }

    public static <D extends DomainEvent> void prepareOldDomainEventSchemas(SchemaRegistryClient src) {
        Jackson2ObjectMapperBuilder objectMapperBuilder = new Jackson2ObjectMapperBuilder();
        objectMapperBuilder.modulesToInstall(new AkcesGDPRModule());
        objectMapperBuilder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapperBuilder.build(),
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
        try {
            src.register("domainevents.BalanceCreated",
                    new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.BalanceCreatedEvent.class), List.of(), Map.of(), 1),
                    1,
                    -1);
            src.register("domainevents.BuyOrderPlaced",
                    new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.BuyOrderPlacedEvent.class), List.of(), Map.of(), 1),
                    1,
                    -1);
            src.register("domainevents.WalletCredited",
                    new JsonSchema(jsonSchemaGenerator.generateSchema(org.elasticsoftware.akcestest.old.WalletCreditedEvent.class), List.of(), Map.of(), 1),
                    1,
                    -1);
        } catch (IOException | RestClientException e) {
            throw new ApplicationContextException("Problem populating SchemaRegistry", e);
        }
    }

    @Test
    @Order(1)
    public void testKafkaAdminClient() {
        assertNotNull(adminClient);
        Map<String, TopicDescription> topics =
                adminClient.describeTopics(
                        "Akces-Control",
                        "Wallet-Commands",
                        "Wallet-DomainEvents",
                        "Account-DomainEvents",
                        "Wallet-AggregateState");
        assertNotNull(topics);
        assertFalse(topics.isEmpty());
    }

    @Test
    @Order(2)
    public void createSchemas() throws RestClientException, IOException {
        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        System.out.println(schemaRegistryClient.getAllSubjects());
    }

    @Test
    @Order(3)
    public void testAkcesControl() throws JsonProcessingException {
        assertNotNull(walletAggregateController);
        assertNotNull(accountAggregateController);
        assertNotNull(orderProcessManagerAggregateController);
        assertNotNull(akcesClient);

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
        Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test");
        Consumer<String, AkcesControlRecord> controlConsumer = controlConsumerFactory.createConsumer("Test-AkcesControl", "test-akces-control");

        TopicPartition controlParition = new TopicPartition("Akces-Control", 0);
        controlConsumer.assign(List.of(controlParition));
        controlConsumer.seekToBeginning(controlConsumer.assignment());
        Map<TopicPartition, Long> endOffsets = controlConsumer.endOffsets(controlConsumer.assignment());

        while (endOffsets.getOrDefault(controlParition, 0L) > controlConsumer.position(controlParition)) {
            ConsumerRecords<String, AkcesControlRecord> controlRecords = controlConsumer.poll(Duration.ofMillis(1000));
            if (!controlRecords.isEmpty()) {
                for (ConsumerRecord<String, AkcesControlRecord> record : controlRecords.records(controlParition)) {
                    System.out.println(objectMapper.writeValueAsString(record.value()));
                }
            }
        }



        controlConsumer.close();


        while (!walletAggregateController.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "086fe270-f848-4b37-9858-f5311280a32e";
        CreateWalletCommand command = new CreateWalletCommand(userId, "USD");
        CommandRecord commandRecord = new CommandRecord(null, "CreateWallet", 1, objectMapper.writeValueAsBytes(command), PayloadEncoding.JSON, command.getAggregateId(), null, null);
        String topicName = walletAggregateController.resolveTopic(command.getClass());
        int partition = walletAggregateController.resolvePartition(command.getAggregateId());

        testProducer.beginTransaction();
        testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
        testProducer.commitTransaction();

        TopicPartition aggregateStatePartition = new TopicPartition("Wallet-AggregateState", partition);
        TopicPartition domainEventsPartition = new TopicPartition("Wallet-DomainEvents", partition);


        testConsumer.assign(List.of(aggregateStatePartition, domainEventsPartition));

        testConsumer.seekToBeginning(testConsumer.assignment());
        ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
        while (records.isEmpty()) {

            records = testConsumer.poll(Duration.ofMillis(250));
        }

        assertFalse(records.isEmpty());

        CreditWalletCommand creditCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("100.00"));
        CommandRecord creditCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(creditCommand), PayloadEncoding.JSON, creditCommand.getAggregateId(), null, null);

        testProducer.beginTransaction();
        testProducer.send(new ProducerRecord<>(topicName, partition, creditCommandRecord.aggregateId(), creditCommandRecord));
        testProducer.commitTransaction();

        records = testConsumer.poll(Duration.ofMillis(250));
        while (records.isEmpty()) {

            records = testConsumer.poll(Duration.ofMillis(250));
        }

        assertFalse(records.isEmpty());


        CreditWalletCommand invalidCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("-100.00"));
        CommandRecord invalidCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(invalidCommand), PayloadEncoding.JSON, invalidCommand.getAggregateId(), null, null);

        testProducer.beginTransaction();
        testProducer.send(new ProducerRecord<>(topicName, partition, invalidCommandRecord.aggregateId(), invalidCommandRecord));
        testProducer.commitTransaction();

        records = testConsumer.poll(Duration.ofMillis(250));
        while (records.isEmpty()) {

            records = testConsumer.poll(Duration.ofMillis(250));
        }


        assertTrue(records.records(aggregateStatePartition).isEmpty());

        assertEquals(1, records.records(domainEventsPartition).size());

        DomainEventRecord protocolRecord = (DomainEventRecord) records.records(domainEventsPartition).getFirst().value();
        assertEquals("InvalidAmountError", protocolRecord.name());

        testConsumer.close();
        testProducer.close();
    }

    @Test
    @Order(4)
    public void testBatchedCommands() throws JsonProcessingException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        List<String> userIds = List.of(
                "47db2418-dd10-11ed-afa1-0242ac120002",
                "47db2418-dd10-11ed-afa1-0242ac120003",
                "47db2418-dd10-11ed-afa1-0242ac120004",
                "47db2418-dd10-11ed-afa1-0242ac120005",
                "47db2418-dd10-11ed-afa1-0242ac120006",
                "47db2418-dd10-11ed-afa1-0242ac120007",
                "47db2418-dd10-11ed-afa1-0242ac120008",
                "47db2418-dd10-11ed-afa1-0242ac120009",
                "47db2418-dd10-11ed-afa1-0242ac120010",
                "47db2418-dd10-11ed-afa1-0242ac120011");
        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {


            Map<TopicPartition, Long> endOffsets = testConsumer.endOffsets(
                    Stream.concat(
                                    generateTopicPartitions("Wallet-AggregateState", 3),
                                    generateTopicPartitions("Wallet-DomainEvents", 3))
                            .toList());

            testProducer.beginTransaction();
            for (String userId : userIds) {
                CreateWalletCommand command = new CreateWalletCommand(userId, "USD");
                CommandRecord commandRecord = new CommandRecord(null, "CreateWallet", 1, objectMapper.writeValueAsBytes(command), PayloadEncoding.JSON, command.getAggregateId(), null, null);
                String topicName = walletAggregateController.resolveTopic(command.getClass());
                int partition = walletAggregateController.resolvePartition(command.getAggregateId());

                testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
            }
            testProducer.commitTransaction();

            testConsumer.subscribe(List.of("Wallet-AggregateState", "Wallet-DomainEvents"), new ConsumerRebalanceListener() {
                @Override
                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {

                }

                @Override
                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                    partitions.forEach(partition -> testConsumer.seek(partition, endOffsets.get(partition)));
                }
            });

            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();
            while (allRecords.size() < 40) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }

            assertEquals(40, allRecords.size());
        }

    }

    @Test
    @Order(5)
    public void testCreateViaExternalDomainEvent() throws JsonProcessingException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "47db2418-dd10-11ed-afa1-0242ac120012";

        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {


            Map<TopicPartition, Long> endOffsets = testConsumer.endOffsets(
                    Stream.concat(
                                    generateTopicPartitions("Wallet-AggregateState", 3),
                                    generateTopicPartitions("Wallet-DomainEvents", 3))
                            .toList());

            testProducer.beginTransaction();
            CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Fahim", "Zuijderwijk", "FahimZuijderwijk@jourrapide.com");
            CommandRecord commandRecord = new CommandRecord(
                    null,
                    "CreateAccount",
                    1,
                    objectMapper.writeValueAsBytes(command),
                    PayloadEncoding.JSON,
                    command.getAggregateId(),
                    null,
                    null);
            String topicName = walletAggregateController.resolveTopic(command.getClass());
            int partition = walletAggregateController.resolvePartition(command.getAggregateId());

            testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
            testProducer.commitTransaction();

            testConsumer.subscribe(List.of("Wallet-AggregateState", "Wallet-DomainEvents"), new ConsumerRebalanceListener() {
                @Override
                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {

                }

                @Override
                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                    partitions.forEach(partition -> testConsumer.seek(partition, endOffsets.get(partition)));
                }
            });

            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();
            while (allRecords.size() < 4) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }

            assertEquals(4, allRecords.size());
        }
    }

    @Test
    @Order(6)
    public void testGDPREncryption() throws IOException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "ca7c8e7f-d1a3-46ba-b400-f543d0c04bc6";

        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {


            Map<TopicPartition, Long> endOffsets = testConsumer.endOffsets(
                    Stream.concat(
                                    generateTopicPartitions("Account-AggregateState", 3),
                                    generateTopicPartitions("Account-DomainEvents", 3))
                            .toList());

            testProducer.beginTransaction();
            CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Fahim", "Zuijderwijk", "FahimZuijderwijk@jourrapide.com");
            CommandRecord commandRecord = new CommandRecord(
                    null,
                    "CreateAccount",
                    1,
                    objectMapper.writeValueAsBytes(command),
                    PayloadEncoding.JSON,
                    command.getAggregateId(),
                    null,
                    null);
            String topicName = walletAggregateController.resolveTopic(command.getClass());
            int partition = walletAggregateController.resolvePartition(command.getAggregateId());

            testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
            testProducer.commitTransaction();

            testConsumer.subscribe(List.of("Account-AggregateState", "Account-DomainEvents"), new ConsumerRebalanceListener() {
                @Override
                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {

                }

                @Override
                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                    partitions.forEach(partition -> testConsumer.seek(partition, endOffsets.get(partition)));
                }
            });

            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();
            while (allRecords.size() < 2) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }

            assertEquals(2, allRecords.size());

            assertTrue(allRecords.get(1) instanceof DomainEventRecord);
            DomainEventRecord domainEventRecord = (DomainEventRecord) allRecords.get(1);
            AccountCreatedEvent accountCreatedEvent = objectMapper.readValue(domainEventRecord.payload(), AccountCreatedEvent.class);
            assertNotEquals("Fahim", accountCreatedEvent.firstName());
            assertNotEquals("Zuijderwijk", accountCreatedEvent.lastName());
            assertNotEquals("FahimZuijderwijk@jourrapide.com", accountCreatedEvent.email());
            AggregateStateRecord stateRecord = (AggregateStateRecord) allRecords.get(0);
            AccountState accountState = objectMapper.readValue(stateRecord.payload(), AccountState.class);
            assertNotEquals("Fahim", accountState.firstName());
            assertNotEquals("Zuijderwijk", accountState.lastName());
            assertNotEquals("FahimZuijderwijk@jourrapide.com", accountState.email());
        }
    }

    @Test
    @Order(7)
    public void testDomainEventIndexing() throws IOException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "1837552e-45c4-41ff-a833-075c5a5fa49e";

        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {
            testProducer.beginTransaction();
            CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Fahim", "Zuijderwijk", "FahimZuijderwijk@jourrapide.com");
            CommandRecord commandRecord = new CommandRecord(
                    null,
                    "CreateAccount",
                    1,
                    objectMapper.writeValueAsBytes(command),
                    PayloadEncoding.JSON,
                    command.getAggregateId(),
                    null,
                    null);
            String topicName = walletAggregateController.resolveTopic(command.getClass());
            int partition = walletAggregateController.resolvePartition(command.getAggregateId());

            testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
            testProducer.commitTransaction();


            TopicDescription topicDescription = getTopicDescription("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex");
            while (topicDescription == null) {
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
                topicDescription = getTopicDescription("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex");
            }

            testConsumer.assign(generateTopicPartitions("Users-1837552e-45c4-41ff-a833-075c5a5fa49e-DomainEventIndex", 1).toList());
            testConsumer.seekToBeginning(testConsumer.assignment());
            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();

            while (allRecords.size() < 4) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }
            assertEquals(4, allRecords.size());

            List<String> names = allRecords.stream().map(ProtocolRecord::name).toList();

            assertEquals("AccountCreated", names.getFirst());
            assertTrue(names.indexOf("WalletCreated") < names.indexOf("BalanceCreated"));
            assertTrue(names.contains("UserOrderProcessesCreated"));
        }
    }

    @Test
    @Order(8)
    public void testDomainEventIndexingWithErrorEvents() throws IOException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }
        String userId = "d3bd665a-6c67-4301-a8f1-4381f8d7d567";
        try (
                Producer<String, ProtocolRecord> testProducer = producerFactory.createProducer("test");
                Consumer<String, ProtocolRecord> testConsumer = consumerFactory.createConsumer("Test", "test")
        ) {
            CreateWalletCommand command = new CreateWalletCommand(userId, "USD");
            CommandRecord commandRecord = new CommandRecord(null, "CreateWallet", 1, objectMapper.writeValueAsBytes(command), PayloadEncoding.JSON, command.getAggregateId(), null, null);
            String topicName = walletAggregateController.resolveTopic(command.getClass());
            int partition = walletAggregateController.resolvePartition(command.getAggregateId());

            testProducer.beginTransaction();
            testProducer.send(new ProducerRecord<>(topicName, partition, commandRecord.aggregateId(), commandRecord));
            testProducer.commitTransaction();

            CreditWalletCommand creditCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("100.00"));
            CommandRecord creditCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(creditCommand), PayloadEncoding.JSON, creditCommand.getAggregateId(), null, null);
            testProducer.beginTransaction();
            testProducer.send(new ProducerRecord<>(topicName, partition, creditCommandRecord.aggregateId(), creditCommandRecord));
            testProducer.commitTransaction();

            CreditWalletCommand invalidCommand = new CreditWalletCommand(userId, "USD", new BigDecimal("-100.00"));
            CommandRecord invalidCommandRecord = new CommandRecord(null, "CreditWallet", 1, objectMapper.writeValueAsBytes(invalidCommand), PayloadEncoding.JSON, invalidCommand.getAggregateId(), null, null);

            testProducer.beginTransaction();
            testProducer.send(new ProducerRecord<>(topicName, partition, invalidCommandRecord.aggregateId(), invalidCommandRecord));
            testProducer.commitTransaction();


            TopicDescription topicDescription = getTopicDescription("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex");
            while (topicDescription == null) {
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
                topicDescription = getTopicDescription("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex");
            }


            testConsumer.assign(generateTopicPartitions("Users-d3bd665a-6c67-4301-a8f1-4381f8d7d567-DomainEventIndex", 1).toList());
            testConsumer.seekToBeginning(testConsumer.assignment());
            ConsumerRecords<String, ProtocolRecord> records = testConsumer.poll(Duration.ofMillis(250));
            List<ProtocolRecord> allRecords = new ArrayList<>();
            while (allRecords.size() < 3) {
                records.forEach(record -> allRecords.add(record.value()));

                records = testConsumer.poll(Duration.ofMillis(250));
            }

            assertEquals(3, allRecords.size());

            assertEquals("WalletCreated", allRecords.getFirst().name());
            assertEquals("BalanceCreated", allRecords.get(1).name());
            assertEquals("WalletCredited", allRecords.getLast().name());

        }
    }

    @Test
    @Order(9)
    public void testWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "243f2482-d81f-42cb-91f1-105a79f35e34";
        CreateWalletCommand command = new CreateWalletCommand(userId, "USD");
        List<DomainEvent> result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);

        Assertions.assertNotNull(result);
        Assertions.assertEquals(2, result.size());
        assertInstanceOf(WalletCreatedEvent.class, result.getFirst());
        assertInstanceOf(BalanceCreatedEvent.class, result.getLast());
    }

    @Test
    @Order(10)
    public void testOrderFlowWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "a28d41c4-9f9c-4708-b142-6b83768ee8f3";
        CreateAccountCommand command = new CreateAccountCommand(userId, "NL", "Bella", "Fowler", "bella.fowler@example.com");
        List<DomainEvent> result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(AccountCreatedEvent.class, result.getFirst());

        akcesClient.sendAndForget("TEST_TENANT", new CreditWalletCommand(userId, "EUR", new BigDecimal("100.00")));

        PlaceBuyOrderCommand orderCommand = new PlaceBuyOrderCommand(userId, new FxMarket("USDEUR", "USD", "EUR"), new BigDecimal("90.00"), new BigDecimal("1.05"), "trade-1");
        result = akcesClient.send("TEST_TENANT", orderCommand).toCompletableFuture().get(10, TimeUnit.SECONDS);

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(BuyOrderCreatedEvent.class, result.getFirst());
    }

    @Test
    @Order(11)
    public void testAggregateAlreadyExistsErrorWithAkcesClient() throws ExecutionException, InterruptedException, TimeoutException {

        while (!walletAggregateController.isRunning() ||
                !accountAggregateController.isRunning() ||
                !orderProcessManagerAggregateController.isRunning() ||
                !akcesClient.isRunning()) {
            Thread.onSpinWait();
        }

        String userId = "854c0c16-b3be-4d04-8ce8-a606eec89a1f";

        CreateAccountCommand command = new CreateAccountCommand(userId, "USA", "Angelo", "Plummer", "AngeloRPlummer@rhyta.com");
        List<DomainEvent> result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);

        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(AccountCreatedEvent.class, result.getFirst());


        result = akcesClient.send("TEST_TENANT", command).toCompletableFuture().get(10, TimeUnit.SECONDS);
        Assertions.assertNotNull(result);
        Assertions.assertEquals(1, result.size());
        assertInstanceOf(AggregateAlreadyExistsErrorEvent.class, result.getFirst());


    }

    public TopicDescription getTopicDescription(String topic) {
        try {
            return adminClient.describeTopics(topic).get(topic);
        } catch (KafkaException e) {
            if (e.getCause().getClass().equals(ExecutionException.class) &&
                    e.getCause().getCause().getClass().equals(UnknownTopicOrPartitionException.class)) {
                return null;
            } else {
                throw e;
            }
        }
    }

    public static class DataSourceInitializer
            implements ApplicationContextInitializer<ConfigurableApplicationContext> {

        @Override
        public void initialize(ConfigurableApplicationContext applicationContext) {

            prepareKafka(kafka.getBootstrapServers());
            SchemaRegistryClient src = new CachedSchemaRegistryClient("http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081), 100);
            prepareExternalSchemas(src, List.of(AccountCreatedEvent.class));
            prepareOldCommandSchemas(src);
            prepareOldDomainEventSchemas(src);
            try {
                prepareAggregateServiceRecords(kafka.getBootstrapServers());
            } catch (IOException e) {
                throw new RuntimeException(e);
            }

            TestPropertySourceUtils.addInlinedPropertiesToEnvironment(
                    applicationContext,
                    "akces.aggregate.schemas.forceRegister=true",
                    "akces.rocksdb.baseDir=/tmp/akces",
                    "spring.kafka.enabled=true",
                    "spring.kafka.bootstrap-servers=" + kafka.getBootstrapServers(),
                    "akces.schemaregistry.url=http://" + schemaRegistry.getHost() + ":" + schemaRegistry.getMappedPort(8081)
            );
        }
    }

}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/TestUtils.java
================
package org.elasticsoftware.akcestest;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.github.victools.jsonschema.generator.*;
import com.github.victools.jsonschema.module.jackson.JacksonModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationOption;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;
import org.elasticsoftware.akces.events.DomainEvent;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.serialization.AkcesControlRecordSerde;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.springframework.context.ApplicationContextException;
import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;
import org.springframework.kafka.core.KafkaAdmin;

import java.io.IOException;
import java.math.BigDecimal;
import java.util.List;
import java.util.Map;

public class TestUtils {
    public static void prepareKafka(String bootstrapServers) {
        KafkaAdmin kafkaAdmin = new KafkaAdmin(Map.of(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));
        kafkaAdmin.createOrModifyTopics(
                createCompactedTopic("Akces-Control", 3),
                createTopic("Akces-CommandResponses", 3, 604800000L),
                createCompactedTopic("Akces-GDPRKeys", 3),
                createTopic("Wallet-Commands", 3),
                createTopic("Wallet-DomainEvents", 3),
                createTopic("Account-Commands", 3),
                createTopic("Account-DomainEvents", 3),
                createTopic("OrderProcessManager-Commands", 3),
                createTopic("OrderProcessManager-DomainEvents", 3),
                createCompactedTopic("Wallet-AggregateState", 3),
                createCompactedTopic("Account-AggregateState", 3),
                createCompactedTopic("OrderProcessManager-AggregateState", 3));
    }

    private static NewTopic createTopic(String name, int numPartitions) {
        return createTopic(name, numPartitions, -1L);
    }

    private static NewTopic createTopic(String name, int numPartitions, long retentionMs) {
        NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
        return topic.configs(Map.of(
                "cleanup.policy", "delete",
                "max.message.bytes", "20971520",
                "retention.ms", Long.toString(retentionMs),
                "segment.ms", "604800000"));
    }

    private static NewTopic createCompactedTopic(String name, int numPartitions) {
        NewTopic topic = new NewTopic(name, numPartitions, Short.parseShort("1"));
        return topic.configs(Map.of(
                "cleanup.policy", "compact",
                "max.message.bytes", "20971520",
                "retention.ms", "-1",
                "segment.ms", "604800000",
                "min.cleanable.dirty.ratio", "0.1",
                "delete.retention.ms", "604800000",
                "compression.type", "lz4"));
    }

    public static <E extends DomainEvent> void prepareExternalSchemas(SchemaRegistryClient src, List<Class<E>> externalDomainEvents) {
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_7, OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS, JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);
        SchemaGeneratorConfig config = configBuilder.build();
        SchemaGenerator jsonSchemaGenerator = new SchemaGenerator(config);
        try {
            for (Class<E> eventClass : externalDomainEvents) {
                DomainEventInfo info = eventClass.getAnnotation(DomainEventInfo.class);
                src.register("domainevents." + info.type(),
                        new JsonSchema(jsonSchemaGenerator.generateSchema(eventClass), List.of(), Map.of(), info.version()),
                        info.version(),
                        -1);
            }
        } catch (IOException | RestClientException e) {
            throw new ApplicationContextException("Problem populating SchemaRegistry", e);
        }
    }

    public static void prepareAggregateServiceRecords(String bootstrapServers) throws IOException {
        Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder();
        builder.modulesToInstall(new AkcesGDPRModule());
        builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        ObjectMapper objectMapper = builder.build();
        AkcesControlRecordSerde controlSerde = new AkcesControlRecordSerde(objectMapper);
        Map<String, Object> controlProducerProps = Map.of(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers,
                ProducerConfig.ACKS_CONFIG, "all",
                ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true",
                ProducerConfig.LINGER_MS_CONFIG, "0",
                ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "1",
                ProducerConfig.RETRIES_CONFIG, "2147483647",
                ProducerConfig.RETRY_BACKOFF_MS_CONFIG, "0",
                ProducerConfig.TRANSACTIONAL_ID_CONFIG, "Test-AkcesControllerProducer",
                ProducerConfig.CLIENT_ID_CONFIG, "Test-AkcesControllerProducer");
        try (Producer<String, AkcesControlRecord> controlProducer = new KafkaProducer<>(controlProducerProps, new StringSerializer(), controlSerde.serializer())) {
            controlProducer.initTransactions();
            AggregateServiceRecord accountServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Account\",\"commandTopic\":\"Account-Commands\",\"domainEventTopic\":\"Account-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"CreateAccount\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateAccount\"}],\"producedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[]}", AggregateServiceRecord.class);
            AggregateServiceRecord orderProcessManagerServiceRecord = objectMapper.readValue("{\"aggregateName\":\"OrderProcessManager\",\"commandTopic\":\"OrderProcessManager-Commands\",\"domainEventTopic\":\"OrderProcessManager-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"PlaceBuyOrder\",\"version\":1,\"create\":false,\"schemaName\":\"commands.PlaceBuyOrder\"}],\"producedEvents\":[{\"typeName\":\"BuyOrderRejected\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderRejected\"},{\"typeName\":\"BuyOrderCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderCreated\"},{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"UserOrderProcessesCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.UserOrderProcessesCreated\"},{\"typeName\":\"BuyOrderPlaced\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BuyOrderPlaced\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"}],\"consumedEvents\":[{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":true,\"schemaName\":\"domainevents.AmountReserved\"}]}", AggregateServiceRecord.class);
            AggregateServiceRecord walletServiceRecord = objectMapper.readValue("{\"aggregateName\":\"Wallet\",\"commandTopic\":\"Wallet-Commands\",\"domainEventTopic\":\"Wallet-DomainEvents\",\"supportedCommands\":[{\"typeName\":\"ReserveAmount\",\"version\":1,\"create\":false,\"schemaName\":\"commands.ReserveAmount\"},{\"typeName\":\"CreateWallet\",\"version\":1,\"create\":true,\"schemaName\":\"commands.CreateWallet\"},{\"typeName\":\"CreateBalance\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreateBalance\"},{\"typeName\":\"CreditWallet\",\"version\":1,\"create\":false,\"schemaName\":\"commands.CreditWallet\"}],\"producedEvents\":[{\"typeName\":\"AggregateAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AggregateAlreadyExistsError\"},{\"typeName\":\"CommandExecutionError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.CommandExecutionError\"},{\"typeName\":\"BalanceCreated\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceCreated\"},{\"typeName\":\"AmountReserved\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.AmountReserved\"},{\"typeName\":\"BalanceAlreadyExistsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.BalanceAlreadyExistsError\"},{\"typeName\":\"WalletCredited\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.WalletCredited\"},{\"typeName\":\"InsufficientFundsError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InsufficientFundsError\"},{\"typeName\":\"WalletCreated\",\"version\":1,\"create\":true,\"external\":false,\"schemaName\":\"domainevents.WalletCreated\"},{\"typeName\":\"InvalidCurrencyError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidCurrencyError\"},{\"typeName\":\"InvalidAmountError\",\"version\":1,\"create\":false,\"external\":false,\"schemaName\":\"domainevents.InvalidAmountError\"}],\"consumedEvents\":[{\"typeName\":\"AccountCreated\",\"version\":1,\"create\":true,\"external\":true,\"schemaName\":\"domainevents.AccountCreated\"}]}", AggregateServiceRecord.class);
            controlProducer.beginTransaction();
            for (int partition = 0; partition < 3; partition++) {
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Account", accountServiceRecord));
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "OrderProcessManager", orderProcessManagerServiceRecord));
                controlProducer.send(new ProducerRecord<>("Akces-Control", partition, "Wallet", walletServiceRecord));
            }
            controlProducer.commitTransaction();
        }
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/WalletConfiguration.java
================
package org.elasticsoftware.akcestest;

import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import org.elasticsoftware.akces.beans.AggregateBeanFactoryPostProcessor;
import org.elasticsoftware.akces.gdpr.jackson.AkcesGDPRModule;
import org.elasticsoftware.akces.schemas.KafkaSchemaRegistry;
import org.elasticsoftware.akces.serialization.BigDecimalSerializer;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.boot.autoconfigure.jackson.Jackson2ObjectMapperBuilderCustomizer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.ComponentScan;

import java.math.BigDecimal;

@EnableAutoConfiguration
@ComponentScan(basePackages = {
        "org.elasticsoftware.akcestest.aggregate.wallet",
})
public class WalletConfiguration {
    @Bean(name = "aggregateServiceBeanFactoryPostProcessor")
    public AggregateBeanFactoryPostProcessor aggregateBeanFactoryPostProcessor() {
        return new AggregateBeanFactoryPostProcessor();
    }

    @Bean(name = "aggregateServiceJsonCustomizer")
    public Jackson2ObjectMapperBuilderCustomizer jsonCustomizer() {
        return builder -> {
            builder.modulesToInstall(new AkcesGDPRModule());
            builder.serializerByType(BigDecimal.class, new BigDecimalSerializer());
        };
    }

    @Bean(name = "aggregateServiceSchemaRegistryClient")
    public SchemaRegistryClient createSchemaRegistryClient() {
        return new MockSchemaRegistryClient();
    }

    @Bean(name = "aggregateServiceSchemaRegistry")
    public KafkaSchemaRegistry createSchemaRegistry(@Qualifier("aggregateServiceSchemaRegistryClient") SchemaRegistryClient src,
                                                    ObjectMapper objectMapper) {
        return new KafkaSchemaRegistry(src, objectMapper);
    }
}

================
File: runtime/src/test/java/org/elasticsoftware/akcestest/WalletTests.java
================
package org.elasticsoftware.akcestest;

import com.fasterxml.jackson.databind.ObjectMapper;
import io.confluent.kafka.schemaregistry.ParsedSchema;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import jakarta.inject.Inject;
import org.elasticsoftware.akces.aggregate.*;
import org.elasticsoftware.akces.protocol.*;
import org.elasticsoftware.akces.schemas.*;
import org.elasticsoftware.akcestest.aggregate.account.AccountCreatedEvent;
import org.elasticsoftware.akcestest.aggregate.wallet.*;
import org.elasticsoftware.akcestest.schemas.AccountCreatedEventV2;
import org.elasticsoftware.akcestest.schemas.AccountCreatedEventV3;
import org.elasticsoftware.akcestest.schemas.NotCompatibleAccountCreatedEventV4;
import org.junit.jupiter.api.*;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.context.ApplicationContext;

import java.math.BigDecimal;
import java.util.ArrayList;
import java.util.List;

import static org.junit.jupiter.api.Assertions.*;


@SpringBootTest(classes = WalletConfiguration.class, properties = "spring.autoconfigure.exclude=org.elasticsoftware.akces.client.AkcesClientAutoConfiguration")
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
public class WalletTests {
    @Inject
    ApplicationContext applicationContext;
    @Inject
    ObjectMapper objectMapper;
    @Inject
    KafkaSchemaRegistry schemaRegistry;
    @Inject
    SchemaRegistryClient schemaRegistryClient;

    @Test
    public void testFindBeans() {
        assertEquals(4, applicationContext.getBeansOfType(CommandHandlerFunction.class).size());
        assertEquals(1, applicationContext.getBeansOfType(EventHandlerFunction.class).size());
        assertEquals(4, applicationContext.getBeansOfType(EventSourcingHandlerFunction.class).size());
        Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_create_CreateWallet_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_credit_CreditWallet_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_makeReservation_ReserveAmount_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_ch_createBalance_CreateBalance_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_eh_create_AccountCreated_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_create_WalletCreated_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_createBalance_BalanceCreated_1"));
        Assertions.assertNotNull(applicationContext.getBean("Wallet_esh_credit_WalletCredited_1"));
    }

    @Test
    @Order(1)
    public void testValidateDomainEventsWithMissingExternalDomainEventSchema() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);
        assertThrows(SchemaNotFoundException.class, () -> {
            for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
                walletAggregate.registerAndValidate(domainEventType);
            }
        });
        System.out.println(schemaRegistryClient.getAllSubjects());
    }

    @Test
    public void testValidateDomainEvents() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, AccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        System.out.println(schemaRegistryClient.getAllSubjects());
    }

    @Test
    public void testValidateDomainEventsWithExistingSchemas() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, AccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        schemaRegistryClient.register("domainevents.WalletCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCreated", 1, WalletCreatedEvent.class, true, false, false)),
                1,
                -1);
        schemaRegistryClient.register("domainevents.WalletCredited",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCredited", 1, WalletCreditedEvent.class, false, false, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        System.out.println(schemaRegistryClient.getAllSubjects());
    }

    @Test
    public void testValidateDomainEventsWithExistingSchemasAndExternalEventSubset() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        schemaRegistryClient.register("domainevents.WalletCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCreated", 1, WalletCreatedEvent.class, true, false, false)),
                1,
                -1);
        schemaRegistryClient.register("domainevents.WalletCredited",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCredited", 1, WalletCreditedEvent.class, false, false, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
    }

    @Test
    public void testValidateDomainEventsWithExistingSchemasAndInvalidExternalEvent() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, false, false)),
                1,
                -1);
        schemaRegistryClient.register("domainevents.WalletCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCreated", 1, WalletCreatedEvent.class, true, false, false)),
                1,
                -1);
        schemaRegistryClient.register("domainevents.WalletCredited",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("WalletCredited", 1, WalletCreditedEvent.class, false, false, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        Assertions.assertThrows(IncompatibleSchemaException.class, () ->
                walletAggregate.registerAndValidate(new DomainEventType<>("AccountCreated", 1, InvalidAccountCreatedEvent.class, true, true, false)));
    }

    @Test
    public void testRegisterAndValidateMultipleVersionsOfEvent() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);
        walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false));
        walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 2, AccountCreatedEventV2.class, true, false, false));
        walletAggregate.registerAndValidate(new DomainEventType<>("TestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false));
        List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas("domainevents.TestAccountCreated", false, false);
        assertEquals(3, registeredSchemas.size());
    }

    @Test
    public void testRegisterAndValidateMultipleVersionsOfEventWithSkippedVersion() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);
        walletAggregate.registerAndValidate(new DomainEventType<>("AnotherTestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false));
        Assertions.assertThrows(InvalidSchemaVersionException.class, () ->
                walletAggregate.registerAndValidate(new DomainEventType<>("AnotherTestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false)));
    }

    @Test
    public void testRegisterAndValidateMultipleVersionsOfEventWithNonCompatibleEvent() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);
        walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 1, org.elasticsoftware.akcestest.schemas.AccountCreatedEvent.class, true, false, false));
        walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 2, AccountCreatedEventV2.class, true, false, false));
        walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 3, AccountCreatedEventV3.class, true, false, false));
        SchemaNotBackwardsCompatibleException exception = Assertions.assertThrows(SchemaNotBackwardsCompatibleException.class, () -> {
            walletAggregate.registerAndValidate(new DomainEventType<>("YetAnotherTestAccountCreated", 4, NotCompatibleAccountCreatedEventV4.class, true, false, false));
        });
        assertEquals("Schema not backwards compatible with previous version: 3", exception.getMessage());
    }

    @Test
    public void testCreateWalletByCommand() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        String tenantId = "tenant1";
        String aggregateId = "d43a3afc-3e5a-11ed-b878-0242ac120002";
        String correlationId = "01e04622-3e5b-11ed-b878-0242ac120002";
        List<ProtocolRecord> producedRecords = new ArrayList<>();
        List<DomainEventRecord> indexedEvents = new ArrayList<>();
        walletAggregate.handleCommandRecord(
                new CommandRecord(
                        tenantId,
                        "CreateWallet",
                        1,
                        objectMapper.writeValueAsBytes(
                                new CreateWalletCommand(aggregateId, "EUR")),
                        PayloadEncoding.JSON,
                        aggregateId,
                        correlationId,
                        null),
                producedRecords::add,
                (eventRecord, index) -> indexedEvents.add(eventRecord),
                () -> null
        );
        assertEquals(4, producedRecords.size());
        AggregateStateRecord actualRecord = (AggregateStateRecord) producedRecords.get(0);
        AggregateStateRecord expectedRecord = new AggregateStateRecord(
                tenantId,
                "Wallet",
                1,
                objectMapper.writeValueAsBytes(new WalletState(aggregateId, new ArrayList<>())),
                PayloadEncoding.JSON,
                aggregateId,
                correlationId,
                1);
        assertEquals(expectedRecord.generation(), actualRecord.generation());
        assertEquals(expectedRecord.aggregateId(), actualRecord.aggregateId());
        assertEquals(expectedRecord.correlationId(), actualRecord.correlationId());
        assertArrayEquals(expectedRecord.payload(), actualRecord.payload());
        assertEquals(expectedRecord.encoding(), actualRecord.encoding());
        assertEquals(expectedRecord.name(), actualRecord.name());
        assertEquals(expectedRecord.version(), actualRecord.version());

        DomainEventRecord actual = (DomainEventRecord) producedRecords.get(1);

        assertEquals(1, actual.generation());
        assertEquals(aggregateId, actual.aggregateId());
        assertEquals(correlationId, actual.correlationId());
        assertArrayEquals(objectMapper.writeValueAsBytes(new WalletCreatedEvent(aggregateId)), actual.payload());
        assertEquals(PayloadEncoding.JSON, actual.encoding());
        assertEquals("WalletCreated", actual.name());
        assertEquals(1, actual.version());


        actualRecord = (AggregateStateRecord) producedRecords.get(2);
        expectedRecord = new AggregateStateRecord(
                tenantId,
                "Wallet",
                1,
                objectMapper.writeValueAsBytes(new WalletState(aggregateId, List.of(new WalletState.Balance("EUR", BigDecimal.ZERO)))),
                PayloadEncoding.JSON,
                aggregateId,
                correlationId,
                2);
        assertEquals(expectedRecord.generation(), actualRecord.generation());
        assertEquals(expectedRecord.aggregateId(), actualRecord.aggregateId());
        assertEquals(expectedRecord.correlationId(), actualRecord.correlationId());
        assertArrayEquals(expectedRecord.payload(), actualRecord.payload());
        assertEquals(expectedRecord.encoding(), actualRecord.encoding());
        assertEquals(expectedRecord.name(), actualRecord.name());
        assertEquals(expectedRecord.version(), actualRecord.version());

        actual = (DomainEventRecord) producedRecords.get(3);

        assertEquals(2, actual.generation());
        assertEquals(aggregateId, actual.aggregateId());
        assertEquals(correlationId, actual.correlationId());
        assertArrayEquals(objectMapper.writeValueAsBytes(new BalanceCreatedEvent(aggregateId, "EUR")), actual.payload());
        assertEquals(PayloadEncoding.JSON, actual.encoding());
        assertEquals("BalanceCreated", actual.name());
        assertEquals(1, actual.version());

    }

    @Test
    public void testIndexWalletEventsFromCommand() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        String tenantId = "tenant1";
        String aggregateId = "d43a3afc-3e5a-11ed-b878-0242ac120002";
        String correlationId = "01e04622-3e5b-11ed-b878-0242ac120002";
        List<ProtocolRecord> producedRecords = new ArrayList<>();
        List<DomainEventRecord> indexedEvents = new ArrayList<>();
        walletAggregate.handleCommandRecord(
                new CommandRecord(
                        tenantId,
                        "CreateWallet",
                        1,
                        objectMapper.writeValueAsBytes(
                                new CreateWalletCommand(aggregateId, "EUR")),
                        PayloadEncoding.JSON,
                        aggregateId,
                        correlationId,
                        null),
                producedRecords::add,
                (eventRecord, index) -> indexedEvents.add(eventRecord),
                () -> null
        );

        assertEquals(2, indexedEvents.size());
        DomainEventRecord actual = indexedEvents.get(0);

        assertEquals(1, actual.generation());
        assertEquals(aggregateId, actual.aggregateId());
        assertEquals(correlationId, actual.correlationId());
        assertArrayEquals(objectMapper.writeValueAsBytes(new WalletCreatedEvent(aggregateId)), actual.payload());
        assertEquals(PayloadEncoding.JSON, actual.encoding());
        assertEquals("WalletCreated", actual.name());
        assertEquals(1, actual.version());

        actual = (DomainEventRecord) indexedEvents.get(1);

        assertEquals(2, actual.generation());
        assertEquals(aggregateId, actual.aggregateId());
        assertEquals(correlationId, actual.correlationId());
        assertArrayEquals(objectMapper.writeValueAsBytes(new BalanceCreatedEvent(aggregateId, "EUR")), actual.payload());
        assertEquals(PayloadEncoding.JSON, actual.encoding());
        assertEquals("BalanceCreated", actual.name());
        assertEquals(1, actual.version());
    }

    @Test
    public void testCreateWalletByExternalDomainEvent() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        String tenantId = "tenant1";
        String aggregateId = "d43a3afc-3e5a-11ed-b878-0242ac120002";
        String correlationId = "01e04622-3e5b-11ed-b878-0242ac120002";
        List<ProtocolRecord> producedRecords = new ArrayList<>();
        List<DomainEventRecord> indexedEvents = new ArrayList<>();
        walletAggregate.handleExternalDomainEventRecord(
                new DomainEventRecord(
                        tenantId,
                        "AccountCreated",
                        1,
                        objectMapper.writeValueAsBytes(
                                new AccountCreatedEvent(aggregateId, "NL", "7hdU_mfA_bvkRRgCekTZ0A==", "ioxbJd-hSLj6KNJpdYzN4g==", "6KLIDo3Ii2d-oVZtiv1h3OYNgW5lXYAnCnxPK2fprUU=")),
                        PayloadEncoding.JSON,
                        aggregateId,
                        correlationId,
                        1),
                producedRecords::add,
                (eventRecord, index) -> indexedEvents.add(eventRecord),
                () -> null
        );
        assertEquals(4, producedRecords.size());
        AggregateStateRecord actualRecord = (AggregateStateRecord) producedRecords.get(0);
        AggregateStateRecord expectedRecord = new AggregateStateRecord(
                tenantId,
                "Wallet",
                1,
                objectMapper.writeValueAsBytes(new WalletState(aggregateId, new ArrayList<>())),
                PayloadEncoding.JSON,
                aggregateId,
                correlationId,
                1);
        assertEquals(expectedRecord.generation(), actualRecord.generation());
        assertEquals(expectedRecord.aggregateId(), actualRecord.aggregateId());
        assertEquals(expectedRecord.correlationId(), actualRecord.correlationId());
        assertArrayEquals(expectedRecord.payload(), actualRecord.payload());
        assertEquals(expectedRecord.encoding(), actualRecord.encoding());
        assertEquals(expectedRecord.name(), actualRecord.name());
        assertEquals(expectedRecord.version(), actualRecord.version());

        DomainEventRecord actual = (DomainEventRecord) producedRecords.get(1);

        assertEquals(1, actual.generation());
        assertEquals(aggregateId, actual.aggregateId());
        assertEquals(correlationId, actual.correlationId());
        assertArrayEquals(objectMapper.writeValueAsBytes(new WalletCreatedEvent(aggregateId)), actual.payload());
        assertEquals(PayloadEncoding.JSON, actual.encoding());
        assertEquals("WalletCreated", actual.name());
        assertEquals(1, actual.version());


        actualRecord = (AggregateStateRecord) producedRecords.get(2);
        expectedRecord = new AggregateStateRecord(
                tenantId,
                "Wallet",
                1,
                objectMapper.writeValueAsBytes(new WalletState(aggregateId, List.of(new WalletState.Balance("EUR", BigDecimal.ZERO)))),
                PayloadEncoding.JSON,
                aggregateId,
                correlationId,
                2);
        assertEquals(expectedRecord.generation(), actualRecord.generation());
        assertEquals(expectedRecord.aggregateId(), actualRecord.aggregateId());
        assertEquals(expectedRecord.correlationId(), actualRecord.correlationId());
        assertArrayEquals(expectedRecord.payload(), actualRecord.payload());
        assertEquals(expectedRecord.encoding(), actualRecord.encoding());
        assertEquals(expectedRecord.name(), actualRecord.name());
        assertEquals(expectedRecord.version(), actualRecord.version());

        actual = (DomainEventRecord) producedRecords.get(3);

        assertEquals(2, actual.generation());
        assertEquals(aggregateId, actual.aggregateId());
        assertEquals(correlationId, actual.correlationId());
        assertArrayEquals(objectMapper.writeValueAsBytes(new BalanceCreatedEvent(aggregateId, "EUR")), actual.payload());
        assertEquals(PayloadEncoding.JSON, actual.encoding());
        assertEquals("BalanceCreated", actual.name());
        assertEquals(1, actual.version());

    }

    @Test
    public void testIndexWalletEventsByExternalDomainEvent() throws Exception {
        AggregateRuntime walletAggregate = applicationContext.getBean("WalletAggregateRuntimeFactory", AggregateRuntime.class);

        schemaRegistryClient.register("domainevents.AccountCreated",
                schemaRegistry.generateJsonSchema(new DomainEventType<>("AccountCreated", 1, ExternalAccountCreatedEvent.class, true, true, false)),
                1,
                -1);
        for (DomainEventType<?> domainEventType : walletAggregate.getAllDomainEventTypes()) {
            walletAggregate.registerAndValidate(domainEventType);
        }
        String tenantId = "tenant1";
        String aggregateId = "d43a3afc-3e5a-11ed-b878-0242ac120002";
        String correlationId = "01e04622-3e5b-11ed-b878-0242ac120002";
        List<ProtocolRecord> producedRecords = new ArrayList<>();
        List<DomainEventRecord> indexedEvents = new ArrayList<>();
        walletAggregate.handleExternalDomainEventRecord(
                new DomainEventRecord(
                        tenantId,
                        "AccountCreated",
                        1,
                        objectMapper.writeValueAsBytes(
                                new AccountCreatedEvent(aggregateId, "NL", "7hdU_mfA_bvkRRgCekTZ0A==", "ioxbJd-hSLj6KNJpdYzN4g==", "6KLIDo3Ii2d-oVZtiv1h3OYNgW5lXYAnCnxPK2fprUU=")),
                        PayloadEncoding.JSON,
                        aggregateId,
                        correlationId,
                        1),
                producedRecords::add,
                (eventRecord, index) -> indexedEvents.add(eventRecord),
                () -> null
        );

        assertEquals(2, indexedEvents.size());
    }
}

================
File: runtime/src/test/resources/akces-client.properties
================
#
# Copyright 2022 - 2025 The Original Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
#     you may not use this file except in compliance with the License.
#     You may obtain a copy of the License at
#
#           http://www.apache.org/licenses/LICENSE-2.0
#
#     Unless required by applicable law or agreed to in writing, software
#     distributed under the License is distributed on an "AS IS" BASIS,
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#     See the License for the specific language governing permissions and
#     limitations under the License.
#
#
akces.client.domainEventsPackage=org.elasticsoftware.akcestest.aggregate

================
File: runtime/src/test/resources/logback-test.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<configuration>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>

    <logger name="org.apache.kafka" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.producer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.apache.kafka.clients.consumer" level="warn" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.kafka" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.gdpr" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.state" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <logger name="org.elasticsoftware.akces.client" level="trace" additivity="false">
        <appender-ref ref="CONSOLE"/>
    </logger>

    <root level="info">
        <appender-ref ref="CONSOLE"/>
    </root>

</configuration>

================
File: runtime/src/test/resources/test-application.yaml
================
spring:
  main:
    allow-bean-definition-overriding: true

================
File: runtime/pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.7.21-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>akces-runtime</artifactId>
    <packaging>jar</packaging>

    <name>Elastic Software Foundation :: Akces :: Runtime</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-shared</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>com.google.protobuf</groupId>
            <artifactId>protobuf-java</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-autoconfigure</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-json</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-beans</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.validation</groupId>
            <artifactId>jakarta.validation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.inject</groupId>
            <artifactId>jakarta.inject-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-streams</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-protobuf-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
        </dependency>
        <dependency>
            <groupId>org.semver4j</groupId>
            <artifactId>semver4j</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>kafka</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>com.vaadin.external.google</groupId>
                    <artifactId>android-json</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka_2.13</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.kafka</groupId>
                    <artifactId>kafka-clients</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-client</artifactId>
            <scope>test</scope>
            <version>${project.version}</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <executions>
                    <execution>
                        <goals>
                            <goal>test-jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>

================
File: shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceCommandType.java
================
package org.elasticsoftware.akces.control;

import org.elasticsoftware.akces.aggregate.CommandType;
import org.elasticsoftware.akces.commands.Command;

public record AggregateServiceCommandType(
        String typeName,
        int version,
        boolean create,
        String schemaName
) {
    public <C extends Command> CommandType<C> toLocalCommandType(Class<C> typeClass) {
        return new CommandType<>(typeName, version, typeClass, create, true);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceDomainEventType.java
================
package org.elasticsoftware.akces.control;

import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.events.DomainEvent;

public record AggregateServiceDomainEventType(
        String typeName,
        int version,
        boolean create,
        boolean external,
        String schemaName
) {
    public <E extends DomainEvent> DomainEventType<E> toLocalDomainEventType(Class<E> typeClass, boolean error) {
        return new DomainEventType<>(typeName, version, typeClass, create, external, error);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/control/AggregateServiceRecord.java
================
package org.elasticsoftware.akces.control;

import java.util.List;

public record AggregateServiceRecord(
        String aggregateName,
        String commandTopic,
        String domainEventTopic,
        List<AggregateServiceCommandType> supportedCommands,
        List<AggregateServiceDomainEventType> producedEvents,
        List<AggregateServiceDomainEventType> consumedEvents
) implements AkcesControlRecord {
}

================
File: shared/src/main/java/org/elasticsoftware/akces/control/AkcesControlRecord.java
================
package org.elasticsoftware.akces.control;

import com.fasterxml.jackson.annotation.JsonSubTypes;
import com.fasterxml.jackson.annotation.JsonTypeInfo;

@JsonTypeInfo(use = JsonTypeInfo.Id.DEDUCTION)
@JsonSubTypes(
        @JsonSubTypes.Type(AggregateServiceRecord.class)
)
public sealed interface AkcesControlRecord permits AggregateServiceRecord {
}

================
File: shared/src/main/java/org/elasticsoftware/akces/errors/AggregateAlreadyExistsErrorEvent.java
================
package org.elasticsoftware.akces.errors;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.ErrorEvent;

@DomainEventInfo(type = "AggregateAlreadyExistsError")
public record AggregateAlreadyExistsErrorEvent(
        @NotNull String aggregateId,
        @NotNull String aggregateName
) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return aggregateId();
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/errors/CommandExecutionErrorEvent.java
================
package org.elasticsoftware.akces.errors;

import jakarta.validation.constraints.NotNull;
import org.elasticsoftware.akces.annotations.DomainEventInfo;
import org.elasticsoftware.akces.events.ErrorEvent;

@DomainEventInfo(type = "CommandExecutionError")
public record CommandExecutionErrorEvent(
        @NotNull String aggregateId,
        @NotNull String aggregateName,
        @NotNull String commandName,
        String errorDescription
) implements ErrorEvent {
    @Override
    public String getAggregateId() {
        return aggregateId();
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/AkcesGDPRModule.java
================
package org.elasticsoftware.akces.gdpr.jackson;

import com.fasterxml.jackson.core.Version;
import com.fasterxml.jackson.databind.Module;
import org.semver4j.Semver;

import java.net.URL;
import java.util.jar.Attributes;
import java.util.jar.Manifest;

import static java.lang.String.format;

public class AkcesGDPRModule extends Module {
    private static Version extractVersion() {
        String className = format("/%s.class", AkcesGDPRModule.class.getName().replace('.', '/'));
        URL resource = AkcesGDPRModule.class.getResource(className);
        if (resource != null) {
            String classPath = resource.toString();
            if (!classPath.startsWith("jar")) {

                return Version.unknownVersion();
            }

            String manifestPath = classPath.substring(0, classPath.lastIndexOf("!") + 1) +
                    "/META-INF/MANIFEST.MF";
            try {
                Manifest manifest = new Manifest(new URL(manifestPath).openStream());
                Attributes attr = manifest.getMainAttributes();
                String value = attr.getValue("Implementation-Version");
                if (value != null) {
                    return generateVersion(Semver.parse(value));
                } else {
                    return Version.unknownVersion();
                }
            } catch (Exception e) {
                return Version.unknownVersion();
            }
        } else {
            return Version.unknownVersion();
        }
    }

    public static Version generateVersion(Semver semver) {
        if (semver != null) {
            return new Version(
                    semver.getMajor(),
                    semver.getMinor(),
                    semver.getPatch(),
                    !semver.getPreRelease().isEmpty() ? semver.getPreRelease().get(0) : null,
                    "org.elasticsoftwarefoundation.akces",
                    "akces-runtime");
        } else {
            return Version.unknownVersion();
        }
    }

    @Override
    public String getModuleName() {
        return "AkcesGDPR";
    }

    @Override
    public Version version() {
        return extractVersion();
    }

    @Override
    public void setupModule(SetupContext setupContext) {
        setupContext.addBeanSerializerModifier(new PIIDataSerializerModifier());
        setupContext.addBeanDeserializerModifier(new PIIDataDeserializerModifier());
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataDeserializerModifier.java
================
package org.elasticsoftware.akces.gdpr.jackson;

import com.fasterxml.jackson.databind.BeanDescription;
import com.fasterxml.jackson.databind.DeserializationConfig;
import com.fasterxml.jackson.databind.deser.BeanDeserializerBuilder;
import com.fasterxml.jackson.databind.deser.BeanDeserializerModifier;
import com.fasterxml.jackson.databind.deser.SettableBeanProperty;
import com.fasterxml.jackson.databind.deser.std.StdValueInstantiator;
import org.elasticsoftware.akces.annotations.PIIData;

import java.util.Iterator;

public class PIIDataDeserializerModifier extends BeanDeserializerModifier {
    private final PIIDataJsonDeserializer instance = new PIIDataJsonDeserializer();

    @Override
    public BeanDeserializerBuilder updateBuilder(DeserializationConfig config,
                                                 BeanDescription beanDescription,
                                                 BeanDeserializerBuilder builder) {
        Iterator<SettableBeanProperty> it = builder.getProperties();

        while (it.hasNext()) {
            SettableBeanProperty p = it.next();
            if (p.getAnnotation(PIIData.class) != null) {
                builder.addOrReplaceProperty(p.withValueDeserializer(instance), true);
            }
        }

        if (builder.getValueInstantiator() != null) {
            SettableBeanProperty[] constructorArguments = builder.getValueInstantiator().getFromObjectArguments(config);
            if (constructorArguments != null) {
                SettableBeanProperty[] updatedArguments = new SettableBeanProperty[constructorArguments.length];
                for (int i = 0; i < constructorArguments.length; i++) {
                    if (constructorArguments[i].getAnnotation(PIIData.class) != null) {
                        updatedArguments[i] = constructorArguments[i].withValueDeserializer(instance);
                    } else {
                        updatedArguments[i] = constructorArguments[i];
                    }
                }
                builder.setValueInstantiator(new PersonalDataValueInstantiator((StdValueInstantiator) builder.getValueInstantiator(), updatedArguments));
            }
        }


        return builder;
    }

    static class PersonalDataValueInstantiator extends StdValueInstantiator {

        PersonalDataValueInstantiator(StdValueInstantiator src, SettableBeanProperty[] arguments) {
            super(src);
            _constructorArguments = arguments;
        }
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataJsonDeserializer.java
================
package org.elasticsoftware.akces.gdpr.jackson;

import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.databind.DeserializationContext;
import com.fasterxml.jackson.databind.deser.std.StringDeserializer;
import org.elasticsoftware.akces.gdpr.GDPRContext;
import org.elasticsoftware.akces.gdpr.GDPRContextHolder;

import java.io.IOException;

public class PIIDataJsonDeserializer extends StringDeserializer {

    @Override
    public String deserialize(JsonParser p, DeserializationContext ctxt) throws IOException {
        String encryptedString = super.deserialize(p, ctxt);
        GDPRContext gdprContext = GDPRContextHolder.getCurrentGDPRContext();
        return gdprContext != null ? gdprContext.decrypt(encryptedString) : encryptedString;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataJsonSerializer.java
================
package org.elasticsoftware.akces.gdpr.jackson;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.JavaType;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.JsonSerializer;
import com.fasterxml.jackson.databind.SerializerProvider;
import com.fasterxml.jackson.databind.jsonFormatVisitors.JsonFormatVisitorWrapper;
import org.elasticsoftware.akces.gdpr.GDPRContext;
import org.elasticsoftware.akces.gdpr.GDPRContextHolder;

import java.io.IOException;

public class PIIDataJsonSerializer extends JsonSerializer<Object> {
    @Override
    public void serialize(Object value, JsonGenerator gen, SerializerProvider provider) throws IOException {
        GDPRContext gdprContext = GDPRContextHolder.getCurrentGDPRContext();
        if (gdprContext != null) {
            gen.writeString(gdprContext.encrypt((String) value));
        } else {
            gen.writeString((String) value);
        }
    }

    @Override
    public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint) throws JsonMappingException {
        visitor.expectStringFormat(typeHint);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/jackson/PIIDataSerializerModifier.java
================
package org.elasticsoftware.akces.gdpr.jackson;

import com.fasterxml.jackson.databind.BeanDescription;
import com.fasterxml.jackson.databind.JsonSerializer;
import com.fasterxml.jackson.databind.SerializationConfig;
import com.fasterxml.jackson.databind.ser.BeanPropertyWriter;
import com.fasterxml.jackson.databind.ser.BeanSerializerModifier;
import org.elasticsoftware.akces.annotations.PIIData;

import java.util.ArrayList;
import java.util.List;

public class PIIDataSerializerModifier extends BeanSerializerModifier {
    private final PIIDataJsonSerializer instance = new PIIDataJsonSerializer();

    @Override
    public List<BeanPropertyWriter> changeProperties(final SerializationConfig config,
                                                     final BeanDescription beanDescription,
                                                     final List<BeanPropertyWriter> beanProperties) {
        List<BeanPropertyWriter> newWriters = new ArrayList<>();

        for (final BeanPropertyWriter writer : beanProperties) {
            if (null == writer.getAnnotation(PIIData.class)) {
                newWriters.add(writer);
            } else {
                newWriters.add(new PersonalDataPropertyWriter(writer, instance));
            }
        }
        return newWriters;
    }

    static class PersonalDataPropertyWriter extends BeanPropertyWriter {
        PersonalDataPropertyWriter(final BeanPropertyWriter base, final JsonSerializer<Object> serializer) {
            super(base);
            this._serializer = serializer;
        }
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/EncryptingGDPRContext.java
================
package org.elasticsoftware.akces.gdpr;

import jakarta.annotation.Nonnull;
import jakarta.annotation.Nullable;
import org.apache.kafka.common.errors.SerializationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.crypto.BadPaddingException;
import javax.crypto.Cipher;
import javax.crypto.IllegalBlockSizeException;
import javax.crypto.NoSuchPaddingException;
import javax.crypto.spec.IvParameterSpec;
import javax.crypto.spec.SecretKeySpec;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.security.InvalidAlgorithmParameterException;
import java.security.InvalidKeyException;
import java.security.NoSuchAlgorithmException;
import java.util.Base64;
import java.util.HexFormat;
import java.util.UUID;

public final class EncryptingGDPRContext implements GDPRContext {
    private static final Logger logger = LoggerFactory.getLogger(EncryptingGDPRContext.class);
    private final String aggregateId;
    private final Cipher encryptingCipher;
    private final Cipher decryptingCipher;
    private final byte[] encryptionKey;

    public EncryptingGDPRContext(@Nonnull String aggregateId, @Nonnull byte[] encryptionKey, boolean aggregateIsUUID) {
        if (encryptionKey.length != 32) {
            throw new IllegalArgumentException("Key size needs to be 32 bytes");
        }
        this.aggregateId = aggregateId;
        this.encryptionKey = encryptionKey;
        SecretKeySpec keySpec = new SecretKeySpec(encryptionKey, "AES");
        IvParameterSpec ivParameterSpec = null;
        String aesMode = "ECB";

        if (aggregateIsUUID) {
            UUID aggregateUUID = UUID.fromString(aggregateId);
            ivParameterSpec = new IvParameterSpec(ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array());
            aesMode = "CBC";
        }
        try {
            encryptingCipher = Cipher.getInstance("AES/" + aesMode + "/PKCS5PADDING");
            decryptingCipher = Cipher.getInstance("AES/" + aesMode + "/PKCS5PADDING");
            encryptingCipher.init(Cipher.ENCRYPT_MODE, keySpec, ivParameterSpec, GDPRKeyUtils.secureRandom());
            decryptingCipher.init(Cipher.DECRYPT_MODE, keySpec, ivParameterSpec, GDPRKeyUtils.secureRandom());
        } catch (NoSuchPaddingException | NoSuchAlgorithmException | InvalidAlgorithmParameterException e) {
            throw new SerializationException(e);
        } catch (InvalidKeyException e) {
            throw new IllegalArgumentException(e);
        }
    }

    @Override
    @Nullable
    public String encrypt(@Nullable String data) {
        if (data == null) {
            return null;
        }
        try {
            logger.trace("Encrypting data for aggregateId '{}' with algorithm {} and encryptionKey (hash) {}",
                    aggregateId,
                    encryptingCipher.getAlgorithm(),
                    HexFormat.of().formatHex(encryptionKey));
            return Base64.getUrlEncoder().encodeToString(encryptingCipher.doFinal(data.getBytes(StandardCharsets.UTF_8)));
        } catch (IllegalBlockSizeException | BadPaddingException e) {
            throw new SerializationException(e);
        }
    }

    @Override
    @Nullable
    public String decrypt(@Nullable String encryptedData) {
        if (encryptedData == null) {
            return null;
        }
        try {
            if (encryptedData.length() % 4 == 0) {
                byte[] encryptedBytes = Base64.getUrlDecoder().decode(encryptedData);

                if (encryptedBytes.length % 16 == 0) {
                    logger.trace("Decrypting data for aggregateId '{}' with algorithm {} and encryptionKey (hash) {}",
                            aggregateId,
                            decryptingCipher.getAlgorithm(),
                            HexFormat.of().formatHex(encryptionKey));
                    return new String(decryptingCipher.doFinal(encryptedBytes), StandardCharsets.UTF_8);
                }
            }
        } catch (IllegalArgumentException e) {

        } catch (BadPaddingException | IllegalBlockSizeException e) {
            throw new SerializationException(e);
        }

        return encryptedData;
    }

    @Override
    @Nonnull
    public String getAggregateId() {
        return aggregateId;
    }

    @Nullable
    @Override
    public byte[] getEncryptionKey() {
        return encryptionKey;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRAnnotationUtils.java
================
package org.elasticsoftware.akces.gdpr;

import org.elasticsoftware.akces.annotations.PIIData;
import org.springframework.beans.BeanUtils;

import java.lang.annotation.Annotation;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;
import java.util.stream.Stream;

import static java.lang.Boolean.FALSE;
import static java.lang.Boolean.TRUE;
import static java.util.Arrays.stream;

public final class GDPRAnnotationUtils {
    private GDPRAnnotationUtils() {
    }

    public static Boolean hasPIIDataAnnotation(Class<?> type) {
        return hasAnnotation(type, PIIData.class);
    }

    public static Boolean hasAnnotation(Class<?> type, final Class<? extends Annotation> annotation) {
        return hasAnnotationInternal(type, annotation, new HashSet<>());
    }

    private static Boolean hasAnnotationInternal(Class<?> type, final Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
        visitedClasses.add(type);
        if (methodsHaveAnnotation(type, annotation, visitedClasses)) {
            return TRUE;
        }
        if (constructorParametersHaveAnnotation(type, annotation, visitedClasses)) {
            return TRUE;
        }
        if (fieldsHaveAnnotation(type, annotation, visitedClasses)) {
            return TRUE;
        }

        return FALSE;

    }

    private static boolean methodsHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
        Class<?> klass = type;
        while (klass != Object.class && klass != null) {
            if (Stream.of(klass.getDeclaredMethods()).anyMatch(method -> method.isAnnotationPresent(annotation) ||
                (!BeanUtils.isSimpleValueType(method.getReturnType()) &&
                !visitedClasses.contains(method.getReturnType()) &&
                hasAnnotationInternal(method.getReturnType(), annotation, visitedClasses)))) {
                return true;
            }
            if (klass.isArray()) {
                klass = klass.getComponentType();
            } else {
                klass = klass.getSuperclass();
            }
        }
        return false;
    }

    private static boolean constructorParametersHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
        return stream(type.getConstructors())
                .flatMap(constructor -> stream(constructor.getParameters()))
            .anyMatch(parameter -> parameter.isAnnotationPresent(annotation) ||
                (!BeanUtils.isSimpleValueType(parameter.getType()) &&
                !visitedClasses.contains(parameter.getType()) &&
                hasAnnotationInternal(parameter.getType(), annotation, visitedClasses)));
    }

    private static boolean fieldsHaveAnnotation(Class<?> type, Class<? extends Annotation> annotation, Set<Class<?>> visitedClasses) {
        return Arrays.stream(type.getDeclaredFields())
            .anyMatch(field -> field.isAnnotationPresent(annotation) ||
                (!BeanUtils.isSimpleValueType(field.getType()) &&
                !visitedClasses.contains(field.getType()) &&
                hasAnnotationInternal(field.getType(), annotation, visitedClasses)));
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContext.java
================
package org.elasticsoftware.akces.gdpr;


import jakarta.annotation.Nonnull;
import jakarta.annotation.Nullable;

public sealed interface GDPRContext permits NoopGDPRContext, EncryptingGDPRContext {
    @Nullable
    String encrypt(@Nullable String data);

    @Nullable
    String decrypt(@Nullable String encryptedData);

    @Nonnull
    String getAggregateId();

    @Nullable
    default byte[] getEncryptionKey() {
        return null;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextHolder.java
================
package org.elasticsoftware.akces.gdpr;

import jakarta.annotation.Nullable;

public final class GDPRContextHolder {

    private static final ThreadLocal<GDPRContext> currentContext = new ThreadLocal<>();

    private GDPRContextHolder() {
    }

    public static GDPRContext getCurrentGDPRContext() {
        return currentContext.get();
    }

    public static GDPRContext setCurrentGDPRContext(@Nullable GDPRContext gdprContext) {
        final GDPRContext ctx = currentContext.get();
        currentContext.set(gdprContext);
        return ctx;
    }

    public static GDPRContext resetCurrentGDPRContext() {
        final GDPRContext ctx = currentContext.get();
        currentContext.remove();
        return ctx;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepository.java
================
package org.elasticsoftware.akces.gdpr;

import jakarta.annotation.Nonnull;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.requests.ProduceResponse;
import org.elasticsoftware.akces.protocol.GDPRKeyRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;

import java.io.Closeable;
import java.util.List;
import java.util.concurrent.Future;

public interface GDPRContextRepository extends Closeable {
    default long getOffset() {
        return ProduceResponse.INVALID_OFFSET;
    }

    void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture);

    void commit();

    void rollback();

    void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords);

    boolean exists(String aggregateId);

    @Nonnull
    GDPRContext get(String aggregateId);
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepositoryException.java
================
package org.elasticsoftware.akces.gdpr;

public class GDPRContextRepositoryException extends RuntimeException {

    public GDPRContextRepositoryException(String message, Throwable cause) {
        super(message, cause);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRContextRepositoryFactory.java
================
package org.elasticsoftware.akces.gdpr;


public interface GDPRContextRepositoryFactory {
    GDPRContextRepository create(String runtimeName, Integer partitionId);
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/GDPRKeyUtils.java
================
package org.elasticsoftware.akces.gdpr;

import javax.crypto.spec.SecretKeySpec;
import java.security.NoSuchAlgorithmException;
import java.security.SecureRandom;
import java.util.regex.Pattern;

public class GDPRKeyUtils {
    private static final Pattern UUID_REGEX =
            Pattern.compile("^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$");
    private static SecureRandom secureRandom;

    static {
        try {
            secureRandom = SecureRandom.getInstance("NativePRNGNonBlocking");
        } catch (NoSuchAlgorithmException e) {
            secureRandom = new SecureRandom();
        }
    }

    private GDPRKeyUtils() {

    }

    public static SecretKeySpec createKey() {
        byte[] keyBytes = new byte[32];
        secureRandom.nextBytes(keyBytes);
        return new SecretKeySpec(keyBytes, "AES");
    }

    public static SecureRandom secureRandom() {
        return secureRandom;
    }

    public static boolean isUUID(String possibleUUID) {
        return UUID_REGEX.matcher(possibleUUID).matches();
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/InMemoryGDPRContextRepository.java
================
package org.elasticsoftware.akces.gdpr;

import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.LoadingCache;
import jakarta.annotation.Nonnull;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.requests.ProduceResponse;
import org.elasticsoftware.akces.protocol.GDPRKeyRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.elasticsoftware.akces.kafka.RecordAndMetadata;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.concurrent.Future;

public class InMemoryGDPRContextRepository implements GDPRContextRepository {
    private static final Logger log = LoggerFactory.getLogger(InMemoryGDPRContextRepository.class);
    private final Map<String, GDPRKeyRecord> stateRecordMap = new HashMap<>();
    private final Map<String, RecordAndMetadata<GDPRKeyRecord>> transactionStateRecordMap = new HashMap<>();
    private boolean aggregateIdIsUUID = false;
    private boolean aggregateIdTypeCheckDone = false;
    private final LoadingCache<String, GDPRContext> gdprContexts = Caffeine.newBuilder()
            .maximumSize(1000)
            .build(this::createGDPRContext);
    private long offset = -1L;

    @Override
    public void close() {
        stateRecordMap.clear();
        transactionStateRecordMap.clear();
        gdprContexts.invalidateAll();
        gdprContexts.cleanUp();
    }

    @Override
    public void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture) {
        transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));

        gdprContexts.invalidate(record.aggregateId());
    }

    @Override
    public void commit() {

        if (!transactionStateRecordMap.isEmpty()) {

            this.offset = transactionStateRecordMap.values().stream()
                    .map(RecordAndMetadata::metadata)
                    .map(recordMetadataFuture -> {
                        try {
                            return recordMetadataFuture.get();
                        } catch (Exception e) {
                            log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
                            return null;
                        }
                    })
                    .map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
                    .max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
            log.trace("Committing {} records and offset {}", transactionStateRecordMap.size(), this.offset);
            transactionStateRecordMap.values().forEach(recordAndMetadata -> stateRecordMap.put(recordAndMetadata.record().aggregateId(), (GDPRKeyRecord) recordAndMetadata.record()));
            transactionStateRecordMap.clear();
        }
    }

    @Override
    public void rollback() {

        gdprContexts.invalidateAll(transactionStateRecordMap.keySet());
        transactionStateRecordMap.clear();
    }

    @Override
    public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {
        for (ConsumerRecord<String, ProtocolRecord> consumerRecord : consumerRecords) {
            GDPRKeyRecord record = (GDPRKeyRecord) consumerRecord.value();
            if (record != null) {
                stateRecordMap.put(record.aggregateId(), record);
            } else {
                stateRecordMap.remove(consumerRecord.key());
            }

            gdprContexts.invalidate(consumerRecord.key());
            this.offset = consumerRecord.offset();
        }
    }

    @Override
    public boolean exists(String aggregateId) {
        return getGDPRKeyRecord(aggregateId) != null;
    }

    @Override
    @Nonnull
    public GDPRContext get(String aggregateId) {
        return gdprContexts.get(aggregateId);
    }

    private GDPRContext createGDPRContext(String aggregateId) {
        GDPRKeyRecord record = getGDPRKeyRecord(aggregateId);
        if (record != null) {
            checkAggregateIdType(aggregateId);
            return new EncryptingGDPRContext(record.aggregateId(), record.payload(), aggregateIdIsUUID);
        } else {
            return new NoopGDPRContext(aggregateId);
        }
    }

    private GDPRKeyRecord getGDPRKeyRecord(String aggregateId) {

        if (transactionStateRecordMap.containsKey(aggregateId)) {
            return transactionStateRecordMap.get(aggregateId).record();
        } else {
            return stateRecordMap.get(aggregateId);
        }
    }

    @Override
    public long getOffset() {
        return offset;
    }

    private void checkAggregateIdType(String aggregateId) {
        if (!aggregateIdTypeCheckDone) {
            try {
                UUID.fromString(aggregateId);
                aggregateIdIsUUID = true;
                log.trace("AggregateId '{}' is a UUID", aggregateId);
            } catch (IllegalArgumentException e) {
                aggregateIdIsUUID = false;
                log.trace("AggregateId '{}' is not a UUID", aggregateId);
            }
            aggregateIdTypeCheckDone = true;
        }
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/InMemoryGDPRContextRepositoryFactory.java
================
package org.elasticsoftware.akces.gdpr;

public class InMemoryGDPRContextRepositoryFactory implements GDPRContextRepositoryFactory {
    @Override
    public GDPRContextRepository create(String runtimeName, Integer partitionId) {
        return new InMemoryGDPRContextRepository();
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/NoopGDPRContext.java
================
package org.elasticsoftware.akces.gdpr;

import jakarta.annotation.Nonnull;
import jakarta.annotation.Nullable;

public final class NoopGDPRContext implements GDPRContext {
    private final String aggregateId;

    public NoopGDPRContext(@Nonnull String aggregateId) {
        this.aggregateId = aggregateId;
    }

    @Nullable
    @Override
    public String encrypt(@Nullable String data) {
        return data;
    }

    @Nullable
    @Override
    public String decrypt(@Nullable String encryptedData) {
        return encryptedData;
    }

    @Nonnull
    @Override
    public String getAggregateId() {
        return aggregateId;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepository.java
================
package org.elasticsoftware.akces.gdpr;

import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.LoadingCache;
import com.google.common.primitives.Longs;
import jakarta.annotation.Nonnull;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.errors.SerializationException;
import org.apache.kafka.common.requests.ProduceResponse;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serializer;
import org.elasticsoftware.akces.kafka.RecordAndMetadata;
import org.elasticsoftware.akces.protocol.GDPRKeyRecord;
import org.elasticsoftware.akces.protocol.ProtocolRecord;
import org.rocksdb.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.UUID;
import java.util.concurrent.Future;

public class RocksDBGDPRContextRepository implements GDPRContextRepository {
    private static final Logger log = LoggerFactory.getLogger(RocksDBGDPRContextRepository.class);

    private static final byte[] OFFSET = new byte[]{0x4f, 0x46, 0x46, 0x53, 0x45, 0x54};
    private final TransactionDB db;
    private final File rocksDBDataDir;
    private final Map<String, RecordAndMetadata<GDPRKeyRecord>> transactionStateRecordMap = new HashMap<>();
    private final String topicName;
    private final Serializer<ProtocolRecord> serializer;
    private final Deserializer<ProtocolRecord> deserializer;
    private boolean aggregateIdIsUUID = false;
    private boolean aggregateIdTypeCheckDone = false;
    private final LoadingCache<String, GDPRContext> gdprContexts = Caffeine.newBuilder()
            .maximumSize(1000)
            .build(this::createGDPRContext);
    private long lastOffset = ProduceResponse.INVALID_OFFSET;

    public RocksDBGDPRContextRepository(String baseDir,
                                        String partitionId,
                                        String topicName,
                                        Serializer<ProtocolRecord> serializer,
                                        Deserializer<ProtocolRecord> deserializer) {
        this.topicName = topicName;
        this.serializer = serializer;
        this.deserializer = deserializer;
        RocksDB.loadLibrary();
        final Options options = new Options();
        final TransactionDBOptions transactionDBOptions = new TransactionDBOptions();
        options.setCreateIfMissing(true);


        options.setAllowFAllocate(false);
        this.rocksDBDataDir = new File(baseDir, partitionId);
        try {
            Files.createDirectories(this.rocksDBDataDir.getParentFile().toPath());
            Files.createDirectories(this.rocksDBDataDir.getAbsoluteFile().toPath());
            db = TransactionDB.open(options, transactionDBOptions, this.rocksDBDataDir.getAbsolutePath());


            initializeOffset();
            log.info("RocksDBGDPRContextRepository for partition {} initialized in folder {}", partitionId, this.rocksDBDataDir.getAbsolutePath());
        } catch (IOException | RocksDBException e) {
            throw new GDPRContextRepositoryException("Error initializing RocksDB", e);
        }
    }

    @Override
    public void close() {
        try {
            db.syncWal();
        } catch (RocksDBException e) {
            log.error("Error syncing WAL. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
        }
        db.close();
    }

    private void initializeOffset() {
        try {
            byte[] offsetBytes = db.get(OFFSET);
            if (offsetBytes != null) {
                lastOffset = Longs.fromByteArray(offsetBytes);
            }
        } catch (RocksDBException e) {
            throw new GDPRContextRepositoryException("Error initializing offset", e);
        }
    }

    private void updateOffset(long offset) {
        this.lastOffset = offset;
        log.trace("Updated offset to {}", offset);
    }

    @Override
    public long getOffset() {
        return lastOffset;
    }

    @Override
    public void prepare(GDPRKeyRecord record, Future<RecordMetadata> recordMetadataFuture) {
        checkAggregateIdType(record.aggregateId());
        transactionStateRecordMap.put(record.aggregateId(), new RecordAndMetadata<>(record, recordMetadataFuture));
    }

    @Override
    public void commit() {
        if (!transactionStateRecordMap.isEmpty()) {

            Transaction transaction = db.beginTransaction(new WriteOptions());
            try {
                for (RecordAndMetadata<?> recordAndMetadata : transactionStateRecordMap.values()) {
                    transaction.put(keyBytes(recordAndMetadata.record().aggregateId()), serializer.serialize(topicName, recordAndMetadata.record()));
                }

                long offset = transactionStateRecordMap.values().stream()
                        .map(RecordAndMetadata::metadata)
                        .map(recordMetadataFuture -> {
                            try {
                                return recordMetadataFuture.get();
                            } catch (Exception e) {
                                log.error("Error getting offset. Exception: '{}', message: '{}'", e.getCause(), e.getMessage(), e);
                                return null;
                            }
                        })
                        .map(recordMetadata -> recordMetadata != null ? recordMetadata.offset() : ProduceResponse.INVALID_OFFSET)
                        .max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
                transaction.put(OFFSET, Longs.toByteArray(offset));
                transaction.commit();
                transaction.close();
                updateOffset(offset);
            } catch (RocksDBException e) {
                throw new GDPRContextRepositoryException("Error committing records", e);
            } finally {
                transactionStateRecordMap.clear();
            }
        }
    }

    @Override
    public void rollback() {
        transactionStateRecordMap.clear();
    }

    @Override
    public void process(List<ConsumerRecord<String, ProtocolRecord>> consumerRecords) {

        long offset = consumerRecords.stream()
                .map(ConsumerRecord::offset)
                .max(Long::compareTo).orElse(ProduceResponse.INVALID_OFFSET);
        if (offset > lastOffset) {
            Transaction transaction = db.beginTransaction(new WriteOptions());
            try {
                for (ConsumerRecord<String, ProtocolRecord> consumerRecord : consumerRecords) {
                    if(consumerRecord.value() != null) {

                        transaction.put(keyBytes(consumerRecord.key()), serializer.serialize(topicName, consumerRecord.value()));
                        log.trace("{} Wrote record with key {}", rocksDBDataDir.getAbsolutePath(), consumerRecord.key());
                    } else {

                        transaction.delete(keyBytes(consumerRecord.key()));
                    }
                }
                transaction.put(OFFSET, Longs.toByteArray(offset));
                transaction.commit();
                transaction.close();

                consumerRecords.forEach(consumerRecord -> gdprContexts.invalidate(consumerRecord.key()));
                updateOffset(offset);
            } catch (RocksDBException e) {
                throw new GDPRContextRepositoryException("Error processing records", e);
            }
        }
    }

    @Override
    public boolean exists(String aggregateId) {
        return transactionStateRecordMap.containsKey(aggregateId) || db.keyExists(keyBytes(aggregateId));
    }

    @Override
    @Nonnull
    public GDPRContext get(String aggregateId) {
        return gdprContexts.get(aggregateId);
    }

    private GDPRContext createGDPRContext(String aggregateId) {
        GDPRKeyRecord record = getGDPRKeyRecord(aggregateId);
        if (record != null) {
            checkAggregateIdType(aggregateId);
            return new EncryptingGDPRContext(record.aggregateId(), record.payload(), aggregateIdIsUUID);
        } else {
            return new NoopGDPRContext(aggregateId);
        }
    }

    private GDPRKeyRecord getGDPRKeyRecord(String aggregateId) {
        log.trace("{} Getting record for aggregateId {}",rocksDBDataDir.getAbsolutePath(), aggregateId);
        checkAggregateIdType(aggregateId);

        if (transactionStateRecordMap.containsKey(aggregateId)) {
            return transactionStateRecordMap.get(aggregateId).record();
        } else {
            byte[] keyBytes = keyBytes(aggregateId);
            if (db.keyExists(keyBytes)) {
                try {
                    return (GDPRKeyRecord) deserializer.deserialize(topicName, db.get(keyBytes));
                } catch (RocksDBException | SerializationException e) {
                    throw new GDPRContextRepositoryException("Problem reading record with aggregateId " + aggregateId, e);
                }
            } else {
                return null;
            }
        }
    }

    private byte[] keyBytes(String aggregateId) {
        checkAggregateIdType(aggregateId);
        if (aggregateIdIsUUID) {
            UUID aggregateUUID = UUID.fromString(aggregateId);
            return ByteBuffer.wrap(new byte[16]).putLong(aggregateUUID.getMostSignificantBits()).putLong(aggregateUUID.getLeastSignificantBits()).array();
        } else
            return aggregateId.getBytes(StandardCharsets.UTF_8);
    }

    private void checkAggregateIdType(String aggregateId) {
        if (!aggregateIdTypeCheckDone) {
            try {
                UUID.fromString(aggregateId);
                aggregateIdIsUUID = true;
            } catch (IllegalArgumentException e) {
                aggregateIdIsUUID = false;
            }
            aggregateIdTypeCheckDone = true;
        }
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepositoryFactory.java
================
package org.elasticsoftware.akces.gdpr;

import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;

public class RocksDBGDPRContextRepositoryFactory implements GDPRContextRepositoryFactory {
    private final ProtocolRecordSerde serde;
    private final String baseDir;

    public RocksDBGDPRContextRepositoryFactory(ProtocolRecordSerde serde, String baseDir) {
        this.serde = serde;
        this.baseDir = baseDir;
    }

    @Override
    public GDPRContextRepository create(String runtimeName, Integer partitionId) {
        return new RocksDBGDPRContextRepository(
                baseDir,
                runtimeName + "-Akces-GDPRKeys-" + partitionId.toString(),
                "Akces-GDPRKeys",
                serde.serializer(),
                serde.deserializer());
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/kafka/CustomKafkaConsumerFactory.java
================
package org.elasticsoftware.akces.kafka;

import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.Deserializer;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;

import java.util.Map;

public class CustomKafkaConsumerFactory<K, V> extends DefaultKafkaConsumerFactory<K, V> {
    public CustomKafkaConsumerFactory(Map<String, Object> configs, Deserializer<K> keyDeserializer, Deserializer<V> valueDeserializer) {
        super(configs, keyDeserializer, valueDeserializer);
    }

    @Override
    protected Consumer<K, V> createKafkaConsumer(Map<String, Object> configProps) {

        configProps.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, configProps.get(ConsumerConfig.CLIENT_ID_CONFIG));
        return super.createKafkaConsumer(configProps);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/kafka/CustomKafkaProducerFactory.java
================
package org.elasticsoftware.akces.kafka;

import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.common.serialization.Serializer;
import org.springframework.kafka.KafkaException;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.ProducerFactory;

import java.util.Map;

public class CustomKafkaProducerFactory<K, V> extends DefaultKafkaProducerFactory<K, V> {

    public CustomKafkaProducerFactory(Map<String, Object> configs, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
        super(configs, keySerializer, valueSerializer);
    }

    @Override
    protected Producer<K, V> createTransactionalProducer(String transactionId) {


        Producer<K, V> newProducer = createRawProducer(getTxProducerConfigs(transactionId));
        try {
            newProducer.initTransactions();
        } catch (RuntimeException ex) {
            try {
                newProducer.close(ProducerFactory.DEFAULT_PHYSICAL_CLOSE_TIMEOUT);
            } catch (RuntimeException ex2) {
                KafkaException newEx = new KafkaException("initTransactions() failed and then close() failed", ex);
                newEx.addSuppressed(ex2);
                throw newEx;
            }
            throw new KafkaException("initTransactions() failed", ex);
        }
        return newProducer;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/kafka/RecordAndMetadata.java
================
package org.elasticsoftware.akces.kafka;

import org.apache.kafka.clients.producer.RecordMetadata;
import org.elasticsoftware.akces.protocol.ProtocolRecord;

import java.util.concurrent.Future;

public record RecordAndMetadata<PR extends ProtocolRecord>(PR record, Future<RecordMetadata> metadata) {
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/AggregateStateRecord.java
================
package org.elasticsoftware.akces.protocol;

public record AggregateStateRecord(
        String tenantId,
        String name,
        int version,
        byte[] payload,
        PayloadEncoding encoding,
        String aggregateId,
        String correlationId,
        long generation
) implements ProtocolRecord {
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/CommandRecord.java
================
package org.elasticsoftware.akces.protocol;

import java.util.UUID;

public record CommandRecord(
        String id,
        String tenantId,
        String name,
        int version,
        byte[] payload,
        PayloadEncoding encoding,
        String aggregateId,
        String correlationId,
        String replyToTopicPartition
) implements ProtocolRecord {
    public CommandRecord(String tenantId,
                         String name,
                         int version,
                         byte[] payload,
                         PayloadEncoding encoding,
                         String aggregateId,
                         String correlationId,
                         String replyToTopicPartition) {
        this(UUID.randomUUID().toString(),
                tenantId,
                name,
                version,
                payload,
                encoding,
                aggregateId,
                correlationId,
                replyToTopicPartition);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/CommandResponseRecord.java
================
package org.elasticsoftware.akces.protocol;

import java.util.List;

public record CommandResponseRecord(
        String tenantId,
        String name,
        int version,
        byte[] payload,
        PayloadEncoding encoding,
        String aggregateId,
        String correlationId,
        String commandId,
        List<DomainEventRecord> events,
        byte[] encryptionKey
) implements ProtocolRecord {
    public CommandResponseRecord(String tenantId, String aggregateId, String correlationId, String commandId, List<DomainEventRecord> events, byte[] encryptionKey) {
        this(tenantId, "CommandResponse", 1, null, PayloadEncoding.BYTES, aggregateId, correlationId, commandId, events, encryptionKey);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/DomainEventRecord.java
================
package org.elasticsoftware.akces.protocol;

import java.util.UUID;

public record DomainEventRecord(
        String id,
        String tenantId,
        String name,
        int version,
        byte[] payload,
        PayloadEncoding encoding,
        String aggregateId,
        String correlationId,
        long generation
) implements ProtocolRecord {
    public DomainEventRecord(String tenantId, String name, int version, byte[] payload, PayloadEncoding encoding, String aggregateId, String correlationId, long generation) {
        this(UUID.randomUUID().toString(), tenantId, name, version, payload, encoding, aggregateId, correlationId, generation);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/GDPRKeyRecord.java
================
package org.elasticsoftware.akces.protocol;

public record GDPRKeyRecord(
        String tenantId,
        String name,
        int version,
        byte[] payload,
        PayloadEncoding encoding,
        String aggregateId,
        String correlationId
) implements ProtocolRecord {
    public GDPRKeyRecord(String tenantId, String aggregateId, byte[] payload) {
        this(tenantId, "GDPRKey", 1, payload, PayloadEncoding.BYTES, aggregateId, null);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/PayloadEncoding.java
================
package org.elasticsoftware.akces.protocol;

public enum PayloadEncoding {
    JSON,
    PROTOBUF,
    BYTES
}

================
File: shared/src/main/java/org/elasticsoftware/akces/protocol/ProtocolRecord.java
================
package org.elasticsoftware.akces.protocol;

public sealed interface ProtocolRecord permits AggregateStateRecord, CommandRecord, DomainEventRecord, GDPRKeyRecord, CommandResponseRecord {
    String tenantId();

    String name();

    int version();

    byte[] payload();

    PayloadEncoding encoding();

    String aggregateId();

    String correlationId();
}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/IncompatibleSchemaException.java
================
package org.elasticsoftware.akces.schemas;

import io.confluent.kafka.schemaregistry.json.diff.Difference;

import java.util.List;

public class IncompatibleSchemaException extends SchemaException {
    private final int schemaVersion;
    private final List<Difference> differences;

    public IncompatibleSchemaException(String schemaIdentifier,
                                       int schemaVersion,
                                       Class<?> implementationClass,
                                       List<Difference> differences) {
        super("Registered Schema incompatible with local implementation", schemaIdentifier, implementationClass);
        this.schemaVersion = schemaVersion;
        this.differences = differences;
    }

    public int getSchemaVersion() {
        return schemaVersion;
    }

    public List<Difference> getDifferences() {
        return differences;
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/InvalidSchemaVersionException.java
================
package org.elasticsoftware.akces.schemas;

public class InvalidSchemaVersionException extends SchemaException {
    private final int schemaVersion;

    public InvalidSchemaVersionException(String schemaIdentifier,
                                         int highestRegisteredSchemaVersion,
                                         int schemaVersion,
                                         Class<?> implementationClass) {
        super("Invalid Schema Version, highest registered schema version is " + highestRegisteredSchemaVersion,
                schemaIdentifier,
                implementationClass);
        this.schemaVersion = schemaVersion;
    }

    public int getSchemaVersion() {
        return schemaVersion;
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/KafkaSchemaRegistry.java
================
package org.elasticsoftware.akces.schemas;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ArrayNode;
import com.github.victools.jsonschema.generator.*;
import com.github.victools.jsonschema.module.jackson.JacksonModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationModule;
import com.github.victools.jsonschema.module.jakarta.validation.JakartaValidationOption;
import io.confluent.kafka.schemaregistry.CompatibilityLevel;
import io.confluent.kafka.schemaregistry.ParsedSchema;
import io.confluent.kafka.schemaregistry.SimpleParsedSchemaHolder;
import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
import io.confluent.kafka.schemaregistry.json.JsonSchema;
import io.confluent.kafka.schemaregistry.json.diff.Difference;
import io.confluent.kafka.schemaregistry.json.diff.SchemaDiff;
import org.elasticsoftware.akces.aggregate.CommandType;
import org.elasticsoftware.akces.aggregate.DomainEventType;
import org.elasticsoftware.akces.aggregate.SchemaType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.Comparator;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.stream.Collectors;

public class KafkaSchemaRegistry {
    private static final Logger logger = LoggerFactory.getLogger(KafkaSchemaRegistry.class);
    private final SchemaRegistryClient schemaRegistryClient;

    private final ThreadLocal<SchemaGenerator> schemaGeneratorTheadLocal;

    public KafkaSchemaRegistry(SchemaRegistryClient schemaRegistryClient, ObjectMapper objectMapper) {
        this.schemaRegistryClient = schemaRegistryClient;
        this.schemaGeneratorTheadLocal = ThreadLocal.withInitial(() -> createJsonSchemaGenerator(objectMapper));
    }

    public JsonSchema validate(CommandType<?> commandType) throws SchemaException {
        return validate(commandType, true);
    }

    public JsonSchema validate(DomainEventType<?> domainEventType) throws SchemaException {
        return validate(domainEventType, false);
    }

    private JsonSchema validate(SchemaType schemaType, boolean strict) throws SchemaException {
        try {
            logger.info("Validating schema {} v{}", schemaType.getSchemaName(), schemaType.version());
            JsonSchema localSchema = generateJsonSchema(schemaType);

            List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas(schemaType.getSchemaName(), false, false);
            if (!registeredSchemas.isEmpty()) {
                logger.trace("Found {} schemas for type {}", registeredSchemas.size(), schemaType.typeName());

                ParsedSchema registeredSchema = registeredSchemas.stream()
                        .filter(parsedSchema -> getSchemaVersion(schemaType, parsedSchema) == schemaType.version())
                        .findFirst().orElse(null);
                if (registeredSchema != null) {
                    logger.trace("Found schema for type {} v{}", schemaType.typeName(), schemaType.version());



                    List<Difference> differences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());
                    if (!differences.isEmpty()) {
                        if (!strict) {


                            List<Difference> violatingDifferences = differences.stream()
                                    .filter(difference -> !difference.getType().equals(Difference.Type.PROPERTY_REMOVED_FROM_CLOSED_CONTENT_MODEL))
                                    .toList();
                            if (!violatingDifferences.isEmpty()) {

                                throw new IncompatibleSchemaException(
                                        schemaType.getSchemaName(),
                                        schemaType.version(),
                                        schemaType.typeClass(),
                                        violatingDifferences);
                            }
                        } else {
                            throw new IncompatibleSchemaException(
                                    schemaType.getSchemaName(),
                                    schemaType.version(),
                                    schemaType.typeClass(),
                                    differences);
                        }
                    }
                    return localSchema;
                } else {

                    throw new SchemaVersionNotFoundException(
                            schemaType.getSchemaName(),
                            schemaType.version(),
                            schemaType.typeClass());
                }
            } else {

                throw new SchemaNotFoundException(
                        schemaType.getSchemaName(),
                        schemaType.typeClass());
            }
        } catch (IOException | RestClientException e) {
            throw new SchemaException(
                    "Unexpected Error while validating schema",
                    schemaType.getSchemaName(),
                    schemaType.typeClass(),
                    e);
        }
    }

    public JsonSchema registerAndValidate(SchemaType schemaType, boolean forceRegisterOnIncompatibleSchema) throws SchemaException {
        try {

            JsonSchema localSchema = generateJsonSchema(schemaType);

            String schemaName = schemaType.getSchemaName();
            List<ParsedSchema> registeredSchemas = schemaRegistryClient.getSchemas(
                    schemaName,
                    false,
                    false);
            if (registeredSchemas.isEmpty()) {
                if (!schemaType.external()) {
                    if (schemaType.version() == 1) {

                        schemaRegistryClient.register(
                                schemaName,
                                localSchema,
                                schemaType.version(),
                                -1);
                    } else {

                        throw new PreviousSchemaVersionMissingException(
                                schemaName,
                                schemaType.version(),
                                schemaType.typeClass());
                    }
                } else {

                    throw new SchemaNotFoundException(
                            schemaName,
                            schemaType.typeClass());
                }
            } else {

                ParsedSchema registeredSchema = registeredSchemas.stream()
                        .filter(parsedSchema -> getSchemaVersion(schemaType, parsedSchema) == schemaType.version())
                        .findFirst().orElse(null);
                if (registeredSchema != null) {

                    if (schemaType.external() && schemaType.relaxExternalValidation()) {



                        List<Difference> differences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());
                        if (!differences.isEmpty()) {


                            List<Difference> violatingDifferences = differences.stream()
                                    .filter(difference -> !difference.getType().equals(Difference.Type.PROPERTY_REMOVED_FROM_CLOSED_CONTENT_MODEL))
                                    .toList();
                            if (!violatingDifferences.isEmpty()) {

                                throw new IncompatibleSchemaException(
                                        schemaName,
                                        schemaType.version(),
                                        schemaType.typeClass(),
                                        violatingDifferences);
                            }
                        }
                    } else {

                        if (!registeredSchema.deepEquals(localSchema)) {


                            if (!Objects.equals(registeredSchema.toString(), localSchema.toString())) {
                                List<Difference> violatingDifferences = SchemaDiff.compare(((JsonSchema) registeredSchema).rawSchema(), localSchema.rawSchema());

                                if(forceRegisterOnIncompatibleSchema) {
                                    logger.warn("Found an incompatible schema for {} v{} but forceRegisterOnIncompatibleSchema=true. Overwriting existing entry in SchemaRegistry", schemaName, schemaType.version());
                                    try {

                                        schemaRegistryClient.deleteSchemaVersion(
                                                schemaName,
                                                "" + schemaType.version());
                                        // then do a hard delete of the version
                                        schemaRegistryClient.deleteSchemaVersion(
                                                schemaName,
                                                "" + schemaType.version(),
                                                true);
                                        // and recreate it
                                        schemaRegistryClient.register(
                                                schemaName,
                                                localSchema,
                                                schemaType.version(),
                                                -1);
                                    } catch (IOException | RestClientException e) {
                                        logger.error(
                                                "Exception during overwrite of Schema {} with version {}",
                                                schemaName,
                                                schemaType.version(),
                                                e);
                                }
                                } else {
                                    throw new IncompatibleSchemaException(
                                            schemaName,
                                            schemaType.version(),
                                            schemaType.typeClass(),
                                            violatingDifferences);
                                }
                            }
                        }
                    }
                } else if (schemaType.external()) {
                    // we did not find any schema with the exact version.
                    // since we are registering the type ourselves, this is an error
                    throw new SchemaNotFoundException(
                            schemaName,
                            schemaType.typeClass());
                } else {
                    // ensure we have an ordered list of schemas
                    registeredSchemas.sort(Comparator.comparingInt(ParsedSchema::version));
                    // see if the new version is exactly one higher than the last version
                    if (schemaType.version() != registeredSchemas.getLast().version() + 1) {
                        throw new InvalidSchemaVersionException(
                                schemaName,
                                registeredSchemas.getLast().version(),
                                schemaType.version(),
                                schemaType.typeClass());
                    }
                    // see if the new schema is backwards compatible with the previous ones
                    List<String> compatibilityErrors = localSchema.isCompatible(CompatibilityLevel.BACKWARD_TRANSITIVE,
                            registeredSchemas.stream().map(SimpleParsedSchemaHolder::new)
                                    .collect(Collectors.toList()));
                    if (compatibilityErrors.isEmpty()) {
                        // register the new schema
                        schemaRegistryClient.register(
                                schemaName,
                                localSchema,
                                schemaType.version(),
                                -1);
                    } else {
                        // incomp
                        throw new SchemaNotBackwardsCompatibleException(
                                schemaName,
                                registeredSchemas.getLast().version(),
                                schemaType.version(),
                                schemaType.typeClass(),
                                compatibilityErrors);
                    }
                }
            }
            // schema is fine, return
            return localSchema;
        } catch (IOException | RestClientException e) {
            throw new SchemaException(
                    "Unexpected Error while validating schema",
                    schemaType.getSchemaName(),
                    schemaType.typeClass(),
                    e);
        }
    }

    public JsonSchema generateJsonSchema(SchemaType schemaType) {
        return new JsonSchema(schemaGeneratorTheadLocal.get().generateSchema(schemaType.typeClass()), List.of(), Map.of(), schemaType.version());
    }

    private int getSchemaVersion(SchemaType schemaType, ParsedSchema parsedSchema) {
        try {
            return schemaRegistryClient.getVersion(schemaType.getSchemaName(), parsedSchema);
        } catch (IOException | RestClientException e) {
            throw new RuntimeException(e);
        }
    }

    private SchemaGenerator createJsonSchemaGenerator(ObjectMapper objectMapper) {
        SchemaGeneratorConfigBuilder configBuilder = new SchemaGeneratorConfigBuilder(objectMapper,
                SchemaVersion.DRAFT_7,
                OptionPreset.PLAIN_JSON);
        configBuilder.with(new JakartaValidationModule(JakartaValidationOption.INCLUDE_PATTERN_EXPRESSIONS,
                JakartaValidationOption.NOT_NULLABLE_FIELD_IS_REQUIRED));
        configBuilder.with(new JacksonModule());
        configBuilder.with(Option.FORBIDDEN_ADDITIONAL_PROPERTIES_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_FIELDS_BY_DEFAULT);
        configBuilder.with(Option.NULLABLE_METHOD_RETURN_VALUES_BY_DEFAULT);

        configBuilder.forTypesInGeneral().withTypeAttributeOverride((collectedTypeAttributes, scope, context) -> {
            if (scope.getType().getTypeName().equals("java.math.BigDecimal")) {
                JsonNode typeNode = collectedTypeAttributes.get("type");
                if (typeNode.isArray()) {
                    ((ArrayNode) collectedTypeAttributes.get("type")).set(0, "string");
                } else
                    collectedTypeAttributes.put("type", "string");
            }
        });
        return new SchemaGenerator(configBuilder.build());
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/PreviousSchemaVersionMissingException.java
================
package org.elasticsoftware.akces.schemas;

public class PreviousSchemaVersionMissingException extends SchemaException {
    private final int schemaVersion;

    public PreviousSchemaVersionMissingException(String schemaIdentifier, int schemaVersion, Class<?> implementationClass) {
        super("Missing Previous Schema version(s)", schemaIdentifier, implementationClass);
        this.schemaVersion = schemaVersion;
    }

    public int getSchemaVersion() {
        return schemaVersion;
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaException.java
================
package org.elasticsoftware.akces.schemas;

public class SchemaException extends RuntimeException {
    private final String schemaIdentifier;
    private final Class<?> implementationClass;

    public SchemaException(String message, String schemaIdentifier, Class<?> implementationClass) {
        super(message);
        this.schemaIdentifier = schemaIdentifier;
        this.implementationClass = implementationClass;
    }

    public SchemaException(String message,
                           String schemaIdentifier,
                           Class<?> implementationClass,
                           Throwable cause) {
        super(message, cause);
        this.schemaIdentifier = schemaIdentifier;
        this.implementationClass = implementationClass;
    }

    public String getSchemaIdentifier() {
        return schemaIdentifier;
    }

    public Class<?> getImplementationClass() {
        return implementationClass;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaNotBackwardsCompatibleException.java
================
package org.elasticsoftware.akces.schemas;

import java.util.List;

public class SchemaNotBackwardsCompatibleException extends SchemaException {
    private final int previousSchemaVersion;
    private final int schemaVersion;
    private final List<String> differences;

    public SchemaNotBackwardsCompatibleException(String schemaIdentifier,
                                                 int previousSchemaVersion,
                                                 int schemaVersion,
                                                 Class<?> implementationClass,
                                                 List<String> differences) {
        super("Schema not backwards compatible with previous version: " + previousSchemaVersion, schemaIdentifier, implementationClass);
        this.previousSchemaVersion = previousSchemaVersion;
        this.schemaVersion = schemaVersion;
        this.differences = differences;
    }

    public int getPreviousSchemaVersion() {
        return previousSchemaVersion;
    }

    public int getSchemaVersion() {
        return schemaVersion;
    }

    public List<String> getDifferences() {
        return differences;
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaNotFoundException.java
================
package org.elasticsoftware.akces.schemas;

public class SchemaNotFoundException extends SchemaException {

    public SchemaNotFoundException(String schemaIdentifier, Class<?> implementationClass) {
        super("Schema "+schemaIdentifier+" for class "+implementationClass.getName()+" Not Found", schemaIdentifier, implementationClass);
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/schemas/SchemaVersionNotFoundException.java
================
package org.elasticsoftware.akces.schemas;

public class SchemaVersionNotFoundException extends SchemaException {
    private final int schemaVersion;

    public SchemaVersionNotFoundException(String schemaIdentifier, int schemaVersion, Class<?> implementationClass) {
        super("Schema Version Not Found", schemaIdentifier, implementationClass);
        this.schemaVersion = schemaVersion;
    }

    public int getSchemaVersion() {
        return schemaVersion;
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/serialization/AkcesControlRecordSerde.java
================
package org.elasticsoftware.akces.serialization;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.errors.SerializationException;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serializer;
import org.elasticsoftware.akces.control.AggregateServiceRecord;
import org.elasticsoftware.akces.control.AkcesControlRecord;

import java.io.IOException;

public final class AkcesControlRecordSerde implements Serde<AkcesControlRecord> {
    private final Serializer<AkcesControlRecord> serializer;
    private final Deserializer<AkcesControlRecord> deserializer;

    public AkcesControlRecordSerde(ObjectMapper objectMapper) {
        this.serializer = new SerializerImpl(objectMapper);
        this.deserializer = new DeserializerImpl(objectMapper);
    }

    @Override
    public Serializer<AkcesControlRecord> serializer() {
        return serializer;
    }

    @Override
    public Deserializer<AkcesControlRecord> deserializer() {
        return deserializer;
    }

    private static class SerializerImpl implements Serializer<AkcesControlRecord> {
        private final ObjectMapper objectMapper;

        private SerializerImpl(ObjectMapper objectMapper) {
            this.objectMapper = objectMapper;
        }

        @Override
        public byte[] serialize(String topic, AkcesControlRecord data) {
            try {
                if (data instanceof AggregateServiceRecord csr) {
                    return objectMapper.writeValueAsBytes(csr);
                } else {
                    throw new SerializationException("Unsupported AkcesControlRecord type " + data.getClass().getSimpleName());
                }
            } catch (JsonProcessingException e) {
                throw new SerializationException(e);
            }
        }
    }

    private static class DeserializerImpl implements Deserializer<AkcesControlRecord> {
        private final ObjectMapper objectMapper;

        private DeserializerImpl(ObjectMapper objectMapper) {
            this.objectMapper = objectMapper;
        }

        @Override
        public AkcesControlRecord deserialize(String topic, byte[] data) {
            try {
                if (data == null) {
                    return null;
                } else if (topic.endsWith("Akces-Control")) {
                    return objectMapper.readValue(data, AkcesControlRecord.class);
                } else {
                    throw new SerializationException("Unsupported topic " + topic);
                }
            } catch (IOException e) {
                throw new SerializationException(e);
            }
        }
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/serialization/BigDecimalSerializer.java
================
package org.elasticsoftware.akces.serialization;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.JavaType;
import com.fasterxml.jackson.databind.JsonMappingException;
import com.fasterxml.jackson.databind.SerializerProvider;
import com.fasterxml.jackson.databind.jsonFormatVisitors.JsonFormatVisitorWrapper;
import com.fasterxml.jackson.databind.ser.std.StdSerializer;

import java.io.IOException;
import java.math.BigDecimal;

public final class BigDecimalSerializer extends StdSerializer<BigDecimal> {

    public BigDecimalSerializer() {
        super(BigDecimal.class);
    }

    @Override
    public void serialize(BigDecimal value, JsonGenerator gen, SerializerProvider provider) throws IOException {
        gen.writeString(value.toPlainString());
    }

    @Override
    public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint)
            throws JsonMappingException {
        visitor.expectStringFormat(typeHint);
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/serialization/ProtocolRecordSerde.java
================
package org.elasticsoftware.akces.serialization;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.ObjectReader;
import com.fasterxml.jackson.databind.ObjectWriter;
import com.fasterxml.jackson.dataformat.protobuf.ProtobufMapper;
import com.fasterxml.jackson.dataformat.protobuf.schema.ProtobufSchema;
import com.fasterxml.jackson.dataformat.protobuf.schema.ProtobufSchemaLoader;
import org.apache.kafka.common.errors.SerializationException;
import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serializer;
import org.elasticsoftware.akces.protocol.*;

import java.io.IOException;
import java.io.StringReader;
import java.util.Map;

public final class ProtocolRecordSerde implements Serde<ProtocolRecord> {
    private static final String domainEventRecordProto = """



            message DomainEventRecord {
              optional string id = 1;
              optional string tenantId = 2;
              optional string name = 3;
              optional int32 version = 4;
              optional bytes payload = 5;
              optional PayloadEncoding encoding = 6;
              optional string aggregateId = 7;
              optional string correlationId = 8;
              optional int64 generation = 9;
            }

            enum PayloadEncoding {
              JSON = 0;
              PROTOBUF = 1;
              BYTES = 2;
            }
            """;
    private static final String aggregateStateRecordProto = """



            message AggregateStateRecord {
              optional string name = 1;
              optional int32 version = 2;
              optional bytes payload = 3;
              optional PayloadEncoding encoding = 4;
              optional string aggregateId = 5;
              optional string correlationId = 6;
              optional int64 generation = 7;
              optional string tenantId = 8;
            }

            enum PayloadEncoding {
              JSON = 0;
              PROTOBUF = 1;
              BYTES = 2;
            }
            """;
    private static final String commandRecordProto = """



            message CommandRecord {
              optional string id = 1;
              optional string tenantId = 2;
              optional string name = 3;
              optional int32 version = 4;
              optional bytes payload = 5;
              optional PayloadEncoding encoding = 6;
              optional string aggregateId = 7;
              optional string correlationId = 8;
              optional string replyToTopicPartition = 9;
            }

            enum PayloadEncoding {
              JSON = 0;
              PROTOBUF = 1;
              BYTES = 2;
            }
            """;
    private static final String gdprKeyRecordProto = """



            message GDPRKeyRecord {
              optional string name = 1;
              optional int32 version = 2;
              optional bytes payload = 3;
              optional PayloadEncoding encoding = 4;
              optional string aggregateId = 5;
              optional string correlationId = 6;
              optional string tenantId = 7;
            }

            enum PayloadEncoding {
              JSON = 0;
              PROTOBUF = 1;
              BYTES = 2;
            }
            """;
    private static final String commandResponseRecordProto = """



            message CommandResponseRecord {
              optional string tenantId = 1;
              optional string name = 2;
              optional int32 version = 3;
              optional bytes payload = 4;
              optional PayloadEncoding encoding = 5;
              optional string aggregateId = 6;
              optional string correlationId = 7;
              optional string commandId = 8;
              repeated DomainEventRecord events = 9;
              optional bytes encryptionKey = 10;
            }

            message DomainEventRecord {
              optional string id = 1;
              optional string tenantId = 2;
              optional string name = 3;
              optional int32 version = 4;
              optional bytes payload = 5;
              optional PayloadEncoding encoding = 6;
              optional string aggregateId = 7;
              optional string correlationId = 8;
              optional int64 generation = 9;
            }

            enum PayloadEncoding {
              JSON = 0;
              PROTOBUF = 1;
              BYTES = 2;
            }
            """;
    private final ObjectMapper objectMapper = new ProtobufMapper();
    private final Serializer<ProtocolRecord> serializer;
    private final Deserializer<ProtocolRecord> deserializer;

    public ProtocolRecordSerde() {
        try {
            ProtobufSchema domainEventRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(domainEventRecordProto));
            ProtobufSchema aggregateStateRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(aggregateStateRecordProto));
            ProtobufSchema commandRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(commandRecordProto));
            ProtobufSchema gdprKeyRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(gdprKeyRecordProto));
            ProtobufSchema commandResponseRecordSchema = ProtobufSchemaLoader.std.load(new StringReader(commandResponseRecordProto));
            serializer = new SerializerImpl(objectMapper.writer(domainEventRecordSchema),
                    objectMapper.writer(aggregateStateRecordSchema),
                    objectMapper.writer(commandRecordSchema),
                    objectMapper.writer(gdprKeyRecordSchema),
                    objectMapper.writer(commandResponseRecordSchema));
            deserializer = new DeserializerImpl(objectMapper.readerFor(DomainEventRecord.class).with(domainEventRecordSchema),
                    objectMapper.readerFor(AggregateStateRecord.class).with(aggregateStateRecordSchema),
                    objectMapper.readerFor(CommandRecord.class).with(commandRecordSchema),
                    objectMapper.readerFor(GDPRKeyRecord.class).with(gdprKeyRecordSchema),
                    objectMapper.readerFor(CommandResponseRecord.class).with(commandResponseRecordSchema));
        } catch (IOException e) {
            throw new SerializationException(e);
        }
    }

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {

    }

    @Override
    public void close() {

    }

    @Override
    public Serializer<ProtocolRecord> serializer() {
        return serializer;
    }

    @Override
    public Deserializer<ProtocolRecord> deserializer() {
        return deserializer;
    }

    private static class SerializerImpl implements Serializer<ProtocolRecord> {
        private final ObjectWriter domainEventRecordWriter;
        private final ObjectWriter aggregateStateRecordWriter;
        private final ObjectWriter commandRecordWriter;
        private final ObjectWriter gdprKeyRecordWriter;
        private final ObjectWriter commandResponseRecordWriter;

        private SerializerImpl(ObjectWriter domainEventRecordWriter,
                               ObjectWriter aggregateStateRecordWriter,
                               ObjectWriter commandRecordWriter,
                               ObjectWriter gdprKeyRecordWriter,
                               ObjectWriter commandResponseRecordWriter) {
            this.domainEventRecordWriter = domainEventRecordWriter;
            this.aggregateStateRecordWriter = aggregateStateRecordWriter;
            this.commandRecordWriter = commandRecordWriter;
            this.gdprKeyRecordWriter = gdprKeyRecordWriter;
            this.commandResponseRecordWriter = commandResponseRecordWriter;
        }


        @Override
        public byte[] serialize(String topic, ProtocolRecord data) {
            try {
                if (data instanceof DomainEventRecord r) {
                    return domainEventRecordWriter.writeValueAsBytes(r);
                } else if (data instanceof AggregateStateRecord r) {
                    return aggregateStateRecordWriter.writeValueAsBytes(r);
                } else if (data instanceof CommandRecord r) {
                    return commandRecordWriter.writeValueAsBytes(r);
                } else if (data instanceof CommandResponseRecord e) {
                    return commandResponseRecordWriter.writeValueAsBytes(e);
                } else if (data instanceof GDPRKeyRecord r) {
                    return gdprKeyRecordWriter.writeValueAsBytes(r);
                } else {
                    throw new SerializationException("Unsupported ProtocolRecord type " + data.getClass().getSimpleName());
                }
            } catch (JsonProcessingException e) {
                throw new SerializationException(e);
            }
        }
    }

    private static class DeserializerImpl implements Deserializer<ProtocolRecord> {
        private final ObjectReader domainEventRecordReader;
        private final ObjectReader aggregateStateRecordReader;
        private final ObjectReader commandRecordReader;
        private final ObjectReader gdprKeyRecordReader;
        private final ObjectReader commandResponseRecordReader;

        public DeserializerImpl(ObjectReader domainEventRecordReader,
                                ObjectReader aggregateStateRecordReader,
                                ObjectReader commandRecordReader,
                                ObjectReader gdprKeyRecordReader,
                                ObjectReader commandResponseRecordReader) {
            this.domainEventRecordReader = domainEventRecordReader;
            this.aggregateStateRecordReader = aggregateStateRecordReader;
            this.commandRecordReader = commandRecordReader;
            this.gdprKeyRecordReader = gdprKeyRecordReader;
            this.commandResponseRecordReader = commandResponseRecordReader;
        }

        @Override
        public ProtocolRecord deserialize(String topic, byte[] data) {
            try {
                if (data == null) {
                    return null;
                } else if (topic.endsWith("DomainEvents") || topic.endsWith("DomainEventIndex")) {
                    return domainEventRecordReader.readValue(data);
                } else if (topic.endsWith("AggregateState")) {
                    return aggregateStateRecordReader.readValue(data);
                } else if (topic.endsWith("Commands")) {
                    return commandRecordReader.readValue(data);
                } else if (topic.endsWith("CommandResponses")) {
                    return commandResponseRecordReader.readValue(data);
                } else if (topic.endsWith("GDPRKeys")) {
                    return gdprKeyRecordReader.readValue(data);
                } else {
                    throw new SerializationException("Unsupported topic name " + topic + " cannot determine ProtocolRecordType");
                }
            } catch (IOException e) {
                throw new SerializationException(e);
            }
        }
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/util/EnvironmentPropertiesPrinter.java
================
package org.elasticsoftware.akces.util;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.context.event.ContextRefreshedEvent;
import org.springframework.context.event.EventListener;
import org.springframework.core.env.ConfigurableEnvironment;
import org.springframework.core.env.MapPropertySource;

import java.util.Collection;

public class EnvironmentPropertiesPrinter {
    private static final Logger logger = LoggerFactory.getLogger(EnvironmentPropertiesPrinter.class);

    @EventListener
    public void handleContextRefreshed(ContextRefreshedEvent event) {
        ConfigurableEnvironment env = (ConfigurableEnvironment) event.getApplicationContext().getEnvironment();
        logger.info("******* Environment Properties *******");
        env.getPropertySources()
                .stream()
                .filter(ps -> ps instanceof MapPropertySource)
                .map(ps -> ((MapPropertySource) ps).getSource().keySet())
                .flatMap(Collection::stream)
                .distinct()
                .filter(key -> key.startsWith("akces.") ||
                        key.startsWith("spring.") ||
                        key.startsWith("management.") ||
                        key.startsWith("server."))
                .sorted()
                .forEach(key -> logger.info("{}={}", key, env.getProperty(key)));
        logger.info("**************************************");
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/util/HostUtils.java
================
package org.elasticsoftware.akces.util;

import java.net.InetAddress;

public class HostUtils {
    private HostUtils() {

    }

    public static String getHostName() {

        String hostName = System.getenv("HOSTNAME");

        if (hostName == null || hostName.isEmpty()) {
            try {
                InetAddress addr = InetAddress.getLocalHost();
                hostName = addr.getHostName();
            } catch (Exception e) {
                hostName = "Unknown";
            }
        }
        return hostName;
    }
}

================
File: shared/src/main/java/org/elasticsoftware/akces/util/KafkaSender.java
================
package org.elasticsoftware.akces.util;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.errors.ApiException;

import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;

public class KafkaSender {

    private KafkaSender() {

    }

    public static <K, V> Future<RecordMetadata> send(Producer<K, V> producer, ProducerRecord<K, V> producerRecord) {
        return send(producer, producerRecord, null);
    }

    public static <K, V> Future<RecordMetadata> send(Producer<K, V> producer, ProducerRecord<K, V> producerRecord, Callback callback) {
        Future<RecordMetadata> future = producer.send(producerRecord, callback);
        if (future.isDone()) {
            try {
                future.get();
            } catch (InterruptedException ignored) {

            } catch (ExecutionException e) {
                if (e.getCause() instanceof ApiException) {
                    throw (ApiException) e.getCause();
                } else if (e.getCause() instanceof RuntimeException) {
                    throw (RuntimeException) e.getCause();
                } else {
                    throw new RuntimeException(e.getCause());
                }
            }
        }
        return future;
    }

}

================
File: shared/src/main/java/org/elasticsoftware/akces/util/KafkaUtils.java
================
package org.elasticsoftware.akces.util;

import org.apache.kafka.clients.admin.NewTopic;

import java.util.Map;

public final class KafkaUtils {
    public static final String DOMAIN_EVENT_INDEX = "-DomainEventIndex";

    private KafkaUtils() {
    }

    public static String getIndexTopicName(String indexName, String indexKey) {
        return indexName + "-" + indexKey + DOMAIN_EVENT_INDEX;
    }

    public static NewTopic createCompactedTopic(String name, int numPartitions, short replicationFactor) {
        NewTopic topic = new NewTopic(name, numPartitions, replicationFactor);

        String minInSyncReplicas = String.valueOf(calculateQuorum(replicationFactor));
        return topic.configs(Map.of(
                "min.insync.replicas", minInSyncReplicas,
                "cleanup.policy", "compact",
                "max.message.bytes", "20971520",
                "retention.ms", "-1",
                "segment.ms", "604800000",
                "min.cleanable.dirty.ratio", "0.1",
                "delete.retention.ms", "604800000",
                "compression.type", "lz4"));
    }

    public static int calculateQuorum(short replicationFactor) {
        return (replicationFactor / 2) + 1;
    }

}

================
File: shared/src/test/java/org/elasticsoftware/akces/gdpr/jackson/AkcesGDPRModuleTests.java
================
package org.elasticsoftware.akces.gdpr.jackson;

import com.fasterxml.jackson.core.Version;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.semver4j.Semver;

public class AkcesGDPRModuleTests {
    @Test
    public void testVersion() {
        Version version = AkcesGDPRModule.generateVersion(Semver.parse("0.11.0-SNAPSHOT"));
        Assertions.assertNotEquals(Version.unknownVersion(), version);
        Assertions.assertEquals(new Version(0, 11, 0, "SNAPSHOT", "org.elasticsoftwarefoundation.akces", "akces-runtime"), version);
    }

    @Test
    public void testVersionFromManifest() {

        AkcesGDPRModule akcesGDPRModule = new AkcesGDPRModule();
        Version version = akcesGDPRModule.version();
        Assertions.assertEquals(Version.unknownVersion(), version);
    }
}

================
File: shared/src/test/java/org/elasticsoftware/akces/gdpr/GDPRContextTests.java
================
package org.elasticsoftware.akces.gdpr;

import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;

import javax.crypto.spec.SecretKeySpec;

public class GDPRContextTests {
    @Test
    public void testEncryptDecrypt() {
        GDPRContext ctx = new EncryptingGDPRContext("ef234add-e0df-4769-b5f4-612a3207bad3", GDPRKeyUtils.createKey().getEncoded(), true);
        String encryptedData = ctx.encrypt("test");
        System.out.println(encryptedData);
        String decryptedData = ctx.decrypt(encryptedData);
        Assertions.assertEquals("test", decryptedData);
    }

    @Test
    public void testEncrypt() {
        GDPRContext ctx = new EncryptingGDPRContext("ef234add-e0df-4769-b5f4-612a3207bad3", GDPRKeyUtils.createKey().getEncoded(), true);
        String name = ctx.encrypt("Jasin Terlouw");
        String street = ctx.encrypt("Gershwinstraat 125");
        System.out.println("name: " + name);
        System.out.println("street: " + street);
        Assertions.assertEquals("Jasin Terlouw", ctx.decrypt(name));
        Assertions.assertEquals("Gershwinstraat 125", ctx.decrypt(street));
    }

    @Test
    public void testEncryptForTests() {
        GDPRContext ctx = new EncryptingGDPRContext("ef234add-e0df-4769-b5f4-612a3207bad3", GDPRKeyUtils.createKey().getEncoded(), true);
        String firstName = ctx.encrypt("Fahim");
        String lastName = ctx.encrypt("Zuijderwijk");
        String email = ctx.encrypt("FahimZuijderwijk@jourrapide.com");
        System.out.println(firstName);
        System.out.println(lastName);
        System.out.println(email);
        Assertions.assertEquals("Fahim", ctx.decrypt(firstName));
        Assertions.assertEquals("Zuijderwijk", ctx.decrypt(lastName));
        Assertions.assertEquals("FahimZuijderwijk@jourrapide.com", ctx.decrypt(email));
    }

    @Test
    public void testBadPaddingException() {
        String aggregateId = "47db2418-dd10-11ed-afa1-0242ac120012";
        String encrypted = "CnTg8ppy-GCWYLHEB4Ia9Q==";
        byte[] key = {90, -73, -103, -5, 40, -102, -103, 86, -29, -13, -30, -96, 109, -7, -61, 83, 58, 93, 60, -37, -54, -82, 24, 118, 53, 13, 32, 28, -68, -30, 63, -16};

        GDPRContext ctx = new EncryptingGDPRContext(aggregateId, key, true);

        ctx.decrypt(encrypted);

        String encryptedFirstName = ctx.encrypt("Fahim");
        String descryptedFirstName = ctx.decrypt(encryptedFirstName);

        Assertions.assertEquals("Fahim", descryptedFirstName);
    }

    @Test
    public void testECBEncryption() {
        String aggregateId = "user-1203847939";
        GDPRContext ctx = new EncryptingGDPRContext(aggregateId, GDPRKeyUtils.createKey().getEncoded(), false);

        String encryptedFirstName = ctx.encrypt("Fahim");
        String descryptedFirstName = ctx.decrypt(encryptedFirstName);

        Assertions.assertEquals("Fahim", descryptedFirstName);
    }

    @Test
    public void testECBEncryptionWithOtherContext() {
        String aggregateId = "user-1203847939";
        SecretKeySpec secretKey = GDPRKeyUtils.createKey();
        GDPRContext one = new EncryptingGDPRContext(aggregateId, secretKey.getEncoded(), false);
        GDPRContext two = new EncryptingGDPRContext(aggregateId, secretKey.getEncoded(), false);

        String encryptedFirstName = one.encrypt("Fahim");
        String decryptedFirstName = two.decrypt(encryptedFirstName);

        Assertions.assertEquals("Fahim", decryptedFirstName);
    }
}

================
File: shared/src/test/java/org/elasticsoftware/akces/gdpr/RocksDBGDPRContextRepositoryTests.java
================
package org.elasticsoftware.akces.gdpr;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.elasticsoftware.akces.protocol.GDPRKeyRecord;
import org.elasticsoftware.akces.serialization.ProtocolRecordSerde;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;

import java.util.List;

public class RocksDBGDPRContextRepositoryTests {
    @Test
    public void testWriteInTransaction() {
        ProtocolRecordSerde serde = new ProtocolRecordSerde();
        GDPRContextRepository repository = new RocksDBGDPRContextRepository(
                "target/rocksdb",
                "2",
                "Akces-GDPRKeys",
                serde.serializer(),
                serde.deserializer());

        repository.process(List.of(new ConsumerRecord<>(
                "Akces-GDPRKeys",
                2,
                0,
                "4117b11f-3dde-4b71-b80c-fa20a12d9add",
                new GDPRKeyRecord(
                        "TEST_TENANT",
                        "4117b11f-3dde-4b71-b80c-fa20a12d9add",
                        GDPRKeyUtils.createKey().getEncoded()))));

        Assertions.assertTrue(repository.exists("4117b11f-3dde-4b71-b80c-fa20a12d9add"));
    }
}

================
File: shared/pom.xml
================
<?xml version="1.0" encoding="UTF-8"?>

















<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-main</artifactId>
        <version>0.7.21-SNAPSHOT</version>
    </parent>

    <name>Elastic Software Foundation :: Akces :: Shared Classes</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>
    <artifactId>akces-shared</artifactId>

    <properties>
        <maven.compiler.source>17</maven.compiler.source>
        <maven.compiler.target>17</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.elasticsoftwarefoundation.akces</groupId>
            <artifactId>akces-api</artifactId>
            <version>${project.version}</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>org.semver4j</groupId>
            <artifactId>semver4j</artifactId>
        </dependency>
        <dependency>
            <groupId>jakarta.annotation</groupId>
            <artifactId>jakarta.annotation-api</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>joda-time</groupId>
            <artifactId>joda-time</artifactId>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-json-schema-serializer</artifactId>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.dataformat</groupId>
            <artifactId>jackson-dataformat-protobuf</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-generator</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jakarta-validation</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.victools</groupId>
            <artifactId>jsonschema-module-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>org.rocksdb</groupId>
            <artifactId>rocksdbjni</artifactId>
        </dependency>
        <dependency>
            <groupId>com.github.ben-manes.caffeine</groupId>
            <artifactId>caffeine</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

</project>

================
File: pom.xml
================
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.elasticsoftwarefoundation.akces</groupId>
        <artifactId>akces-framework-parent</artifactId>
        <version>0.7.21-SNAPSHOT</version>
    </parent>

    <artifactId>akces-framework-main</artifactId>
    <packaging>pom</packaging>

    <name>Elastic Software Foundation :: Akces :: Main module</name>
    <url>https://github.com/elasticsoftwarefoundation/akces-framework</url>

    <properties>
        <protobuf.version>4.29.3</protobuf.version>
        <log4j.version>2.24.3</log4j.version>
        <jackson.version>2.18.2</jackson.version>
        <slf4j.version>2.0.16</slf4j.version>
        <javassist.version>3.30.2-GA</javassist.version>
        <lz4.version>1.8.0</lz4.version>
        <kafka.version>3.8.1</kafka.version>
        <java-uuid-generator.version>5.1.0</java-uuid-generator.version>
        <rocksdb.version>9.10.0</rocksdb.version>
        <semver4j.version>5.6.0</semver4j.version>

        <testng-version>7.11.0</testng-version>
        <awaitility.version>4.2.2</awaitility.version>
        <persistence-api.version>2.2</persistence-api.version>
        <plexus-utils.version>4.0.2</plexus-utils.version>
        <netty.version>4.1.118.Final</netty.version>
        <micrometer.version>1.14.4</micrometer.version>
        <fast-uuid.version>0.2.0</fast-uuid.version>
        <confluent.version>7.9.0</confluent.version>
        <victools.version>4.37.0</victools.version>
        <joda-time.version>2.13.1</joda-time.version>
        <logback.version>1.5.16</logback.version>
        <testcontainers.version>1.20.5</testcontainers.version>
    </properties>

    <scm>
        <connection>scm:git:git@github.com:elasticsoftwarefoundation/elasticactors.git</connection>
        <developerConnection>scm:git:git@github.com:elasticsoftwarefoundation/elasticactors.git
        </developerConnection>
        <url>https://github.com/elasticsoftwarefoundation/elasticactors</url>
        <tag>v0.3.4</tag>
    </scm>

    <distributionManagement>
        <repository>
            <id>github</id>
            <name>GitHub Packages</name>
            <url>https://maven.pkg.github.com/elasticsoftwarefoundation/akces-framework</url>
        </repository>








    </distributionManagement>

    <repositories>
        <repository>
            <id>central</id>
            <url>https://repo.maven.apache.org/maven2</url>
        </repository>
        <repository>
            <id>Confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
    </repositories>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>com.fasterxml.jackson</groupId>
                <artifactId>jackson-bom</artifactId>
                <version>${jackson.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>io.netty</groupId>
                <artifactId>netty-bom</artifactId>
                <version>${netty.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>com.google.guava</groupId>
                <artifactId>guava-bom</artifactId>
                <version>${guava.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>io.micrometer</groupId>
                <artifactId>micrometer-bom</artifactId>
                <version>${micrometer.version}</version>
                <scope>import</scope>
                <type>pom</type>
            </dependency>
            <dependency>
                <groupId>jakarta.validation</groupId>
                <artifactId>jakarta.validation-api</artifactId>
                <version>3.1.1</version>
            </dependency>
            <dependency>
                <groupId>jakarta.inject</groupId>
                <artifactId>jakarta.inject-api</artifactId>
                <version>2.0.1</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-generator</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-module-jakarta-validation</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.victools</groupId>
                <artifactId>jsonschema-module-jackson</artifactId>
                <version>${victools.version}</version>
            </dependency>
            <dependency>
                <groupId>commons-validator</groupId>
                <artifactId>commons-validator</artifactId>
                <version>1.9.0</version>
            </dependency>
            <dependency>
                <groupId>org.apache.commons</groupId>
                <artifactId>commons-compress</artifactId>
                <version>1.27.1</version>
            </dependency>
            <dependency>
                <groupId>commons-io</groupId>
                <artifactId>commons-io</artifactId>
                <version>2.18.0</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-json-serializer</artifactId>
                <version>${confluent.version}</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-json-schema-serializer</artifactId>
                <version>${confluent.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>joda-time</groupId>
                        <artifactId>joda-time</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.jetbrains.kotlin</groupId>
                        <artifactId>kotlin-scripting-compiler-embeddable</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.jetbrains</groupId>
                        <artifactId>annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.scala-lang</groupId>
                        <artifactId>scala-library</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>commons-validator</groupId>
                        <artifactId>commons-validator</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.checkerframework</groupId>
                        <artifactId>checker-qual</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.guava</groupId>
                        <artifactId>guava</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>joda-time</groupId>
                <artifactId>joda-time</artifactId>
                <version>${joda-time.version}</version>
            </dependency>
            <dependency>
                <groupId>io.confluent</groupId>
                <artifactId>kafka-protobuf-serializer</artifactId>
                <version>${confluent.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.jetbrains.kotlin</groupId>
                        <artifactId>*</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.errorprone</groupId>
                        <artifactId>error_prone_annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.apache.commons</groupId>
                        <artifactId>commons-compress</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>org.checkerframework</groupId>
                        <artifactId>checker-qual</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.squareup.okio</groupId>
                        <artifactId>okio-jvm</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.j2objc</groupId>
                        <artifactId>j2objc-annotations</artifactId>
                    </exclusion>
                    <exclusion>
                        <groupId>com.google.protobuf</groupId>
                        <artifactId>protobuf-java</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.lz4</groupId>
                <artifactId>lz4-java</artifactId>
                <version>${lz4.version}</version>
            </dependency>
            <dependency>
                <groupId>com.eatthepath</groupId>
                <artifactId>fast-uuid</artifactId>
                <version>${fast-uuid.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-beans</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-context</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.aspectj</groupId>
                <artifactId>aspectjrt</artifactId>
                <version>${aspectj.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-tx</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-core</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-context-support</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-api</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>jcl-over-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>jul-to-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>log4j-over-slf4j</artifactId>
                <version>${slf4j.version}</version>
            </dependency>
            <dependency>
                <groupId>ch.qos.logback</groupId>
                <artifactId>logback-classic</artifactId>
                <version>${logback.version}</version>
            </dependency>
            <dependency>
                <groupId>ch.qos.logback</groupId>
                <artifactId>logback-core</artifactId>
                <version>${logback.version}</version>
            </dependency>
            <dependency>
                <groupId>org.javassist</groupId>
                <artifactId>javassist</artifactId>
                <version>${javassist.version}</version>
            </dependency>
            <dependency>
                <groupId>com.fasterxml.uuid</groupId>
                <artifactId>java-uuid-generator</artifactId>
                <version>${java-uuid-generator.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-webmvc</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-aspects</artifactId>
                <version>${spring.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.aspectj</groupId>
                        <artifactId>aspectjweaver</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-aop</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>javax.persistence</groupId>
                <artifactId>javax.persistence-api</artifactId>
                <version>${persistence-api.version}</version>
            </dependency>
            <dependency>
                <groupId>com.google.protobuf</groupId>
                <artifactId>protobuf-java</artifactId>
                <version>${protobuf.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-clients</artifactId>
                <version>${kafka.version}</version>
            </dependency>
            <dependency>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-streams</artifactId>
                <version>${kafka.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.rocksdb</groupId>
                        <artifactId>rocksdbjni</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.awaitility</groupId>
                <artifactId>awaitility</artifactId>
                <version>${awaitility.version}</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.plexus</groupId>
                <artifactId>plexus-utils</artifactId>
                <version>${plexus-utils.version}</version>
            </dependency>
            <dependency>
                <groupId>javax.annotation</groupId>
                <artifactId>javax.annotation-api</artifactId>
                <version>${javax-annotation-api.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testng</groupId>
                <artifactId>testng</artifactId>
                <version>${testng-version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>org.yaml</groupId>
                        <artifactId>snakeyaml</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.mockito</groupId>
                <artifactId>mockito-core</artifactId>
                <version>${mockito.version}</version>
            </dependency>
            <dependency>
                <groupId>org.springframework</groupId>
                <artifactId>spring-test</artifactId>
                <version>${spring.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>kafka</artifactId>
                <version>${testcontainers.version}</version>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>junit-jupiter</artifactId>
                <version>${testcontainers.version}</version>
            </dependency>
            <dependency>
                <groupId>com.github.ben-manes.caffeine</groupId>
                <artifactId>caffeine</artifactId>
                <version>${caffeine.version}</version>
                <exclusions>
                    <exclusion>
                        <groupId>com.google.errorprone</groupId>
                        <artifactId>error_prone_annotations</artifactId>
                    </exclusion>
                </exclusions>
            </dependency>
            <dependency>
                <groupId>org.semver4j</groupId>
                <artifactId>semver4j</artifactId>
                <version>${semver4j.version}</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-simple</artifactId>
                <version>${slf4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.apache.logging.log4j</groupId>
                <artifactId>log4j-core</artifactId>
                <version>${log4j.version}</version>
            </dependency>

            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>${spring-boot.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
            <dependency>
                <groupId>org.rocksdb</groupId>
                <artifactId>rocksdbjni</artifactId>
                <version>${rocksdb.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <modules>
        <module>api</module>
        <module>runtime</module>
        <module>client</module>
        <module>shared</module>
        <module>query-support</module>
    </modules>

    <build>
        <resources>
            <resource>
                <filtering>false</filtering>
                <directory>${basedir}/src/main/resources</directory>
                <includes>
                    <include>*.xml</include>
                    <include>*.yaml</include>
                    <include>*.properties</include>
                    <include>META-INF/*</include>
                    <include>META-INF/services/*</include>
                    <include>META-INF/spring/*</include>
                    <include>protobuf/*.proto</include>
                </includes>
            </resource>
        </resources>
        <testResources>
            <testResource>
                <filtering>true</filtering>
                <directory>${basedir}/src/test/resources</directory>
                <includes>
                    <include>*.xml</include>
                    <include>*.properties</include>
                    <include>*.yaml</include>
                    <include>META-INF/*</include>
                </includes>
            </testResource>
        </testResources>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>



================================================================
End of Codebase
================================================================
